---
title: Bayesian Power Analysis for A/B Testing
author: Erdem KarakÃ¶ylÃ¼
date: '2025-05-16'
description: A bayesian approach to sample size planning for A/B testing.
categories:
  - Power Analysis
  - Bayesian A/B testing
  - Minimum Detectable Effect
  - Region of Practical Equivalence
  - Prospective sample size planning
image: ./figures/cover_2.png
format: html

---


### Background

A/B testing is a critical tool for informing decisions, whether comparing webpages, app features, or marketing campaigns. The goal is to identify which version performs best based on a chosen metric like conversion rate, click-through rate, or revenue. For frequentists, A/B testing often involves Null Hypothesis Significance Testing (NHST), where concepts such as  $Î±$, $p-\text{value}$, and power analysis are central. My focus here is on power analysis.


#### Frequentist power analysis: a brief critique
In Frequentist Statistics, power analysis is essential during the planning phase of an experiment. Frequentist power is defined as the probability of correctly rejecting a false null hypothesis, which is often equated to the probability of correctly identifying a true effect. It's used to determine the minimum sample size required for a study to have a reasonable chance (typically 80% or higher) of detecting a statistically significant effect of a given size at a predetermined significance level (Î±, often 0.05). This helps researchers avoid underpowered studies that might fail to detect real effects and informs decisions about resource allocation.

The standard definition of frequentist power (the probability of rejecting a false null hypothesis) is framed within a $P(Dâˆ£H)$ perspective. It asks: "If the null hypothesis is indeed false (meaning a true effect exists), what is the probability that statistical testing will produce data that leads to rejecting that null hypothesis?" In my opinion, this is a risky and often flawed approach due to its incorrect use of probability for inference. This topic is beyond the scope of this post, but interested readers are invited to consult Aubrey Clayton's "Bernoulli's Fallacy" for a deeper technical and historical dive into this issue.

#### The Bayesian Alternative
The point is that Bayesian analysis offers a compelling alternative to the frequentist approach for A/B testing. It provides more direct and intuitive interpretations of results, without concerns about stopping issues, p-hacking, or needing multiple comparison corrections. Instead of focusing on the probability of observing data under a null hypothesis, Bayesian methods yield the probability distribution of the parameters of interest (e.g., the conversion rates of each variant and their difference). This allows for direct statements about the probability that one variant is better than the other, or the probability that the difference exceeds a practically significant threshold.

Nevertheless, power analysis can still be important in the Bayesian paradigm. While it takes on an altered meaning, it remains essential in experiment planning. Specifically, the goal is to determine the sample size, and consequently, adequate resource allocation to collect so that the experiment has a high probability of yielding informative results. Typically, this means assessing the probability of obtaining posterior distributions with sufficient precision (e.g., narrow credible intervals) or the probability that the effect size (the difference between the variants) exceeds a Minimum Detectable Effect (MDE). In practice, this means simulating potential experiment outcomes for a variety of sample sizes and analyzing the resulting posterior distributions.

### Brief Demonstration of Bayesian Power Analysis
>ðŸ‘‰ [Here is the notebook with all the code](https://github.com/erdemkarakoylu/erdemkarakoylu.github.io/blob/main/blog/bayesian_power/bayesian_power.ipynb)

#### Step 1: Define Goal and Metric(s)

Company X wants to increase the conversion rate on its landing page. To do so a new landing page is proposed.  Thus we have
* Current landing page; hereafter referred to as `A`
* Proposed landing page; hereafter referred to as `B`
* A single primary metric, `conversion rate` (number of conversions / total landings )

Obviously this is an oversimplified example but will serve to illustrate the approach.

#### Step 2: Eliciting Prior Beliefs

To compute posteriors I need to combine likelihoods obtained with data with priors. Unlike  Frequentist settings, Bayesian experiments are not conducted in a vaccum. Instead the paradigm encourages the incorporation of prior knowledge. While it is often the case that data will swamp out priors, there is value in carefully constructing them.   

Company X data scientists should therefore base their priors on any relevant existing knowledge. This could include:

1. **Historical Data:** If Company X has run similar A/B tests in the past on the same website or for similar features, the results of those tests can provide valuable information for setting priors. For example, if previous versions of the landing page had conversion rates consistently around 4-6%, this could inform the prior for variant A.

2. **Industry Benchmarks:** Depending on the industry, there might be typical conversion rate ranges that can inform the priors.

3. **Expert Opinions:** Marketing experts or product managers within Company X might have intuitions or expectations about the performance of the new variant B. These subjective beliefs can be formalized into a prior distribution.

4. **A "Skeptical" or Weakly Informative Prior:** If Company X has little to no prior information, they might choose a weakly informative prior. This is a prior that doesn't strongly favor any particular outcome but still provides some regularization. 


The outcome is binary, thus the natural likelihood to model the data is the Binomial distribution, which aggregates binary Bernoulli trials. There a few distributions that can be used to encode priors in this case. To keep things simple and avoid reaching for my favorite MCMC sampler, I define priors in terms of Beta distributions. A Beta distribution is a conjugate prior to the Binomial, meaning that updating the posterior has a closed form and does not require approximation. The parameters $Î±$ and $Î²$ of the Beta distribution represent prior "successes" and "failures," respectively. A higher $Î±$ relative to $Î²$ shifts the distribution towards higher values, and vice versa. The sum of $Î±$ and $Î²$ influences the "strength" or concentration of the prior belief (higher sum means more certainty). Below I show what this might look like.

* Let's assume historical data suggest a current conversion rate of 5% for variant $A$.
* Without any additional information and to keep things simple a reasonable prior for $A$ is therefore $prior_A=Beta(Î±=5, Î²=95)$. 
* For one reason or another, I also expect a lift of 1% from variant B so an acceptable prior is $prior_B=Beta(Î±=6, Î²=94)$. Importantly, this does not discard the possibility that variant B is worse or better. The point is that my assumptions are made explicit, open to critique, and certainly revisable. 
* 

These priors can be seen in Figure 1.

<figure>
  <img src="./figures/figure1.png" alt="Beta Priors for A and B variants" style="width:800px;">
  <figcaption>Figure 1: Beta Priors for A and B variants.</figcaption>
</figure>

* Updating the Beta prior to get the posterior is trivial as it boils down to adding new successes and new failures to the existing $Î±$ and $Î²$ parameters, respectively. 
* The function `run_analytical_ab_test` in the [notebook](https://github.com/erdemkarakoylu/erdemkarakoylu.github.io/blob/main/blog/bayesian_power/bayesian_power.ipynb) simulates the Data Generation Process with a know conversion rate for variant A and a latent (not yet known) conversion rate for variant B.

#### Step 3: Defining the Minimum Detectable Effect (MDE)

* Question: what is the smallest <u>practically significant</u> difference in conversion rates that Company X would want to detect?
* The answer is the Minimum Detectable Effect, hereafter MDE. 
* Note the actual effect would need to be greater than the MDE; the greater the actual effect relative to the MDE, the smaller the number of samples needed to detect a difference at the MDE level. 

* Let's assume Company X is interested in detecting an absolute difference of at least 1% in the conversion rate, that's our MDE
* If the baseline conversion rate is around 5%, stakeholders then want to be able to reliably detect if variant B increases it to 6% or more. 
* Given the above and to make it easier, let's say that the latent true conversion rate of Variant B will be 7%  - Obviously several scenarios with different conversion rates coould be simulated. To keep it short, however, we'll just limit the simulation to a single case. 

#### Step 4: Prospective Sample Size Planning (Bayesian "Power Analysis")

* We are not trying to calculate the probability of rejecting a null hypothesis - this wouldn't tell us much about the alternative anyway.
* The focus is on the probability of achieving a desired level of certainty about the difference in conversion rates. 
* This often involves simulating potential outcomes for different sample sizes and evaluating the resulting posterior distribution.
* We assess the following conditions
    1. The posterior probability that variant B is better than variant A (i.e., conversion rate of B > conversion rate of A) is above a certain threshold; set here at 95%.
    2. The posterior probability that the difference in conversion rates (B - A) is greater than the MDE is above a certain threshold; set here at 80%.

* The function  `evaluate_analytica_power` in the [notebook](https://github.com/erdemkarakoylu/erdemkarakoylu.github.io/blob/main/blog/bayesian_power/bayesian_power.ipynb) is used to evaluate the posterior difference for a given number of samples in view of these criteria. Now we can build a `power curve` against sample size

<figure>
  <img src="./figures/figure2.png" alt="Power Analysis Curve" style="width:800px;">
  <figcaption>Figure 2: Power curve showing showing expected power given the number of samples for each variant. Red line indicates desired threshold probability of detecting that stipulated MDE is satisfied; in this case 80%.</figcaption>

</figure>
### Power Analysis Interpretation
* The figure above suggests 3500 FOR EACH variant would be a sufficient size. 
* I recommend running it a bunch of times and get uncertainty envelopes around these numbers for added credibility to threshold surpassing; this is skipped for expediency.

#### Running The Actual Experiment.
* Armed with this information we can actually run an A/B test and see what kind of posterior we get. 
* Note that unlike Frequentists we don't have traditionally accepted significance levels. How to come to a decision is an important discussion point between  the data scientist and the stakeholder/decision maker.
* As a decision guide as to whether the difference is significantly greater than 0, I propose a Region of Pratical Equivalence (ROPE).
* Determining a good ROPE is beyond the scope of this post. But the power analysis had two criteria so for illustration, I will use these as follows
* First ROPE will be between (-1 and 0.005) and I will visually evaluate whether 95% of the Highest Density Interval (HDI) of the posterior of the difference (B-A) is greater
* Second rope will be between (0 and 0.015) and I will visually evaluate whether 80% of the HDI of the same posterior is greater. 
* Note that for both I left some wiggle room and bumped the ROPE's upper limits by half a percent relative to the Power Analysis criteria

<figure>
  <img src="./figures/figure3.png" alt="Experiment posterior" style="width:800px;">
  <figcaption>Figure 3: Experiment outcome depicted as the posterior distribution of the difference, B-A, in terms of conversion between variants. Posterior statistics in black. Left panel includes Region of Practical Equivalence (ROPE) for absolute difference criterion, Right includes ROPE for minimum detectability criterion. See text for more. </figcaption>
</figure>

Figure above:
* Both plots above show the same posterior with different ROPEs
* In black, the chosen HDI - 94% in this case as a reminder to be practical, not religious about these numbers
* Numbers in black are lower and upper HDI bounds, and posterior mean.  

* Left panel shows 0% of the posterior  overlaps with the ROPE, which satisfies the condition that $P((B-A)>0) > 95\%$.

* Right panel shows shows 2.1% of the posterior overlaps with the ROPE, meaning $P((B-A)>0.01)=97.1\%>80\%$

### In conclusion
Power analysis is not a strict necessity for Bayesians. It's OK for us to go and collect more data if the posterior is not what we want it to be -  data snooping or p-hacking is not a thing here. Nevertheless it can still be a useful tool for experiment planning and adequate resource allocation.

Thanks for reading & happy (probabilistic) coding!


