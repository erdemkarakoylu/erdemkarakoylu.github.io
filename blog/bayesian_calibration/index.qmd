---
title: A Visual Tool for Bayesian Model Calibration
author: Erdem Karaköylü
date: '2025-05-16'
description: A brief description and demonstration of LOO-PIT
categories:
  - LOO-PIT
  - Model Calibration
  - Bayesian Modeling
  - Arviz
image: ./figures/cover.png
format: html
bibliography: literature.bib
---

## Introduction

Model calibration is a critical requirement for any model that produces probabilistic outputs. For Bayesian models, which yield full distributions rather than point estimates, calibration is especially important. The reliability of these predictive distributions determines how well these models will generalize to unseen data.

In this post, I focus on the **Leave-One-Out Probability Integral Transform (LOO-PIT)** as a tool to assess calibration in Bayesian models. I illustrate both its theoretical foundation and practical use through simulated examples and a real-world case study in Oceanographic Remote Sensing. 

> Please see [this Jupyter notebook](https://github.com/erdemkarakoylu/bayesian_TOA_2_IOP/blob/main/02_02_Understanding_loo_pit.ipynb) for the code used to generate the various cases. 

---

## Understanding Leave-One-Out Probability Integral Transform (LOO-PIT)

### PIT: A Little-Known Result from Probability Theory

To build some intuition about  **Probability Integral Transform (PIT)**, consider the following:

1. Take a probability distribution $G$ with cumulative distribution function (CDF) $F$.
2. Sample a set of values, $X \sim G$.
3. For each sampled value $x_i$, compute  $u_i = F(x_i)$.

It turns out that the collection of $u_i$ values are uniformly distributed on the interval [0, 1]. This is a classic identity in probability theory. The transformation from sample space to uniform space under the CDF is the backbone of the PIT, and it holds whether $G$ is a normal distribution, a beta, or something else entirely — <u>as long as it’s continuous</u>. It also turns out this result is useful to assess model calibration in Bayesian models.

### Understanding PIT through Visualization 
To make these ideas concrete, I simulate synthetic predictions, draw samples from a known “data-generating” distribution, then compute CDF values under a model’s assumed predictive distribution. By manipulating the model's assumptions (e.g., narrower or wider spread, shifted mean), I generate different calibration failure modes and compare them to the case of a well-calibrated model (*cf.* @fig-pit_demo). These stylized examples are helpful when interpreting real-world diagnostics later.

::: {#fig-pit_demo}
![](./figures/PIT-demo.png)

Simulated PIT histograms showing four canonical calibration behaviors. ***Top left*** well-calibrated model results in PIT values that are uniform; ***top right*** <u>underdispersed</u> — model is overconfident, resulting in U-shaped PIT histogram; ***bottom right*** <u>overdispersed</u> — model is too uncertain, resulting in a hump-shaped PIT histogram; ***bottom left***: <u>biased</u> — predictive distribution systematically misaligned in this case, biased high. 
:::

---

### Extending PIT to Bayesian Models

In traditional machine learning, performance is often assessed using out-of-sample testing — splitting the dataset into training and testing portions. However, this approach can be problematic because valuable training information is discarded and arbitrary splitting risks introducing **random partition bias**. K-Fold cross validation can mitigate, but the arbitrary choice of "K" remains problematic for the same reasons. In the case of large complex Bayesian models, K-Fold CV is not practical due to the cost of recomputing high-dimensional posteriors.

Bayesian modeling, however, offers an remarkable alternative via **Pareto-Smoothed Importance Sampling Leave-One-Out (PSIS-LOO)** [@vehtari2017]. This makes assessing generalization possible **without needing to retrain the model N** (number of observations) **times**. LOO-PIT builds on PSIS-LOO to assess **calibration** by applying PIT to the posterior predictive distribution of left-out points.

::: {.callout-important title="Calibration vs. Generalization"}
In non-Bayesian models, **generalization** is typically assessed using metrics like accuracy, RMSE, or log-loss on a held-out test set — all of which focus on **point predictions**.

Bayesian models, by contrast, aim to represent the full **data generation process**. Each prediction is in fact a **predictive distribution** that quantifies uncertainty about the outcome. A Bayesian model is considered **well-calibrated** when these predictive distributions accurately reflect the uncertainty in new observations — that is, when they are consistent with what is epistemically known about the process based on the data and the model.

Calibration is therefore central to generalization in Bayesian modeling — not just how well the model predicts, but how honest it is about what it doesn’t know.
:::

---

### How LOO-PIT Works

1. Train the model on $N-1$ observations, leaving one out.
2. Predict the **posterior predictive distribution** of the left-out observation — not a point estimate, but a full distribution.
3. Compute the **CDF of the predictive distribution** at the actual observed value.
4. This gives a single value between 0 and 1 — the **LOO-PIT value** for that observation.
5. Repeat this for all $N$ observations, rotating the one left out each time.
6. Collect the resulting LOO-PIT values and compare their distribution to **Uniform(0, 1)**.
7. If the model is well-calibrated, the LOO-PIT values should resemble a uniform distribution.

If the distribution **deviates from uniformity**, it can reveal the **type of miscalibration**:
- **Underdispersed** → model is overconfident (predictive intervals too narrow)
- **Overdispersed** → model is too uncertain (intervals too wide)
- **Biased** → model consistently over- or underpredicts

Next, I will train 4 models. For the non-calibrated cases, I force the models to yield the particular pathologies mentioned above. 
---

### Visualizing Simulated LOO-PIT Scenarios

The main output of a trained Bayesian model is a multi-dimensional posterior distribution. This is a rich construct - for which classical machine learning has no answer. Consequently the posterior can be mined for a number of insights, however, its complexity means that most Bayesian modeling diagnostic tools are visual. This helps the user to spot major problems quickly, and provides hints as to how to address them. Below I show three different types of plot for different models - calibrated or affected by one of the three pathologies. 
Let's start with what a calibrated model looks like. @fig-loo-pit-calibrated shows 3 panels including Posterior Predictive Checks (PPC), and two ways of comparing the LOO-PIT to a Uniform distribution. PPC [@gelman1996] use the posterior distribution to simulate the output data and show whether the data generation process is well approximated. The caveat though is that they represent in-sample diagnostic; some call it Posterior Retrodictive Checks as a reminder. LOO-based tools are better suited to verify out-of-sample performance, and in this case calibration. Thus the middle row [@fig-loo-pit-calibrated] compares the density estimation of many draws from the  Uniform distribution to the density estimation of PIT of the left-out sample $y_i$ conditioned on the model posterior fit on the remaining $N-1$ samples; $y_{-i}$. 

::: {#fig-loo-pit-calibrated}
![](./figures/loo_pit_calibrated.png)

Visual diagnostics useful to see how well a model has learned and whether it is well calibrated. **Top row** - Kernel density estimation of Posterior Predictive Checks (PPC). PPC use the model posterior distribution and the training data to simulate output to verify conformity to labeled output. Multiple simulatino runs are shown (light gray), along with their mean (orange dashed line). This is compared to the actual observations (black line). **Bottom left** - Kernel density estimation of model LOO-PIT (dark blue line)  compared from multiple draws from a Uniform distribution (light blue lines). **Bottom right** Deviation of the empirical CDF of LOO-PIT values from the ideal uniform (dark blue line), with a 94% Highest Density Interval envelope from simulated uniform samples. 
:::

In contrast @fig-loo_pit_pathologies provides a visual primer for the canonical pathologies LOO-PIT can help detect with the same three types of plots.  
For instance, in the case of the top row showing the PPC plots, notice how the estimated density of the observations are situated in relation to the simulated draws (and their mean). PPC's begin to highlight problems, even though this is an in-sample evaluation. But as mentioned earlier, we want to know about out-of-sample calibration. So we go to the middle row and see that each pathology results in LOO-PIT density estimates of a particular shape; *U-* , *hump-*shaped or skewed in the respective cases of underdispersion, overdispersion and bias. Moreover in each case there is little overlap with the Uniform distribution denstiy estimates.

::: {#fig-loo_pit_pathologies}
![](./figures/loo_pit_pathologies.png)

**Top Row** Model Posterior Predictive Checks, same as before. Pathologies start to show up at this stage. 
Simulated LOO-PIT histograms showing four canonical calibration behaviors. ***Top left*** well-calibrated model results in PIT values that are uniform; ***top right*** <u>underdispersed</u> — model is overconfident, resulting in U-shaped PIT histogram; ***bottom right*** <u>overdispersed</u> — model is too uncertain, resulting in a hump-shaped PIT historgram; ***bottom left***: <u>biased</u> — predictive distribution systematically misaligned - this cased biased high. 
:::

### A Real World LOO-PIT Example 
The examples shown so far were designed to be clearly distinguishable. This type of plot is not as clear in the wild. 

@fig-pit_loo_wild shows the now familiar three-panel figure. This a real world, in this case Bayesian Additive Trees - BART - model I wrote for an oceanographic predictive application. The PPC is pretty clean with no obvious problems. The LOO-PIT plots on the other hand suggest calibration issues that might come back to bite when predicting on new data. But what, here, is specifically the problem? Or problems?

:::{#fig-pit_loo_wild}
![](./figures/model1_loo_pit.png)

Same three panels seen before in @fig-loo-pit-calibrated and @fig-loo_pit_pathologies. This time the model is fitting real observations, and the results are more nuanced; clean PPC (top), problematic LOO-PIT plots. 
:::

In Bayesian modeling ***we model the data generation process***;  not the data. Thus unlike in classical machine learning we can iteratively improve on the model without the same concerns for data leakage. The Bayesian Workflow [@gelman2020bayesianworkflow] in fact strongly recommends multiple modeling iterations, usually with increasing complexity in model structure. This is a more cerebral affair, when compared to classical machine learning and frequentist statistical modeling; it takes more time and mental effort. Ultimately, though the insights are deeper, the understanding more complete the experience, both of which make for a more satisfying experience.

I'll buy you coffee if you correctly diagnose @fig-pit_loo_wild.

---



## References



