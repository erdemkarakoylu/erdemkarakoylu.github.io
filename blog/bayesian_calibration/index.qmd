---
title: A Visual Tool for Bayesian Model Calibration"
author: Erdem Karaköylü
date: '2025-05-16'
description: A brief description and demonstration of LOO-PIT
categories:
  - LOO-PIT
  - Model Calibration
  - Bayesian Modeling
  - Arviz
image: ./figures/cover.png
format: html
bibliography: literature.bib
---

## Introduction

Model calibration is a critical requirement for any model that produces probabilistic outputs. For Bayesian models, which yield full distributions rather than point estimates, calibration is especially important. The reliability of these predictive distributions determines how well these models will generalize to unseen data.

In this post, I focus on the **Leave-One-Out Probability Integral Transform (LOO-PIT)** as a tool to assess calibration in Bayesian models. I illustrate both its theoretical foundation and practical use through simulated examples and a real-world case study in Oceanographic Remote Sensing. 

> Please see [this Jupyter notebook](https://github.com/erdemkarakoylu/bayesian_TOA_2_IOP/blob/main/02_02_Understanding_loo_pit.ipynb) for the code used to generate the various cases. 

---

## Understanding Leave-One-Out Probability Integral Transform (LOO-PIT)

### PIT: A Little-Known Result from Probability Theory

To build some intuition about  **Probability Integral Transform (PIT)**, consider the following:

1. Take a probability distribution $G$ with cumulative distribution function (CDF) $F$.
2. Sample a set of values, $X \sim G$.
3. For each sampled value $x_i$, compute  $u_i = F(x_i)$.

It turns out that the collection of $u_i$ values are uniformly distributed on the interval [0, 1]. This is a classic identity in probability theory. The transformation from sample space to uniform space under the CDF is the backbone of the PIT, and it holds whether $G$ is a normal distribution, a beta, or something else entirely — <u>as long as it’s continuous</u>. It also turns out this result is useful to assess model calibration in Bayesian

### Understanding PIT through Visualization 
To make these ideas concrete, I simulate synthetic predictions. draw samples from a known “data-generating” distribution, then compute CDF values under a model’s assumed predictive distribution. By manipulating the model's assumptions (e.g., narrower or wider spread, shifted mean), I generated different calibration failure modes and compare it to the case of a well-calibrated model (*cf.* @fig-pit_demo). 
These stylized examples are helpful when interpreting real-world diagnostics later.

::: {#fig-pit_demo}
![](./figures/PIT-demo.png)

Simulated PIT histograms showing four canonical calibration behaviors. ***Top left*** well-calibrated model results in PIT values that are uniform; ***top right*** <u>underdispersed</u> — model is overconfident, resulting in U-shaped PIT histogram; ***bottom right*** <u>overdispersed</u> — model is too uncertain, resulting in a hump-shaped PIT historgram; ***bottom left***: <u>biased</u> — predictive distribution systematically misaligned - this cased biased high. 
:::

---

### Extending PIT to Bayesian Models with Limited Data

In traditional machine learning, performance is often assessed using out-of-sample testing — splitting the dataset into training and testing portions. However, this approach can be problematic because valuable training information is discared and arbitrary splitting risks introducing **random partition bias**. K-Fold cross validation can mitigate, but the arbitrary choice of "K" remains problematic for the same reasons. In the case of large complex Bayesian models, K-Fold CV is not practical to the computational cost of recomputing high-dimensional posteriors

Bayesian modeling however offers an remarkable alternative via **Pareto-Smoothed Importance Sampling Leave-One-Out (PSIS-LOO)**. This makes assessing generalization possible **without needing to retrain the model N** (number of observations) **times**. LOO-PIT builds on PSIS-LOO to assess **calibration** by applying PIT to the posterior predictive distribution of left-out points.

::: {.callout-important title="Calibration vs. Generalization"}
In non-Bayesian models, **generalization** is typically assessed using metrics like accuracy, RMSE, or log-loss on a held-out test set — all of which focus on **point predictions**.

Bayesian models, by contrast, aim to represent the full **data generation process**. Each prediction is in fact a **predictive distribution** that quantifies uncertainty about the outcome. A Bayesian model is considered **well-calibrated** when these predictive distributions accurately reflect the uncertainty in new observations — that is, when they are consistent with what is epistemically known about the process based on the data and the model.

Calibration is therefore central to generalization in Bayesian modeling — not just how well the model predicts, but how honest it is about what it doesn’t know.
:::

---

### How LOO-PIT Works

1. Train the model on $N-1$ observations, leaving one out.
2. Predict the **posterior predictive distribution** of the left-out observation — not a point estimate, but a full distribution.
3. Compute the **CDF of the predictive distribution** at the actual observed value.
4. This gives a single value between 0 and 1 — the **LOO-PIT value** for that observation.
5. Repeat this for all $N$ observations, rotating the one left out each time.
6. Collect the resulting LOO-PIT values and compare their distribution to **Uniform(0, 1)**.
7. If the model is well-calibrated, the LOO-PIT values should resemble a uniform distribution.

If the distribution **deviates from uniformity**, it can reveal the **type of miscalibration**:
- **Underdispersed** → model is overconfident (predictive intervals too narrow)
- **Overdispersed** → model is too uncertain (intervals too wide)
- **Biased** → model consistently over- or underpredicts

Next, I will train 4 models. For the non-calibrated cases, I will force the models so as to yield the particular pathologies mentioned above. 
---

### Visualizing Simulated LOO-PIT Scenarios

@fig-loo_pit_demo provides a primer of the LOO-PIT approach for the aforementioned 4 canonical cases. Most of Bayesian modeling diagnostic tools are visual. This helps to spot major problems quickly, and provides hints as to how to address them. 


::: {#fig-loo_pit_demo}
![](./figures/loo_pit_demo.png)

**Top Row** Model Posterior Predictive Checks - these simulate output data for comparison with observations. 
Simulated LOO-PIT histograms showing four canonical calibration behaviors. ***Top left*** well-calibrated model results in PIT values that are uniform; ***top right*** <u>underdispersed</u> — model is overconfident, resulting in U-shaped PIT histogram; ***bottom right*** <u>overdispersed</u> — model is too uncertain, resulting in a hump-shaped PIT historgram; ***bottom left***: <u>biased</u> — predictive distribution systematically misaligned - this cased biased high. 
:::

---



## References



