[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Musings & Crumbs",
    "section": "",
    "text": "A Bayesian Approach to Marine Modeling\n\n\nFitting data to a system of Differential Equations\n\n\n\nErdem Karaköylü\n\n\nMar 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausality in observational data\n\n\nMatching and propensity scores for causal inference\n\n\n\nErdem Karaköylü\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Statistical Practices in A/B Testing\n\n\nThe inadequacy of p-values and what to do about it\n\n\n\nErdem Karaköylü\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ZeroSumNormal Distribution\n\n\nA principled approach to avoinding overparameterization and non-identifiability in categorical regression\n\n\n\nErdem Karaköylü\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Porfolio",
    "section": "",
    "text": "BayEstonia!\n\n\nA Bayesian analysis of voting tendencies in Estonia\n\n\n\nErdem Karaköylü\n\n\nJan 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost\n\n\nUsing Satellite Data to Predict and Explain PFGs with XGBoost and SHAP\n\n\n\nErdem M. Karaköylü\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/elections_estonia/elections_estonia.html",
    "href": "portfolio/elections_estonia/elections_estonia.html",
    "title": "BayEstonia!",
    "section": "",
    "text": "Preamble: This is a data analysis and modeling project to predict voter party preference in Estonia on the basis of demographics. The intended process involves initial data exploration, followed by the construction, fitting, and analysis of a series of increasingly complex Bayesian categorical models. This Bayesian approach offers a robust alternative to traditional frequentist Analysis of Variance (ANOVA) methods commonly employed in scientific research and A/B/n testing in fields such as marketing, user experience (UX), and web design. A significant benefit of the Bayesian model is its inherent ability to quantify predictive uncertainty through the calculation of posterior (inferential) probability distributions, contrasting with the reliance on sampling distributions in frequentist procedures\nContents:\n\nExploratory Data Analysis\nModeling  \\(→\\) Model Building and, Prior Elicitation and Prior Predictive Checks  \\(→\\) Model Fitting and Goodness-of-Fit Analysis  \\(→\\) Posterior Predictive Checks  \\(→\\) Summary and Conclusion\n\nData Provenance: This data is courtesy of SALK. SALK refers to the Liberal Citizen Foundation in Estonia; a political organization aimed at influencing Estonian policy and parliamentary elections. This foundation was established with the (ultimately attained) goal of helping liberal forces gain a majority in the 2023 Estonian parliamentary elections.\nAcknowledgments: I thank Alex Andorra and the folks at Intuitive Bayes for helping me understand non-identifiability and overparameterization in statistical modeling.\n\n\nCode\nimport arviz as az\nimport matplotlib.pyplot as pp\nfrom matplotlib import rcParams\nimport numpy as np\nimport pandas as pd\n\nimport pymc as pm\n\n\n\n\nCode\nrcParams['font.size'] = 12\n\n\n\n1. Exploratory Data Analysis\n\n\nCode\ndata = pd.read_csv('~/projex/elections/data/estonian-data.csv')\n\n\n\n\nCode\ndata.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nage_group\n16-24\n16-24\n16-24\n16-24\n16-24\n\n\neducation\nBasic education\nBasic education\nBasic education\nBasic education\nBasic education\n\n\ngender\nMale\nMale\nMale\nMale\nMale\n\n\nnationality\nEstonian\nEstonian\nEstonian\nEstonian\nEstonian\n\n\nelectoral_district\nHaabersti, Põhja-Tallinn ja Kristiine\nHarju- ja Raplamaa\nHarju- ja Raplamaa\nHarju- ja Raplamaa\nHarju- ja Raplamaa\n\n\nunit\nKristiine\nHarjumaa\nHarjumaa\nHarjumaa\nHarjumaa\n\n\nEKRE\n0\n0\n0\n1\n0\n\n\nEesti 200\n0\n0\n0\n0\n0\n\n\nHard to say\n0\n0\n0\n0\n0\n\n\nIsamaa\n0\n0\n0\n0\n0\n\n\nKeskerakond\n0\n0\n0\n0\n1\n\n\nMitte ükski erakond\n1\n0\n0\n0\n0\n\n\nOther\n0\n0\n0\n0\n0\n\n\nParempoolsed\n0\n0\n0\n0\n0\n\n\nReformierakond\n0\n1\n0\n0\n0\n\n\nRohelised\n0\n0\n0\n0\n0\n\n\nSDE\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\nI don’t like the “Hard to say” column name; replace below with the clearer “Undecided” label.\n\n\nCode\ndata.rename(columns={'Hard to say': 'Undecided'}, inplace=True)\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5206 entries, 0 to 5205\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   age_group            5206 non-null   object\n 1   education            5206 non-null   object\n 2   gender               5206 non-null   object\n 3   nationality          5206 non-null   object\n 4   electoral_district   5206 non-null   object\n 5   unit                 5206 non-null   object\n 6   EKRE                 5206 non-null   int64 \n 7   Eesti 200            5206 non-null   int64 \n 8   Undecided            5206 non-null   int64 \n 9   Isamaa               5206 non-null   int64 \n 10  Keskerakond          5206 non-null   int64 \n 11  Mitte ükski erakond  5206 non-null   int64 \n 12  Other                5206 non-null   int64 \n 13  Parempoolsed         5206 non-null   int64 \n 14  Reformierakond       5206 non-null   int64 \n 15  Rohelised            5206 non-null   int64 \n 16  SDE                  5206 non-null   int64 \ndtypes: int64(11), object(6)\nmemory usage: 691.6+ KB\n\n\n\n\nCode\ndata.columns.tolist()\n\n\n['age_group',\n 'education',\n 'gender',\n 'nationality',\n 'electoral_district',\n 'unit',\n 'EKRE',\n 'Eesti 200',\n 'Undecided',\n 'Isamaa',\n 'Keskerakond',\n 'Mitte ükski erakond',\n 'Other',\n 'Parempoolsed',\n 'Reformierakond',\n 'Rohelised',\n 'SDE']\n\n\n\n\nCode\ndata.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nEKRE\n5206.0\n0.124856\n0.330587\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nEesti 200\n5206.0\n0.082981\n0.275880\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nUndecided\n5206.0\n0.127353\n0.333400\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nIsamaa\n5206.0\n0.054552\n0.227126\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nKeskerakond\n5206.0\n0.119285\n0.324155\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nMitte ükski erakond\n5206.0\n0.181521\n0.385487\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nOther\n5206.0\n0.007876\n0.088403\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nParempoolsed\n5206.0\n0.002305\n0.047960\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nReformierakond\n5206.0\n0.215136\n0.410956\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nRohelised\n5206.0\n0.017480\n0.131063\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nSDE\n5206.0\n0.066654\n0.249446\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\ndata.loc[:, 'EKRE':].sum(axis=1).describe().loc[['min', 'max']]\n\n\nmin    1.0\nmax    1.0\ndtype: float64\n\n\n\n\nCode\ndata.loc[:, 'EKRE':].sum(axis=0).plot(kind='bar', title='Party Choice', ylabel='Counts', color='black', alpha=0.6);\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.electoral_district.unique().size, data.unit.unique().size\n\n\n(12, 24)\n\n\n\n\nCode\nf, (left, right) = pp.subplots(ncols=2, figsize=(12, 6), sharey=True)\ndata.electoral_district.value_counts(normalize=True).plot(kind='bar', color=['black', 'darkgray'], ax=left)\ndata.unit.value_counts(normalize=True).plot(kind='bar', color=['darkgray', 'black'])\nleft.set_xticklabels(left.get_xticklabels(), rotation=90)\nf.tight_layout();\n\n\n\n\n\n\n\n\n\n\n\nCode\nf, (left, right) = pp.subplots(ncols=2, figsize=(10, 4), sharey=True)\ndata.age_group.value_counts(normalize=True, sort=False).plot(kind='bar', color=[f'C{i}' for i in range(7)], ax=left);\ndata.gender.value_counts(normalize=True).plot(kind='bar', color= ['C7', 'C8'], ax=right)\nleft.set(ylabel='Fraction', xlabel='age group')\nf.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.education.value_counts()\n\n\neducation\nSecondary education    2684\nHigher education       1948\nBasic education         574\nName: count, dtype: int64\n\n\n\n\nCode\nf, (left, right) = pp.subplots(ncols=2, figsize=(10, 4), sharey=True)\ndata.education.value_counts(normalize=True).plot(kind='bar', color=[f'C{i}' for i in range(0, 3)], ax=left)\ndata.nationality.value_counts(normalize=True).plot(kind='bar', color=['C3', 'C4'])\nleft.set_xticklabels(left.get_xticklabels(), rotation=50)\nf.tight_layout();\n\n\n\n\n\n\n\n\n\n\nPreliminary summary:\nThis table registers voter preference. Each row is a voter. Columns include voter demographics, the election district and unit. The rest of the columns are each dedicated to a party. Data are in the thousands with no apparent missing value. There are two types of data; (1) strings that lend themselves to categorization, and (2) binary data indicating whether a given party has received the vote of that row’s voter. There were no irregularities, i.e. more than 1 vote registered for each person; there were no abstentions noted in this dataset either. All the data is categorical with the following number of categories: * Party choice \\(→\\) 11 * Electoral district \\(\\rightarrow\\) 12 * (Electoral) Unit \\(→\\) 24 * AGe group \\(→\\) 7 * Gender \\(→\\) 2 * Education \\(→\\) 3 * Nationality \\(→\\) 2\n\n\nPreliminary summary, part 2:\nThere is some imbalance in education level, with secondary education being the more proeminent. There is notable imbalance in nationality, with predictable majority of Estonian representation. Party choice, electoral district and unit are also quite imbalanced. Next is to examine party representation breakdown at the group level. Note that I will not consider electoral district and unit hereafter as the emphasis is on demographics as a driver of party choice.\n\n\nCode\ndef make_group_percentage(df: pd.DataFrame, category: 'str') -&gt; pd.DataFrame:\n    \"\"\"Computes percentage for each column in a group\"\"\"\n    group = df.groupby(category).sum(numeric_only=True)\n    group_percent = group.div(group.sum(axis=1), axis=0)*100\n    return group_percent\n\n\n\n\nCode\nparty_names = data.loc[:, 'EKRE':].columns.tolist()\nparty_names\n\n\n['EKRE',\n 'Eesti 200',\n 'Undecided',\n 'Isamaa',\n 'Keskerakond',\n 'Mitte ükski erakond',\n 'Other',\n 'Parempoolsed',\n 'Reformierakond',\n 'Rohelised',\n 'SDE']\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nage_group\neducation\ngender\nnationality\nelectoral_district\nunit\nEKRE\nEesti 200\nUndecided\nIsamaa\nKeskerakond\nMitte ükski erakond\nOther\nParempoolsed\nReformierakond\nRohelised\nSDE\n\n\n\n\n0\n16-24\nBasic education\nMale\nEstonian\nHaabersti, Põhja-Tallinn ja Kristiine\nKristiine\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\nCode\ndata.groupby('gender').sum(\nnumeric_only=True\n)\n\n\n\n\n\n\n\n\n\nEKRE\nEesti 200\nUndecided\nIsamaa\nKeskerakond\nMitte ükski erakond\nOther\nParempoolsed\nReformierakond\nRohelised\nSDE\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFemale\n271\n237\n412\n119\n374\n507\n18\n7\n620\n55\n225\n\n\nMale\n379\n195\n251\n165\n247\n438\n23\n5\n500\n36\n122\n\n\n\n\n\n\n\n\n\nCode\nf, (left, right) = pp.subplots(ncols=2, figsize=(11, 4), sharey=True)\n#gender = data.groupby('gender').sum(numeric_only=True)\n#(gender.div(gender.sum(axis=1), axis=0)*100).plot(kind='bar', colormap='tab20c', ax=left, legend=False)\n#data.groupby('nationality').sum(numeric_only=True).plot(kind='bar', colormap='tab20c', ax=right, legend=False)\nmake_group_percentage(data, 'gender').plot(kind='bar', colormap='tab20c', ax=left, legend=False)\nmake_group_percentage(data, 'nationality').plot(kind='bar', colormap='tab20c', ax=right)\nleft.set(title='Gender', ylabel='Fraction(%)', xlim=(-0.3, 1.3), xlabel='')\nright.set(xlim=(-0.3, 1.3), title='Nationality', xlabel='')\nright.legend(frameon=False, loc=(0.34 ,0.04), fontsize=10)\nright.grid(axis='y', alpha=0.5, ls=':')\nleft.grid(axis='y', alpha=0.5, ls=':')\nf.tight_layout();\n\n\n\n\n\n\n\n\n\n\n\nCode\nf, (top, bottom) = pp.subplots(nrows=2, figsize=(12, 8))\n#data.groupby('age_group').sum(numeric_only=True)\nmake_group_percentage(data, 'age_group').plot(kind='bar', colormap='tab20c', ax=top, legend=False)\n#data.groupby('education').sum(numeric_only=True)\nmake_group_percentage(data, 'education').plot(kind='bar', colormap='tab20c', ax=bottom, legend=False, )\ntop.set(title='Age Group', ylabel='Fraction(%)', xlabel='')\ntop.set_xticklabels(top.get_xticklabels(), rotation=0)\nbottom.legend(frameon=False,  loc=(0.24, 0.04), fontsize=10)\nbottom.set_xticklabels(bottom.get_xticklabels(), rotation=0)\nbottom.set(ylabel='Fraction(%)', title='Education', xlim=(-0.3, 2.3), xlabel='')\nbottom.grid(axis='y', alpha=0.5, ls=':')\ntop.grid(axis='y', alpha=0.5, ls=':')\nf.tight_layout();\n\n\n\n\n\n\n\n\n\n\n\n\nGrouped data summary\n\nOverall\n\nReformierakond (a liberal party) is pretty popular across the board except with non-Estonians;\nNon-Estonians - I’m told - is almost entirely composed up of Russians;\nThere is a fair amount of undecided voters;\nEKRE and Keskeradon are populist parties - EKRE is right-wing, Keskerakond is center-left;\nParempoolsed, center-right party is the underdog across the board.\n\nGroup-specific\n\nGender: there are some subtle differences.\n\nReformierakond is slightly more popular with female voters;\nThere are more undecided among female voters;\nThe right-wing EKRE is more popular with male voters.\n\nNationality:\n\nReformierakond, EKRE are more popular with Estonians;\nKeskerakond, Mitte ukski erakond are favored by ethnic Russians who are also more numerous to be undecided.\n\nAge group:\n\nMitte ukski erakond and Reformierakond are the two prevaiing parties among each age group except the two older tranches;\nAmong 65 and over Reformierakond retains the top but Mitte recedes in favor of Keskeradond.\n\nEducation:\n\nEKRE, Undecided, Mitte ukski erakond are the top 3 party choices for those with a basic education;\nReformierakond and Mitte are the top 2 party choices among those with a college degree.\nHigh school graduates exhibit a preference for Reformierakond, Mitte, and EKRE.\n\n\n\n\n\n2. Modeling\nThe goal is to predict party preference on the basis of available demographic data. I will first start with looking at two categories; education and nationality. The below is some data encoding to ease model fitting and clarity of plotted results. In particular note the model coordinate names; ‘party_choice’, ‘education’, ‘nationality’, ‘obs_idx’. The latter is just the row index of the dataframe containing the observations; each row corresponding to one observation.\n\n\nCode\n# response variable encoding\nparty_choice = pd.Categorical(data[party_names].idxmax(axis=1), categories=party_names, ordered=True)\nparty_choice_label = party_choice.categories.to_list()\nparty_choice_code = party_choice.codes\n\n# input variables encoding\neducation_code, education_label = data.education.factorize(sort=True) \nnationality_code, nationality_label = data.nationality.factorize(sort=True)\n\n# setting model coordinates\n\ncoords = {\n    'party_choice': party_choice_label,\n    'education': education_label,\n    'nationality': nationality_label,\n    'obs_idx': data.index} # observation index, the row location of an observation in the dataframe.\n\n\nBelow I write a first model using a baseline intercept and I use nationality and education as predictors and baseline offsets. The corresponding coefficients have ‘party choice’ as dimension (11 possibilities) and in the case of education and nationality, have also a dimension of ‘education’ (3 possibilities) and ‘nationality’ (2 possibilities), respectively. These are linearly combined and passed to a softmax function to scale the output to the \\((0-1)\\) interval, as shown below. \\[ p = softmax(α_{baseline} + α_{nationality}[\\small{nationality\\_category}] + α_{education}[\\small{education\\_category}]) \\]\nAs prior distribution for all three coefficients, instead of using the \\(\\mathcal{Normal}\\) distribution, I use below the \\(\\mathcal{Zero-sum\\ Normal}\\). This distribution imposes zero sum constraints on specified axes of the fitted coefficients, thereby addressing the problem of overparameterization and identifiability that often plague categorical regression models; a subject of my blog post found here.\nFinally I use for likelihood the \\(\\mathcal{Categorical}\\) distribution, which is a n-dimensional \\(\\mathcal{Bernoulli}\\) process, and through which I expose the model to the data.\n\n\nCode\ndef build_model1(α_sigma=1, α_nat_sigma=1, α_edu_sigma=1):\n    with pm.Model(coords=coords) as model1:\n        # Data containers\n        party_choice_idx = pm.Data('party_choice_index', party_choice_code, dims='obs_idx')\n        nationality_idx = pm.Data('nationality_index', nationality_code, dims='obs_idx')\n        education_idx = pm.Data('eductation_index', education_code, dims='obs_idx')\n        \n        # Model priors\n        # baseline\n        α = pm.ZeroSumNormal('α', sigma=α_sigma, dims='party_choice') \n        # baseline offset due to nationality\n        α_nationality = pm.ZeroSumNormal('α_nationality', sigma=α_nat_sigma, dims=('nationality', 'party_choice'), n_zerosum_axes=2) \n        # baseline offset due to education\n        α_education = pm.ZeroSumNormal('α_education', sigma=α_edu_sigma, dims=('education', 'party_choice'), n_zerosum_axes=2)\n\n        # Link function for choice probability\n        p = pm.math.softmax(α + α_nationality[nationality_idx] + α_education[education_idx], axis=-1)\n        \n        # Likelihood\n        _ = pm.Categorical('y', p=p, observed=party_choice_idx, dims='obs_idx')\n    return model1\n\n\n\n\nCode\nmodel1 = build_model1()\n\n\nBelow is the model’s diagram with dimension information included.\n\n\nCode\nmodel1.to_graphviz()\n\n\n\n\n\n\n\n\n\nNext is to sample model priors to see the soundness of assumptions made\n\n\nCode\nwith model1:\n    idata1 = pm.sample_prior_predictive(model=model1)\n    idata1.extend(pm.sample(chains=4, draws=2000, random_seed=42))\n    idata1.extend(pm.sample_posterior_predictive(idata1))\n\n\nSampling: [y, α, α_education, α_nationality]\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, α_nationality, α_education]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 88 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\nCode\ndef plot_forward_samples(\n        idata: az.InferenceData, categories:list, categories_name:str, \n        figsize=(12, 6), plot_prior=True, plot_posterior=True):\n    \"\"\"A quick function to plot model predictives. \"\"\"\n    f, (left, right) = pp.subplots(1, 2, figsize=figsize, sharey=True)\n    xticks = [i + 0.5 for i in range(len(categories))]\n    if plot_prior:\n        az.plot_ppc(idata, group=\"prior\", ax=left)\n        left.set(\n            xticks=xticks,\n        )\n        left.set_xticklabels(categories, fontsize=10, rotation=30)\n        left.set_xlabel(categories_name, fontsize=16)\n        left.legend(frameon=True, fontsize=11)\n    else:\n        left.set_visible(False)\n\n    if plot_posterior:\n        az.plot_ppc(idata, ax=right)\n        right.set(\n            xticks=xticks,\n        )\n        right.set_xticklabels(categories, fontsize=10, rotation=30)\n        right.set_xlabel(categories_name, fontsize=16)\n        right.legend(frameon=True, fontsize=11)\n    else:\n        right.set_visible(False)\n    f.tight_layout()\n    return f, (left, right)\n\n\n\n\nCode\nmean_win = pd.Series(party_choice_code).value_counts(normalize=True).round(3).sort_index()\nf, (axl, axr) = plot_forward_samples(idata1, categories=party_choice_label, categories_name='party choice', figsize=(12, 6))\naxl.set_xticklabels(axl.get_xticklabels(), rotation=60);\naxl.bar(x=axl.get_xticks(), height=mean_win.values,  color='k', fill=False, lw=3, width=1, zorder=13, label='data');\naxr.set_xticklabels(axr.get_xticklabels(), rotation=60);\n\n\n\n\n\n\n\n\n\nLeft, in blue lines are sample outcomes for each party choice, before the model has seen the data. Right, model fit results (posterior distribution for each category). The model seems to have learned the posterior distribution well. The model did take some time to fit though and in spite of the lack of difergences, one wonders if sampling could be more efficient. Domain experts have suggested a tighter variance on the coefficient priors could indeed ease sampling. Model1b integrates this domain knowledge as follows:\n\n\nCode\nmodel1b = build_model1(α_sigma=0.5, α_edu_sigma=0.2, α_nat_sigma=0.3)\n\n\n\n\nCode\nwith model1b:\n    idata1b = pm.sample_prior_predictive()\n    idata1b.extend(pm.sample(chains=4, draws=2000, random_seed=42))\n    idata1b.extend(pm.sample_posterior_predictive(idata1b))\n\n\nSampling: [y, α, α_education, α_nationality]\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, α_nationality, α_education]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 78 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\nThere was some speed improvement (~ 18 seconds on a Macbook air M2.) #### Assessing goodness of fit\n\n\nCode\naz.plot_trace(idata1, backend_kwargs={'tight_layout': True});\n\n\n\n\n\n\n\n\n\n\n\nCode\naz.plot_trace(idata1b, backend_kwargs=dict(tight_layout=True));\n\n\n\n\n\n\n\n\n\nGood mixing in the chains and lack of divergences suggest a good fit. The second model (bottom panel) has tighter posteriors, suggesting a slightly better fit.\n\n\nCode\naz.summary(idata1, var_names='α')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα[EKRE]\n0.906\n0.072\n0.772\n1.042\n0.001\n0.001\n6612.0\n6756.0\n1.0\n\n\nα[Eesti 200]\n0.377\n0.087\n0.222\n0.549\n0.001\n0.001\n6715.0\n6112.0\n1.0\n\n\nα[Undecided]\n1.153\n0.067\n1.021\n1.277\n0.001\n0.001\n6441.0\n6223.0\n1.0\n\n\nα[Isamaa]\n-0.710\n0.160\n-1.000\n-0.405\n0.002\n0.001\n8356.0\n6113.0\n1.0\n\n\nα[Keskerakond]\n1.302\n0.063\n1.178\n1.415\n0.001\n0.001\n6106.0\n6320.0\n1.0\n\n\nα[Mitte ükski erakond]\n1.599\n0.059\n1.486\n1.709\n0.001\n0.001\n5842.0\n5910.0\n1.0\n\n\nα[Other]\n-1.575\n0.180\n-1.900\n-1.225\n0.002\n0.001\n10091.0\n6406.0\n1.0\n\n\nα[Parempoolsed]\n-2.961\n0.326\n-3.584\n-2.365\n0.004\n0.003\n5747.0\n6096.0\n1.0\n\n\nα[Reformierakond]\n0.716\n0.084\n0.552\n0.865\n0.001\n0.001\n8024.0\n6563.0\n1.0\n\n\nα[Rohelised]\n-1.054\n0.156\n-1.340\n-0.752\n0.002\n0.001\n8262.0\n6252.0\n1.0\n\n\nα[SDE]\n0.246\n0.093\n0.072\n0.419\n0.001\n0.001\n11979.0\n6935.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata1b, var_names='α')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα[EKRE]\n0.781\n0.064\n0.660\n0.899\n0.001\n0.000\n12336.0\n6688.0\n1.0\n\n\nα[Eesti 200]\n0.320\n0.073\n0.176\n0.452\n0.001\n0.001\n10479.0\n6316.0\n1.0\n\n\nα[Undecided]\n1.020\n0.058\n0.907\n1.126\n0.001\n0.000\n10735.0\n6572.0\n1.0\n\n\nα[Isamaa]\n-0.509\n0.109\n-0.723\n-0.310\n0.001\n0.001\n10705.0\n5825.0\n1.0\n\n\nα[Keskerakond]\n1.155\n0.055\n1.049\n1.253\n0.001\n0.000\n10455.0\n6907.0\n1.0\n\n\nα[Mitte ükski erakond]\n1.464\n0.051\n1.370\n1.563\n0.001\n0.000\n9276.0\n6427.0\n1.0\n\n\nα[Other]\n-1.609\n0.147\n-1.889\n-1.333\n0.001\n0.001\n12469.0\n6006.0\n1.0\n\n\nα[Parempoolsed]\n-2.461\n0.202\n-2.849\n-2.089\n0.002\n0.001\n10961.0\n5924.0\n1.0\n\n\nα[Reformierakond]\n0.698\n0.071\n0.563\n0.830\n0.001\n0.000\n10962.0\n6083.0\n1.0\n\n\nα[Rohelised]\n-1.065\n0.121\n-1.295\n-0.843\n0.001\n0.001\n10573.0\n6503.0\n1.0\n\n\nα[SDE]\n0.205\n0.075\n0.073\n0.357\n0.001\n0.001\n11252.0\n6818.0\n1.0\n\n\n\n\n\n\n\nLooking at the model fit summary tables for the baseline \\(α\\) parameter, in both cases the \\(\\hat{R}\\) (aka Gelman-Rubin statistic) is 1.0. This indicates good convergence of MCMC chains (I fired up 4 of them for sampling). Interestingly however, in the case of the second model with tighter variance constraints on the priors, the Effective Sample Size corresponding to the sampling of the central portion of the posterior (ess_bulk) shows more efficient sampling in most cases. The posterior is therefore likelier to be better characterized. A similar pattern can be observed in the next 4 tables for \\(α_{nationality}\\) and \\(α_{education}\\). Note that in contrast, the tails of the posterior are sampled with similar efficiency in both cases, as noted by ess_tail.\n\n\nCode\naz.summary(idata1, var_names='α_education')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_education[Basic education, EKRE]\n0.234\n0.088\n0.072\n0.400\n0.001\n0.001\n8337.0\n6406.0\n1.0\n\n\nα_education[Basic education, Eesti 200]\n-0.359\n0.125\n-0.602\n-0.134\n0.001\n0.001\n8978.0\n6077.0\n1.0\n\n\nα_education[Basic education, Undecided]\n0.110\n0.091\n-0.052\n0.290\n0.001\n0.001\n8579.0\n6639.0\n1.0\n\n\nα_education[Basic education, Isamaa]\n-0.110\n0.129\n-0.352\n0.129\n0.001\n0.001\n12807.0\n6272.0\n1.0\n\n\nα_education[Basic education, Keskerakond]\n0.385\n0.091\n0.212\n0.555\n0.001\n0.001\n7640.0\n6648.0\n1.0\n\n\nα_education[Basic education, Mitte ükski erakond]\n0.226\n0.082\n0.067\n0.376\n0.001\n0.001\n6905.0\n6441.0\n1.0\n\n\nα_education[Basic education, Other]\n0.148\n0.276\n-0.357\n0.665\n0.003\n0.003\n9166.0\n5883.0\n1.0\n\n\nα_education[Basic education, Parempoolsed]\n0.332\n0.382\n-0.402\n1.037\n0.004\n0.003\n9250.0\n6320.0\n1.0\n\n\nα_education[Basic education, Reformierakond]\n-0.486\n0.092\n-0.658\n-0.317\n0.001\n0.001\n9580.0\n6485.0\n1.0\n\n\nα_education[Basic education, Rohelised]\n-0.054\n0.205\n-0.446\n0.317\n0.002\n0.002\n10698.0\n6133.0\n1.0\n\n\nα_education[Basic education, SDE]\n-0.426\n0.144\n-0.696\n-0.155\n0.001\n0.001\n11192.0\n6408.0\n1.0\n\n\nα_education[Higher education, EKRE]\n-0.399\n0.072\n-0.530\n-0.259\n0.001\n0.001\n8977.0\n6374.0\n1.0\n\n\nα_education[Higher education, Eesti 200]\n0.348\n0.084\n0.200\n0.515\n0.001\n0.001\n8692.0\n5956.0\n1.0\n\n\nα_education[Higher education, Undecided]\n-0.216\n0.071\n-0.343\n-0.075\n0.001\n0.001\n8929.0\n6048.0\n1.0\n\n\nα_education[Higher education, Isamaa]\n-0.110\n0.098\n-0.292\n0.076\n0.001\n0.001\n13473.0\n6687.0\n1.0\n\n\nα_education[Higher education, Keskerakond]\n-0.347\n0.072\n-0.481\n-0.212\n0.001\n0.001\n8540.0\n6733.0\n1.0\n\n\nα_education[Higher education, Mitte ükski erakond]\n-0.278\n0.064\n-0.395\n-0.158\n0.001\n0.001\n7817.0\n6747.0\n1.0\n\n\nα_education[Higher education, Other]\n-0.259\n0.219\n-0.655\n0.164\n0.002\n0.002\n10544.0\n6202.0\n1.0\n\n\nα_education[Higher education, Parempoolsed]\n0.414\n0.304\n-0.160\n0.981\n0.003\n0.003\n8511.0\n6058.0\n1.0\n\n\nα_education[Higher education, Reformierakond]\n0.397\n0.064\n0.279\n0.516\n0.001\n0.000\n8557.0\n6525.0\n1.0\n\n\nα_education[Higher education, Rohelised]\n0.021\n0.152\n-0.253\n0.314\n0.001\n0.002\n11264.0\n5934.0\n1.0\n\n\nα_education[Higher education, SDE]\n0.428\n0.094\n0.258\n0.610\n0.001\n0.001\n11307.0\n6711.0\n1.0\n\n\nα_education[Secondary education, EKRE]\n0.164\n0.067\n0.039\n0.290\n0.001\n0.000\n9386.0\n6782.0\n1.0\n\n\nα_education[Secondary education, Eesti 200]\n0.011\n0.085\n-0.150\n0.169\n0.001\n0.001\n11057.0\n6734.0\n1.0\n\n\nα_education[Secondary education, Undecided]\n0.106\n0.069\n-0.025\n0.234\n0.001\n0.001\n10110.0\n6960.0\n1.0\n\n\nα_education[Secondary education, Isamaa]\n0.220\n0.092\n0.041\n0.388\n0.001\n0.001\n13016.0\n6300.0\n1.0\n\n\nα_education[Secondary education, Keskerakond]\n-0.038\n0.070\n-0.167\n0.094\n0.001\n0.001\n10080.0\n6738.0\n1.0\n\n\nα_education[Secondary education, Mitte ükski erakond]\n0.052\n0.062\n-0.063\n0.170\n0.001\n0.001\n9273.0\n6817.0\n1.0\n\n\nα_education[Secondary education, Other]\n0.111\n0.201\n-0.277\n0.476\n0.002\n0.002\n11753.0\n6409.0\n1.0\n\n\nα_education[Secondary education, Parempoolsed]\n-0.745\n0.350\n-1.389\n-0.074\n0.004\n0.003\n9203.0\n6561.0\n1.0\n\n\nα_education[Secondary education, Reformierakond]\n0.088\n0.065\n-0.038\n0.207\n0.001\n0.001\n9345.0\n7035.0\n1.0\n\n\nα_education[Secondary education, Rohelised]\n0.033\n0.144\n-0.235\n0.302\n0.001\n0.002\n15300.0\n6318.0\n1.0\n\n\nα_education[Secondary education, SDE]\n-0.002\n0.097\n-0.187\n0.176\n0.001\n0.001\n9201.0\n7201.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata1b, var_names='α_education')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_education[Basic education, EKRE]\n0.191\n0.070\n0.055\n0.320\n0.001\n0.000\n13539.0\n6375.0\n1.0\n\n\nα_education[Basic education, Eesti 200]\n-0.235\n0.090\n-0.400\n-0.062\n0.001\n0.001\n11990.0\n7072.0\n1.0\n\n\nα_education[Basic education, Undecided]\n0.083\n0.073\n-0.060\n0.214\n0.001\n0.001\n12012.0\n6297.0\n1.0\n\n\nα_education[Basic education, Isamaa]\n-0.050\n0.093\n-0.231\n0.118\n0.001\n0.001\n13607.0\n6170.0\n1.0\n\n\nα_education[Basic education, Keskerakond]\n0.295\n0.074\n0.156\n0.434\n0.001\n0.000\n12671.0\n6260.0\n1.0\n\n\nα_education[Basic education, Mitte ükski erakond]\n0.189\n0.066\n0.060\n0.309\n0.001\n0.000\n12424.0\n6586.0\n1.0\n\n\nα_education[Basic education, Other]\n0.065\n0.135\n-0.184\n0.323\n0.001\n0.001\n13651.0\n6458.0\n1.0\n\n\nα_education[Basic education, Parempoolsed]\n0.097\n0.144\n-0.174\n0.365\n0.001\n0.001\n13581.0\n5944.0\n1.0\n\n\nα_education[Basic education, Reformierakond]\n-0.393\n0.070\n-0.517\n-0.256\n0.001\n0.000\n11899.0\n6095.0\n1.0\n\n\nα_education[Basic education, Rohelised]\n0.009\n0.122\n-0.223\n0.228\n0.001\n0.001\n13491.0\n6665.0\n1.0\n\n\nα_education[Basic education, SDE]\n-0.251\n0.097\n-0.426\n-0.063\n0.001\n0.001\n11271.0\n6692.0\n1.0\n\n\nα_education[Higher education, EKRE]\n-0.318\n0.059\n-0.430\n-0.208\n0.000\n0.000\n14386.0\n6470.0\n1.0\n\n\nα_education[Higher education, Eesti 200]\n0.297\n0.065\n0.177\n0.419\n0.001\n0.000\n12723.0\n5482.0\n1.0\n\n\nα_education[Higher education, Undecided]\n-0.155\n0.057\n-0.261\n-0.047\n0.001\n0.000\n10573.0\n6068.0\n1.0\n\n\nα_education[Higher education, Isamaa]\n-0.091\n0.076\n-0.235\n0.051\n0.001\n0.001\n14480.0\n6157.0\n1.0\n\n\nα_education[Higher education, Keskerakond]\n-0.249\n0.060\n-0.367\n-0.139\n0.001\n0.000\n12318.0\n6111.0\n1.0\n\n\nα_education[Higher education, Mitte ükski erakond]\n-0.214\n0.053\n-0.316\n-0.116\n0.000\n0.000\n11808.0\n6247.0\n1.0\n\n\nα_education[Higher education, Other]\n-0.080\n0.121\n-0.308\n0.149\n0.001\n0.001\n15069.0\n6587.0\n1.0\n\n\nα_education[Higher education, Parempoolsed]\n0.080\n0.137\n-0.178\n0.335\n0.001\n0.001\n12957.0\n6326.0\n1.0\n\n\nα_education[Higher education, Reformierakond]\n0.373\n0.050\n0.276\n0.462\n0.000\n0.000\n11685.0\n6693.0\n1.0\n\n\nα_education[Higher education, Rohelised]\n0.014\n0.105\n-0.181\n0.212\n0.001\n0.001\n16191.0\n6541.0\n1.0\n\n\nα_education[Higher education, SDE]\n0.344\n0.071\n0.214\n0.479\n0.001\n0.000\n10646.0\n5829.0\n1.0\n\n\nα_education[Secondary education, EKRE]\n0.127\n0.053\n0.031\n0.229\n0.000\n0.000\n12694.0\n6755.0\n1.0\n\n\nα_education[Secondary education, Eesti 200]\n-0.062\n0.065\n-0.185\n0.060\n0.001\n0.001\n11771.0\n6722.0\n1.0\n\n\nα_education[Secondary education, Undecided]\n0.072\n0.054\n-0.027\n0.177\n0.000\n0.000\n12113.0\n7153.0\n1.0\n\n\nα_education[Secondary education, Isamaa]\n0.140\n0.072\n0.007\n0.277\n0.001\n0.000\n14944.0\n6205.0\n1.0\n\n\nα_education[Secondary education, Keskerakond]\n-0.046\n0.055\n-0.144\n0.062\n0.000\n0.000\n12197.0\n6874.0\n1.0\n\n\nα_education[Secondary education, Mitte ükski erakond]\n0.025\n0.049\n-0.068\n0.115\n0.000\n0.000\n12274.0\n6695.0\n1.0\n\n\nα_education[Secondary education, Other]\n0.015\n0.119\n-0.205\n0.239\n0.001\n0.001\n14454.0\n6886.0\n1.0\n\n\nα_education[Secondary education, Parempoolsed]\n-0.177\n0.136\n-0.437\n0.075\n0.001\n0.001\n14120.0\n6326.0\n1.0\n\n\nα_education[Secondary education, Reformierakond]\n0.020\n0.050\n-0.073\n0.112\n0.000\n0.000\n12133.0\n7066.0\n1.0\n\n\nα_education[Secondary education, Rohelised]\n-0.023\n0.100\n-0.209\n0.162\n0.001\n0.001\n13259.0\n5827.0\n1.0\n\n\nα_education[Secondary education, SDE]\n-0.092\n0.070\n-0.223\n0.041\n0.001\n0.001\n11114.0\n6683.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata1, var_names='α_nationality')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_nationality[Estonian, EKRE]\n0.091\n0.064\n-0.031\n0.210\n0.001\n0.001\n7281.0\n6421.0\n1.0\n\n\nα_nationality[Estonian, Eesti 200]\n0.081\n0.072\n-0.059\n0.213\n0.001\n0.001\n8736.0\n6470.0\n1.0\n\n\nα_nationality[Estonian, Undecided]\n-0.287\n0.058\n-0.394\n-0.182\n0.001\n0.001\n6742.0\n6367.0\n1.0\n\n\nα_nationality[Estonian, Isamaa]\n0.935\n0.151\n0.660\n1.228\n0.002\n0.001\n8963.0\n5827.0\n1.0\n\n\nα_nationality[Estonian, Keskerakond]\n-0.791\n0.056\n-0.888\n-0.677\n0.001\n0.000\n6695.0\n6262.0\n1.0\n\n\nα_nationality[Estonian, Mitte ükski erakond]\n-0.422\n0.052\n-0.520\n-0.327\n0.001\n0.000\n6279.0\n6162.0\n1.0\n\n\nα_nationality[Estonian, Other]\n-0.407\n0.155\n-0.703\n-0.120\n0.001\n0.001\n12316.0\n6225.0\n1.0\n\n\nα_nationality[Estonian, Parempoolsed]\n0.095\n0.292\n-0.427\n0.655\n0.004\n0.003\n5923.0\n5253.0\n1.0\n\n\nα_nationality[Estonian, Reformierakond]\n0.780\n0.076\n0.639\n0.921\n0.001\n0.001\n8745.0\n6390.0\n1.0\n\n\nα_nationality[Estonian, Rohelised]\n0.039\n0.135\n-0.209\n0.290\n0.001\n0.001\n10870.0\n6037.0\n1.0\n\n\nα_nationality[Estonian, SDE]\n-0.113\n0.070\n-0.236\n0.024\n0.001\n0.001\n10342.0\n6071.0\n1.0\n\n\nα_nationality[Other, EKRE]\n-0.091\n0.064\n-0.210\n0.031\n0.001\n0.001\n7281.0\n6421.0\n1.0\n\n\nα_nationality[Other, Eesti 200]\n-0.081\n0.072\n-0.213\n0.059\n0.001\n0.001\n8736.0\n6470.0\n1.0\n\n\nα_nationality[Other, Undecided]\n0.287\n0.058\n0.182\n0.394\n0.001\n0.001\n6742.0\n6367.0\n1.0\n\n\nα_nationality[Other, Isamaa]\n-0.935\n0.151\n-1.228\n-0.660\n0.002\n0.001\n8963.0\n5827.0\n1.0\n\n\nα_nationality[Other, Keskerakond]\n0.791\n0.056\n0.677\n0.888\n0.001\n0.000\n6695.0\n6262.0\n1.0\n\n\nα_nationality[Other, Mitte ükski erakond]\n0.422\n0.052\n0.327\n0.520\n0.001\n0.000\n6279.0\n6162.0\n1.0\n\n\nα_nationality[Other, Other]\n0.407\n0.155\n0.120\n0.703\n0.001\n0.001\n12316.0\n6225.0\n1.0\n\n\nα_nationality[Other, Parempoolsed]\n-0.095\n0.292\n-0.655\n0.427\n0.004\n0.003\n5923.0\n5253.0\n1.0\n\n\nα_nationality[Other, Reformierakond]\n-0.780\n0.076\n-0.921\n-0.639\n0.001\n0.001\n8745.0\n6390.0\n1.0\n\n\nα_nationality[Other, Rohelised]\n-0.039\n0.135\n-0.290\n0.209\n0.001\n0.001\n10870.0\n6037.0\n1.0\n\n\nα_nationality[Other, SDE]\n0.113\n0.070\n-0.024\n0.236\n0.001\n0.001\n10342.0\n6071.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata1b, var_names='α_nationality')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_nationality[Estonian, EKRE]\n0.144\n0.057\n0.039\n0.251\n0.001\n0.000\n12637.0\n6667.0\n1.0\n\n\nα_nationality[Estonian, Eesti 200]\n0.112\n0.062\n-0.006\n0.229\n0.001\n0.000\n11945.0\n6603.0\n1.0\n\n\nα_nationality[Estonian, Undecided]\n-0.223\n0.049\n-0.315\n-0.136\n0.000\n0.000\n11503.0\n7039.0\n1.0\n\n\nα_nationality[Estonian, Isamaa]\n0.669\n0.101\n0.476\n0.854\n0.001\n0.001\n11064.0\n6258.0\n1.0\n\n\nα_nationality[Estonian, Keskerakond]\n-0.707\n0.046\n-0.795\n-0.621\n0.000\n0.000\n11305.0\n6511.0\n1.0\n\n\nα_nationality[Estonian, Mitte ükski erakond]\n-0.357\n0.043\n-0.439\n-0.277\n0.000\n0.000\n10679.0\n6792.0\n1.0\n\n\nα_nationality[Estonian, Other]\n-0.260\n0.121\n-0.489\n-0.035\n0.001\n0.001\n12611.0\n6075.0\n1.0\n\n\nα_nationality[Estonian, Parempoolsed]\n-0.091\n0.153\n-0.378\n0.192\n0.001\n0.001\n11387.0\n6724.0\n1.0\n\n\nα_nationality[Estonian, Reformierakond]\n0.758\n0.065\n0.630\n0.875\n0.001\n0.000\n12215.0\n6441.0\n1.0\n\n\nα_nationality[Estonian, Rohelised]\n0.026\n0.105\n-0.156\n0.236\n0.001\n0.001\n12021.0\n6444.0\n1.0\n\n\nα_nationality[Estonian, SDE]\n-0.071\n0.062\n-0.186\n0.043\n0.001\n0.000\n11030.0\n6307.0\n1.0\n\n\nα_nationality[Other, EKRE]\n-0.144\n0.057\n-0.251\n-0.039\n0.001\n0.000\n12637.0\n6667.0\n1.0\n\n\nα_nationality[Other, Eesti 200]\n-0.112\n0.062\n-0.229\n0.006\n0.001\n0.000\n11945.0\n6603.0\n1.0\n\n\nα_nationality[Other, Undecided]\n0.223\n0.049\n0.136\n0.315\n0.000\n0.000\n11503.0\n7039.0\n1.0\n\n\nα_nationality[Other, Isamaa]\n-0.669\n0.101\n-0.854\n-0.476\n0.001\n0.001\n11064.0\n6258.0\n1.0\n\n\nα_nationality[Other, Keskerakond]\n0.707\n0.046\n0.621\n0.795\n0.000\n0.000\n11305.0\n6511.0\n1.0\n\n\nα_nationality[Other, Mitte ükski erakond]\n0.357\n0.043\n0.277\n0.439\n0.000\n0.000\n10679.0\n6792.0\n1.0\n\n\nα_nationality[Other, Other]\n0.260\n0.121\n0.035\n0.489\n0.001\n0.001\n12611.0\n6075.0\n1.0\n\n\nα_nationality[Other, Parempoolsed]\n0.091\n0.153\n-0.192\n0.378\n0.001\n0.001\n11387.0\n6724.0\n1.0\n\n\nα_nationality[Other, Reformierakond]\n-0.758\n0.065\n-0.875\n-0.630\n0.001\n0.000\n12215.0\n6441.0\n1.0\n\n\nα_nationality[Other, Rohelised]\n-0.026\n0.105\n-0.236\n0.156\n0.001\n0.001\n12021.0\n6444.0\n1.0\n\n\nα_nationality[Other, SDE]\n0.071\n0.062\n-0.043\n0.186\n0.001\n0.000\n11030.0\n6307.0\n1.0\n\n\n\n\n\n\n\nGiven the above, I hereafter drop the first model with the more naive priors and continue with the second model with the more informative priors.\nNow to check the impact of each predictor on voter preference by examining the posterior distribution, its central tendency and spread and how it relates to the reference value of 0, which corresponds to to effect. Note that: * The orange number situates probabilistically the reference value (0=neither favorable nor unfavorable.) * The black line corresponds to the 94% Highest Density Interval (HDI) - the narrowest interval containing 94% of the posterior distribution. * The choice of 94% is a reminder that there is nothing magical about this number. Analysts should determine the appropriate size of the credibility interval that works best for their use case.\n* This corresponds to a proabilistic statement; i.e. there is a 94% chance that given this data and this model, a voter correponding to this category would have an an offset within the HDI.\n\nBaseline - average voter tendencies\n\nOn average there is a overwhelmingly positive outlook on EKRE, Eesti, Mitte, Reformierakond, and to a lesser extent, SDE. The average voter has also a high probability of being Undecided.\nOn the other hand, Isamaa, Parempoolsed, Rohelised and smaller parties grouped under the catch-all Other are not likely to be appealing to the average voter.\n\n\n\n\nCode\naz.plot_posterior(idata1b, var_names='α', ref_val=0, textsize=16);\n\n\n\n\n\n\n\n\n\n\nEducation:\n\nBasic Education:\n\nThere is a very high probability that a voter belonging ot this category will be in favor of EKRE, Keskerakond, or Mitte\nThere is a very high probability against such a voter favoring Reformierakond, and to a lesser extent, SDE.\n\nSecondary Education:\n\nVoter have high probability of favoring EKRE and to a lesser extent Isamaa\nThough not quite significant, voter are likely to be most hostile to SDE.\n\nHigher Education:\n\nVoters in this category are very hostile to EKRE, Mitte and Keskerakond and are unlikely to be undecided\nVoters are quite sympathetic to Eesti and Reformierakond.\n\n\n\n\n\nCode\naz.plot_posterior(idata1b, var_names='α_education', ref_val=0, backend_kwargs=dict(tight_layout=True), textsize=20);\n\n\n\n\n\n\n\n\n\n\nNationality\n\n\nEstonians are:\n\npredominantly favorable to EKRE, Isamaa, Reformierakond;\nlargely unfavorable to Keskerakond and Mitte, and are less likely to be Undecided.\n\nEthnic Russians (Other) on the other hand:\n\nfavor Keskerakond, Mitte, and are more likely to be Undecided;\nare hostile to Reformierakond and to a lesser extent EKRE.\n\n\n\n\nCode\nf, axs = pp.subplots(ncols=3, nrows=8, figsize=(15, 40))\nfor ax in axs.ravel()[-2:]:\n    ax.set_visible(False)\naz.plot_posterior(idata1b, var_names='α_nationality', backend_kwargs=dict(tight_layout=True), ref_val=0, ax=axs, textsize=10);\n\n\n\n\n\n\n\n\n\n\n\nAdding gender and age group\nIn accordance with the Bayesian workflow, the next step is to build a slightly more complex model, in this case by adding gender and vote age group as predictors to nationality and education, and see if can extract further insight. The model type is the same, just with a total of 4 offsets to the baseline instead of 2. I will also use stronger informative priors as I did in the second model that I ended up using for my analysis. Finally, the prior distribution for all coefficients is again the ZeroSumNormal distribution that I use to avoid non-identifiability due to overparameterization. But first, to encode the additional predictors…\n\n\nCode\ngender_code, gender_label = data.gender.factorize(sort=True)\nage_gp_code, age_group_label = data.age_group.factorize(sort=True)\n\n\n\n\nCode\ncoords = {\n    'age group': age_group_label,\n    'education': education_label,\n    'gender': gender_label,\n    'nationality': nationality_label,\n    'party choice': party_choice_label,\n    'obs_idx': data.index\n}\n\n\n\n\nCode\ndef build_model2(coords, σ_baseline=1, σ_age=1, σ_edu=1, σ_gen=1, σ_nat=1):\n    with pm.Model(coords=coords) as model2:\n        age_gp_idx = pm.Data('age_gp', age_gp_code, dims='obs_idx')\n        educ_idx = pm.Data('education', education_code, dims='obs_idx')\n        gender_idx = pm.Data('gender', gender_code, dims='obs_idx')\n        nat_idx = pm.Data('nationality', nationality_code, dims='obs_idx')\n        party_choice_idx = pm.Data('party choice', party_choice_code, dims='obs_idx') \n        \n        α_baseline = pm.ZeroSumNormal('α_baseline', sigma=σ_baseline, dims='party choice')\n        α_age_group = pm.ZeroSumNormal('α_age_group', sigma=σ_age, dims=('age group', 'party choice'), n_zerosum_axes=2)\n        α_education = pm.ZeroSumNormal('α_education', sigma=σ_edu, dims=('education', 'party choice'), n_zerosum_axes=2)\n        α_gender = pm.ZeroSumNormal('α_gender', sigma=σ_gen, dims=('gender', 'party choice'), n_zerosum_axes=2)\n        α_nationality = pm.ZeroSumNormal('α_nationality', sigma=σ_nat, dims=('nationality', 'party choice'), n_zerosum_axes=2)\n\n        μ = α_baseline + α_age_group[age_gp_idx] + α_education[educ_idx] + α_gender[gender_idx] + α_nationality[nat_idx]\n        p = pm.math.softmax(μ, axis=-1)\n\n        _ = pm.Categorical('y', p=p, observed=party_choice_idx, dims='obs_idx')\n        \n    return model2\n\nmodel2 = build_model2(coords=coords, σ_baseline=0.3, σ_age=0.4, σ_edu=0.3, σ_gen=0.2, σ_nat=0.2)\nmodel2.to_graphviz()\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith model2:\n    idata2 = pm.sample_prior_predictive()\n    idata2.extend(pm.sample(chains=4, draws=2000, random_seed=42))\n    idata2.extend(pm.sample_posterior_predictive(idata2))\n\n\nSampling: [y, α_age_group, α_baseline, α_education, α_gender, α_nationality]\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α_baseline, α_age_group, α_education, α_gender, α_nationality]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 126 seconds.\nSampling: [y, α_age_group, α_baseline, α_education, α_gender, α_nationality]\n\n\n\n\n\n\n\n\n\n\nCode\naz.plot_trace(idata2, backend_kwargs=dict(tight_layout=True));\n\n\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata2, var_names='α_baseline')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_baseline[EKRE]\n0.713\n0.061\n0.598\n0.826\n0.001\n0.000\n13176.0\n6608.0\n1.0\n\n\nα_baseline[Eesti 200]\n0.168\n0.074\n0.030\n0.305\n0.001\n0.000\n12135.0\n6429.0\n1.0\n\n\nα_baseline[Undecided]\n0.882\n0.059\n0.772\n0.995\n0.001\n0.000\n13437.0\n6458.0\n1.0\n\n\nα_baseline[Isamaa]\n-0.416\n0.093\n-0.586\n-0.239\n0.001\n0.001\n13360.0\n6345.0\n1.0\n\n\nα_baseline[Keskerakond]\n1.044\n0.056\n0.944\n1.157\n0.000\n0.000\n14123.0\n6809.0\n1.0\n\n\nα_baseline[Mitte ükski erakond]\n1.358\n0.051\n1.260\n1.448\n0.000\n0.000\n12378.0\n7131.0\n1.0\n\n\nα_baseline[Other]\n-1.457\n0.129\n-1.695\n-1.215\n0.001\n0.001\n13204.0\n6783.0\n1.0\n\n\nα_baseline[Parempoolsed]\n-1.982\n0.151\n-2.258\n-1.695\n0.001\n0.001\n14895.0\n6244.0\n1.0\n\n\nα_baseline[Reformierakond]\n0.674\n0.066\n0.549\n0.797\n0.001\n0.000\n13091.0\n6590.0\n1.0\n\n\nα_baseline[Rohelised]\n-1.073\n0.113\n-1.290\n-0.866\n0.001\n0.001\n13350.0\n6760.0\n1.0\n\n\nα_baseline[SDE]\n0.089\n0.078\n-0.052\n0.237\n0.001\n0.001\n11862.0\n6514.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata2, var_names='α_age_group')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_age_group[16-24, EKRE]\n-0.307\n0.122\n-0.537\n-0.080\n0.001\n0.001\n16900.0\n5979.0\n1.0\n\n\nα_age_group[16-24, Eesti 200]\n0.465\n0.121\n0.240\n0.696\n0.001\n0.001\n16699.0\n6892.0\n1.0\n\n\nα_age_group[16-24, Undecided]\n0.011\n0.114\n-0.205\n0.225\n0.001\n0.001\n14880.0\n5789.0\n1.0\n\n\nα_age_group[16-24, Isamaa]\n-0.379\n0.167\n-0.684\n-0.070\n0.001\n0.001\n17991.0\n5771.0\n1.0\n\n\nα_age_group[16-24, Keskerakond]\n-0.685\n0.150\n-0.959\n-0.400\n0.001\n0.001\n17828.0\n5346.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nα_age_group[75+, Other]\n-0.044\n0.252\n-0.509\n0.438\n0.002\n0.003\n16116.0\n5912.0\n1.0\n\n\nα_age_group[75+, Parempoolsed]\n0.156\n0.279\n-0.350\n0.715\n0.002\n0.003\n17245.0\n6678.0\n1.0\n\n\nα_age_group[75+, Reformierakond]\n0.209\n0.087\n0.052\n0.378\n0.001\n0.001\n14009.0\n6339.0\n1.0\n\n\nα_age_group[75+, Rohelised]\n-0.412\n0.234\n-0.838\n0.033\n0.002\n0.002\n14955.0\n6332.0\n1.0\n\n\nα_age_group[75+, SDE]\n-0.017\n0.138\n-0.287\n0.228\n0.001\n0.002\n14714.0\n5968.0\n1.0\n\n\n\n\n77 rows × 9 columns\n\n\n\n\n\nCode\naz.summary(idata2, var_names='α_education')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_education[Basic education, EKRE]\n0.208\n0.076\n0.068\n0.353\n0.001\n0.000\n14723.0\n6608.0\n1.0\n\n\nα_education[Basic education, Eesti 200]\n-0.371\n0.102\n-0.553\n-0.172\n0.001\n0.001\n12680.0\n6619.0\n1.0\n\n\nα_education[Basic education, Undecided]\n0.053\n0.080\n-0.096\n0.203\n0.001\n0.001\n15074.0\n7250.0\n1.0\n\n\nα_education[Basic education, Isamaa]\n-0.031\n0.107\n-0.235\n0.164\n0.001\n0.001\n14127.0\n6301.0\n1.0\n\n\nα_education[Basic education, Keskerakond]\n0.321\n0.084\n0.165\n0.476\n0.001\n0.000\n15304.0\n6390.0\n1.0\n\n\nα_education[Basic education, Mitte ükski erakond]\n0.175\n0.069\n0.044\n0.305\n0.001\n0.000\n14289.0\n6947.0\n1.0\n\n\nα_education[Basic education, Other]\n0.187\n0.166\n-0.128\n0.491\n0.001\n0.001\n16358.0\n5678.0\n1.0\n\n\nα_education[Basic education, Parempoolsed]\n0.304\n0.177\n-0.022\n0.640\n0.001\n0.001\n16022.0\n6039.0\n1.0\n\n\nα_education[Basic education, Reformierakond]\n-0.466\n0.077\n-0.623\n-0.336\n0.001\n0.000\n15343.0\n6863.0\n1.0\n\n\nα_education[Basic education, Rohelised]\n0.001\n0.143\n-0.281\n0.261\n0.001\n0.002\n17370.0\n6133.0\n1.0\n\n\nα_education[Basic education, SDE]\n-0.381\n0.115\n-0.588\n-0.158\n0.001\n0.001\n11402.0\n6367.0\n1.0\n\n\nα_education[Higher education, EKRE]\n-0.336\n0.064\n-0.453\n-0.214\n0.000\n0.000\n16515.0\n6292.0\n1.0\n\n\nα_education[Higher education, Eesti 200]\n0.365\n0.073\n0.230\n0.508\n0.001\n0.000\n13045.0\n6551.0\n1.0\n\n\nα_education[Higher education, Undecided]\n-0.173\n0.063\n-0.291\n-0.055\n0.001\n0.000\n15164.0\n6155.0\n1.0\n\n\nα_education[Higher education, Isamaa]\n-0.104\n0.084\n-0.255\n0.054\n0.001\n0.001\n16875.0\n6313.0\n1.0\n\n\nα_education[Higher education, Keskerakond]\n-0.280\n0.066\n-0.407\n-0.161\n0.001\n0.000\n15162.0\n6066.0\n1.0\n\n\nα_education[Higher education, Mitte ükski erakond]\n-0.231\n0.056\n-0.339\n-0.129\n0.000\n0.000\n14425.0\n6429.0\n1.0\n\n\nα_education[Higher education, Other]\n-0.145\n0.143\n-0.417\n0.121\n0.001\n0.001\n17730.0\n5713.0\n1.0\n\n\nα_education[Higher education, Parempoolsed]\n0.050\n0.157\n-0.249\n0.353\n0.001\n0.002\n16159.0\n6270.0\n1.0\n\n\nα_education[Higher education, Reformierakond]\n0.400\n0.055\n0.302\n0.507\n0.000\n0.000\n14947.0\n6182.0\n1.0\n\n\nα_education[Higher education, Rohelised]\n0.029\n0.119\n-0.198\n0.255\n0.001\n0.001\n18364.0\n5719.0\n1.0\n\n\nα_education[Higher education, SDE]\n0.425\n0.081\n0.272\n0.577\n0.001\n0.001\n11128.0\n6702.0\n1.0\n\n\nα_education[Secondary education, EKRE]\n0.128\n0.057\n0.027\n0.242\n0.001\n0.000\n12401.0\n6724.0\n1.0\n\n\nα_education[Secondary education, Eesti 200]\n0.006\n0.072\n-0.125\n0.144\n0.001\n0.001\n11841.0\n6781.0\n1.0\n\n\nα_education[Secondary education, Undecided]\n0.121\n0.058\n0.010\n0.227\n0.001\n0.000\n12417.0\n7297.0\n1.0\n\n\nα_education[Secondary education, Isamaa]\n0.135\n0.080\n-0.013\n0.285\n0.001\n0.001\n12403.0\n6082.0\n1.0\n\n\nα_education[Secondary education, Keskerakond]\n-0.041\n0.060\n-0.155\n0.065\n0.001\n0.001\n13487.0\n6578.0\n1.0\n\n\nα_education[Secondary education, Mitte ükski erakond]\n0.056\n0.051\n-0.039\n0.150\n0.000\n0.000\n12410.0\n7802.0\n1.0\n\n\nα_education[Secondary education, Other]\n-0.042\n0.135\n-0.291\n0.218\n0.001\n0.001\n16671.0\n6445.0\n1.0\n\n\nα_education[Secondary education, Parempoolsed]\n-0.355\n0.159\n-0.647\n-0.057\n0.001\n0.001\n14857.0\n6236.0\n1.0\n\n\nα_education[Secondary education, Reformierakond]\n0.067\n0.053\n-0.036\n0.165\n0.000\n0.000\n12447.0\n6660.0\n1.0\n\n\nα_education[Secondary education, Rohelised]\n-0.030\n0.110\n-0.240\n0.172\n0.001\n0.001\n14568.0\n6149.0\n1.0\n\n\nα_education[Secondary education, SDE]\n-0.044\n0.079\n-0.185\n0.112\n0.001\n0.001\n10521.0\n6345.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata2, var_names='α_gender')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_gender[Female, EKRE]\n-0.228\n0.041\n-0.308\n-0.157\n0.000\n0.000\n17338.0\n6441.0\n1.0\n\n\nα_gender[Female, Eesti 200]\n0.031\n0.047\n-0.057\n0.118\n0.000\n0.000\n16743.0\n6071.0\n1.0\n\n\nα_gender[Female, Undecided]\n0.178\n0.041\n0.101\n0.253\n0.000\n0.000\n15502.0\n6196.0\n1.0\n\n\nα_gender[Female, Isamaa]\n-0.209\n0.053\n-0.306\n-0.110\n0.000\n0.000\n18282.0\n5863.0\n1.0\n\n\nα_gender[Female, Keskerakond]\n0.059\n0.043\n-0.023\n0.139\n0.000\n0.000\n17925.0\n6789.0\n1.0\n\n\nα_gender[Female, Mitte ükski erakond]\n0.005\n0.036\n-0.062\n0.072\n0.000\n0.000\n13068.0\n5882.0\n1.0\n\n\nα_gender[Female, Other]\n-0.089\n0.092\n-0.259\n0.088\n0.001\n0.001\n15853.0\n5529.0\n1.0\n\n\nα_gender[Female, Parempoolsed]\n-0.018\n0.101\n-0.206\n0.175\n0.001\n0.001\n17983.0\n6359.0\n1.0\n\n\nα_gender[Female, Reformierakond]\n0.009\n0.035\n-0.056\n0.074\n0.000\n0.000\n14718.0\n6247.0\n1.0\n\n\nα_gender[Female, Rohelised]\n0.087\n0.078\n-0.056\n0.237\n0.001\n0.001\n17240.0\n6054.0\n1.0\n\n\nα_gender[Female, SDE]\n0.174\n0.050\n0.082\n0.270\n0.000\n0.000\n12087.0\n6810.0\n1.0\n\n\nα_gender[Male, EKRE]\n0.228\n0.041\n0.157\n0.308\n0.000\n0.000\n17338.0\n6441.0\n1.0\n\n\nα_gender[Male, Eesti 200]\n-0.031\n0.047\n-0.118\n0.057\n0.000\n0.000\n16743.0\n6071.0\n1.0\n\n\nα_gender[Male, Undecided]\n-0.178\n0.041\n-0.253\n-0.101\n0.000\n0.000\n15502.0\n6196.0\n1.0\n\n\nα_gender[Male, Isamaa]\n0.209\n0.053\n0.110\n0.306\n0.000\n0.000\n18282.0\n5863.0\n1.0\n\n\nα_gender[Male, Keskerakond]\n-0.059\n0.043\n-0.139\n0.023\n0.000\n0.000\n17925.0\n6789.0\n1.0\n\n\nα_gender[Male, Mitte ükski erakond]\n-0.005\n0.036\n-0.072\n0.062\n0.000\n0.000\n13068.0\n5882.0\n1.0\n\n\nα_gender[Male, Other]\n0.089\n0.092\n-0.088\n0.259\n0.001\n0.001\n15853.0\n5529.0\n1.0\n\n\nα_gender[Male, Parempoolsed]\n0.018\n0.101\n-0.175\n0.206\n0.001\n0.001\n17983.0\n6359.0\n1.0\n\n\nα_gender[Male, Reformierakond]\n-0.009\n0.035\n-0.074\n0.056\n0.000\n0.000\n14718.0\n6247.0\n1.0\n\n\nα_gender[Male, Rohelised]\n-0.087\n0.078\n-0.237\n0.056\n0.001\n0.001\n17240.0\n6054.0\n1.0\n\n\nα_gender[Male, SDE]\n-0.174\n0.050\n-0.270\n-0.082\n0.000\n0.000\n12087.0\n6810.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.summary(idata2, var_names='α_nationality')\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_nationality[Estonian, EKRE]\n0.169\n0.053\n0.073\n0.271\n0.000\n0.000\n14414.0\n6430.0\n1.0\n\n\nα_nationality[Estonian, Eesti 200]\n0.130\n0.057\n0.018\n0.233\n0.000\n0.000\n15715.0\n7377.0\n1.0\n\n\nα_nationality[Estonian, Undecided]\n-0.159\n0.046\n-0.244\n-0.072\n0.000\n0.000\n14413.0\n7107.0\n1.0\n\n\nα_nationality[Estonian, Isamaa]\n0.518\n0.079\n0.366\n0.660\n0.001\n0.000\n13569.0\n6383.0\n1.0\n\n\nα_nationality[Estonian, Keskerakond]\n-0.675\n0.045\n-0.756\n-0.590\n0.000\n0.000\n14815.0\n6751.0\n1.0\n\n\nα_nationality[Estonian, Mitte ükski erakond]\n-0.294\n0.040\n-0.373\n-0.222\n0.000\n0.000\n13450.0\n6685.0\n1.0\n\n\nα_nationality[Estonian, Other]\n-0.225\n0.095\n-0.400\n-0.044\n0.001\n0.001\n16229.0\n5764.0\n1.0\n\n\nα_nationality[Estonian, Parempoolsed]\n-0.145\n0.106\n-0.340\n0.052\n0.001\n0.001\n15378.0\n6900.0\n1.0\n\n\nα_nationality[Estonian, Reformierakond]\n0.730\n0.057\n0.625\n0.837\n0.000\n0.000\n13500.0\n6581.0\n1.0\n\n\nα_nationality[Estonian, Rohelised]\n-0.015\n0.088\n-0.183\n0.145\n0.001\n0.001\n14598.0\n6308.0\n1.0\n\n\nα_nationality[Estonian, SDE]\n-0.033\n0.058\n-0.140\n0.080\n0.001\n0.001\n11830.0\n6737.0\n1.0\n\n\nα_nationality[Other, EKRE]\n-0.169\n0.053\n-0.271\n-0.073\n0.000\n0.000\n14414.0\n6430.0\n1.0\n\n\nα_nationality[Other, Eesti 200]\n-0.130\n0.057\n-0.233\n-0.018\n0.000\n0.000\n15715.0\n7377.0\n1.0\n\n\nα_nationality[Other, Undecided]\n0.159\n0.046\n0.072\n0.244\n0.000\n0.000\n14413.0\n7107.0\n1.0\n\n\nα_nationality[Other, Isamaa]\n-0.518\n0.079\n-0.660\n-0.366\n0.001\n0.000\n13569.0\n6383.0\n1.0\n\n\nα_nationality[Other, Keskerakond]\n0.675\n0.045\n0.590\n0.756\n0.000\n0.000\n14815.0\n6751.0\n1.0\n\n\nα_nationality[Other, Mitte ükski erakond]\n0.294\n0.040\n0.222\n0.373\n0.000\n0.000\n13450.0\n6685.0\n1.0\n\n\nα_nationality[Other, Other]\n0.225\n0.095\n0.044\n0.400\n0.001\n0.001\n16229.0\n5764.0\n1.0\n\n\nα_nationality[Other, Parempoolsed]\n0.145\n0.106\n-0.052\n0.340\n0.001\n0.001\n15378.0\n6900.0\n1.0\n\n\nα_nationality[Other, Reformierakond]\n-0.730\n0.057\n-0.837\n-0.625\n0.000\n0.000\n13500.0\n6581.0\n1.0\n\n\nα_nationality[Other, Rohelised]\n0.015\n0.088\n-0.145\n0.183\n0.001\n0.001\n14598.0\n6308.0\n1.0\n\n\nα_nationality[Other, SDE]\n0.033\n0.058\n-0.080\n0.140\n0.001\n0.001\n11830.0\n6737.0\n1.0\n\n\n\n\n\n\n\nThe fit took predictably longer; 2 minutes and 20 seconds on my macbook air m2. Moreover, * There were no divergences; * Posteriors look sound and the traces exhibit good mixing for all chains; * All \\(\\hat{R}\\) values were 1 * Bulk and tail ESS are about the same as the model with two predictors and relatively informative priors suggesting sampling efficiency has not deteriorated as a result of the increased model complexity.\n\n\nCode\nf, (axl, axr) = plot_forward_samples(idata2, categories=party_choice_label, categories_name='party choice', figsize=(12, 6))\naxl.set_xticklabels(axl.get_xticklabels(), rotation=60);\naxl.bar(x=axl.get_xticks(), height=mean_win.values,  color='k', fill=False, lw=3, width=1, zorder=13, label='data');\naxr.set_xticklabels(axr.get_xticklabels(), rotation=60);"
  },
  {
    "objectID": "blog/bayesian_ode/npz1986_blog.html",
    "href": "blog/bayesian_ode/npz1986_blog.html",
    "title": "A Bayesian Approach to Marine Modeling",
    "section": "",
    "text": "This is a summary of a first installment in a series that aims to demonstrate the use of Bayesian inference for marine modeling. Here, I demonstrate how parameters of a system of ordinary differential equations (ODE) used in a nitrogen-phytoplankton-zooplankton (NPZ) model can be recovered from synthetic data. These are created from the NPZ model using fixed parameters, the equations of which are as follows\n\\[\n\\begin{alignat}{3}\n&\\frac{dN}{dt} = -μ \\frac{N}{k_N + N} P + m_PP + m_ZZ \\\\\n&\\frac{dP}{dt} = μ \\frac{N}{k_N+N} P - g_{max} \\frac{P^2}{k_P^2 + P^2} Z - m_PP \\\\\n&\\frac{dZ}{dt} = τ g_{max} \\frac{P^2}{k_P^2 + P^2} Z - m_ZZ \\\\\n\\end{alignat}\n\\]\nThe above is the original mechanistic framework of Franks et al. (1986), which represents the dynamics of nutrients (N), phytoplankton (P), and zooplankton (Z). To recover the parameters of the simulation, I use a combination of informative priors sourced from the litterature an adaptive version of the Hamiltonian Monte Carlo (HMC), the No U-Turn Sampler (NUTS). The resulting high-dimensional posterior distribution is a rich probabilistic construct that can be mined for parameter estimation along with estimation uncertainty. The following represents the first few steps of a principled Bayesian workflow; cf. Gelman et al., 2020.\n\nPrior elicitation, model building.\nPrior predictive checks: aimed at verifying modeling assumptions before data collection occurs;\nData collection: simulated by running the NPZ model to account of 10 days of activity following which the signals of each compartments were then homoscedasticly noised up;\nSampler setting and model fitting;\nMarginalization and examination of each parameter’s posterior distribution to assess whether and how well the “true” parameters were recovered - recovery is deemed successful if the true value falls within a pre-specified level of significance;\nPosterior predictive checks: aimed at checking model fits and associated uncertainties with comparison with simulated data.\n\nA note on significance - unlike in the classical paradigm, there isn’t a pre-ordained conventional level of significane. Significance here is represented as a region within the posterior distribution known as the Highest Density Interval (HDI). HDI and other uncertainty tools are touched upon in the actual study referenced at the end.\nThe figures below summarize some of the process.\n\n\n\nFigure 1: Prior Predictive Time Series with 94% HDI. This is one of the steps in checking assumption validity in the model structure and prior choices.\n\n\n\n\n\nFigure 2: Simulated data. Top: mean field signal. Bottom: mean with compartment-wise homoscedastic noise added. This is the data from which the parameters will be recovered.\n\n\n\n\n\nFigure 3: Marginal posterior distributions for each model parameters. The orange line shows the “true” values used to generate the synthetic data. The probability of the “true value” to be within the posterior is also indicated in orange. The success criterion is for the orange line to fall within the region depicted by the black HDI line. While almost all parameters are found to be withing this HDI, one is outside and a few are close to the edges of the HDI line. This indicates potential problems and suggests revisions of the model.\n\n\n\n\n\nFigure 4: The mean modeled time series for each compartment, N, P, Z along with 94% HDI. The observed (synthetic) data is overlaid for reference. This may look adequate but as indicated by the probability of\n\n\nThis was a short demonstration of how to infer model parameters of a NPZ model given some data. The Bayesian workflow includes more steps than were shown here. In particular, more than one models are typically built with increasing complexity. The resulting high dimensional posterior distributions are rich constructs than can be mined for far greater insights than provided by alternative frameworks. Information theoretic and probabilistic machine learning approaches can then be leveraged to decide on the better model. Thus a model is not falsified per se, rather the difference in skill between models can be quantitatively measured. I will demonstrate this in a subsequent entry. One should remember however that model performance is conditional on the data observed and the model structure itself.\nThe code for this effort can be found here. For a deeper dive visit this portfolio entry."
  },
  {
    "objectID": "blog/stats_ab_testing/index.html",
    "href": "blog/stats_ab_testing/index.html",
    "title": "On Statistical Practices in A/B Testing",
    "section": "",
    "text": "I just finished watching Dr. Ronny Kohavi’s “Ultimate Guide to A/B Testing” “Ultimate Guide to A/B Testing”. This was a lengthy presentation made enjoyable by his enthusiasm and wealth of practical experience. Dr Kohavi has had extensive experience leading A/B testing efforts in companies like Bing, Microsoft, Airbnb. During this seminar, he touched on the statistical approach and the use of p-values as a decision mechanism in A/B testing. P-values are an essential tool in Null Hypothesis Testing and other frequentist inference efforts. In my view, this potentially consequential decision support approach is the wrong framework, not just for this use case, but in fact most use cases. Rare are the instances where this approach is required. Below I justify this thinking and propose a better framework.\nProblems arising from the use of p-values - The p-value is often described as the probability of observing data that is as extreme or more extreme than what is on hand, given the null hypothesis is true. This is a subtle but crucial point. The P in p-value refers to a conditional probability about the data, not the hypothesis itself. This leads to several issues:\n\nSampling vs. Inference: P-values provide a sampling probability, not an inference probability. We’re interested in the probability that our hypothesis is true, given the data, not the other way around. This is not conducive to drawing definitive conclusions about whether variant A or B is truly better.\nP-hacking Vulnerability: The reliance on p-values can encourage “p-hacking”, where researchers manipulate data or analyses to achieve statistical significance, leading to unreliable results at best.\nCommon Misconception: As Dr. Kohavi correctly notes, many people mistakenly interpret (1-p) as the chance the alternative hypothesis is true. This misunderstanding is widespread and problematic.\n\nThe Importance of Priors - Dr. Kohavi touches on the concept of a “Prior”, in a nod to Bayes’ theorem, and says it is the way to get a probabilistic statement about the null hypothesis. However, he states, “we don’t know the prior.” This is incorrect. The Prior, to be thorough, the prior probability, is what we do know before seeing the data; it is the embodiment of our existing knowledge and assumption. In addition to the role they play in inference, priors are important even before running the model and fitting the data because they exposing modeling assumptions for all to see and critique. Once the model is run and the data is fit the resulting posterior distribution is a rich construct that provides greater understanding of the data, and potentially the data generation process. I also think it is a better underpinning for causal analysis, which is what A/B testing is - I’ll get into that in another post. For the A/B testing scenario specifically though Bayesian analysis can provide actual answers to the product manager’s questions.\nBayesian answers - Essentially, by using Bayesian statistics we can answer the question “what is the probability that B is better than A?” Contrast this with the question the frequentist p-value does answer; i.e. “what is the probability of the observed (or more extreme) data given that A and B are the same?”. Finally it renders unnecessary the dependence on an arbitrary significance level of 0.05 that hails from the obscurantism of the early 20th century. In essence, while A/B testing based on frequentist methods has been widely used, it’s essential to recognize its limitations. The frequentist approach, with its reliance on p-values, can lead to misinterpretations and unreliable conclusion, and has led to considerable damage in the social, political, judicial, and now with the awareness raised on the crisis of replication, scientific arenas.\nEspecially in today’s data-driven world, the Bayesian approach provides a more robust and informative alternative. Yes, it demands more effort, but its consistent framework stands in stark contrast to the hodgepodge of case-dependent recipes inherent in traditional statistics, a complexity that often proves difficult even for classically trained statisticians.\nI will provide an example of doing A/B testing properly in a next post. In the meantime, I would love to hear thoughts and experiences with A/B testing and statistical methods! Feel free to reach out if you’d like to dive deeper into these concepts or need help with your data.\nBe well!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "Machine Learning Researcher – Probabilistic Programmer – All-around Bayesian Enthusiast!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html",
    "href": "blog/causailty_in_observation_data/index.html",
    "title": "Causality in observational data",
    "section": "",
    "text": "Preamble\nIn the real world, we often want to understand if something (a new medicine, a marketing campaign, a policy change) truly causes an outcome. In ideal experiments (like Randomized Control Trials or RCTs), we can control all factors, making it easier to determine cause and effect.\nBut what if we only have observational data, where things weren’t neatly controlled? Establishing a causal link becomes tricky. There might be hidden factors, called confounders, that influence both the treatment and the outcome, making it hard to tell if the treatment is the real cause. This post explores ways to tackle this challenge.\nExample: Does Exercise Cause Weight Loss?\nImagine a study on the impact of a new exercise program on weight loss. Participants with higher fitness levels might be more likely to choose the program. This becomes a confounder – fitness level influences both program participation and weight loss outcomes. Propensity score matching can help – it pairs individuals with similar fitness levels but different program participation, isolating the program’s true effect.\nUsing the Lalonde Dataset\nTo illustrate how matching and propensity scores can help with causal analysis, I’ll use a dataset from a study by Lalonde (1986) evaluating an employment and training program. The study aimed to understand if the program truly caused better employment outcomes. Let’s load the data from the ‘Matching’ R library and take a closer look:\nlibrary(MatchIt)\ndata(lalonde)\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html#matching",
    "href": "blog/causailty_in_observation_data/index.html#matching",
    "title": "Causality in observational data",
    "section": "Matching",
    "text": "Matching\n\nTo match or not to match?\nA first step is whether the data on hand is appropriate for causal inferrence, in particular, whether it should be balanced. A commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;-c(\"hispan\", \"black\", \"white\", \"age\", \"educ\", \"married\", \"nodegree\", \n              \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata, test=FALSE)\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        429               185                 \n  hispan (mean (SD))      0.14 (0.35)       0.06 (0.24)     0.277\n  black (mean (SD))       0.20 (0.40)       0.84 (0.36)     1.668\n  white (mean (SD))       0.66 (0.48)       0.10 (0.30)     1.406\n  age (mean (SD))        28.03 (10.79)     25.82 (7.16)     0.242\n  educ (mean (SD))       10.24 (2.86)      10.35 (2.01)     0.045\n  married (mean (SD))     0.51 (0.50)       0.19 (0.39)     0.719\n  nodegree (mean (SD))    0.60 (0.49)       0.71 (0.46)     0.235\n  re74 (mean (SD))     5619.24 (6788.75) 2095.57 (4886.62)  0.596\n  re75 (mean (SD))     2466.48 (3292.00) 1532.06 (3219.25)  0.287\n\n\nNote that an alternative would be to conduct two-tailed t-tests to assess difference between group (treated and control) means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size. I therefore use \\(SMD\\) in this post.\nBy convention, an \\(SMD\\) greater than 0.1 suggest an imbalance with respect to the corresponding covariate. Here \\(SMD&gt;0.1\\) for all covariates except education. Treated subjects need each to be match via greedy matching to as close as possible a control subject. Matching between subjects is done on the basis of a distance metric indicating how separated they are in the covariate space. The specific metric used in this case is the Mahalanobis distance, which is a kind of standardized difference, computed as follows: \\[d = \\sqrt{(X_i-X_j)^T C^{-1} (X_i-X_j) }\\] where \\(X\\) is a covariate, \\(i\\) and \\(j\\) are treated and control subjects, and \\(C\\) is the covariance matrix\n\nlibrary(Matching)\n# Below M=1 refers to pairwise matching. Even so if \"ties\" is left as TRUE (default)\n# multiple subjects within the tolerance threshold will all be matched. \n# In this case, e.g. not setting ties=TRUE yields 207 pairs, even though there are only # 185 treated subjects.\ngreedymatch&lt;-Match(Tr=treat, M=1, X=mydata[xvars], ties=FALSE) \ngreedymatched&lt;-mydata[unlist(greedymatch[c(\"index.treated\", \"index.control\")]), ]\n\nI create another Table 1 with the matched data check the SMDs.\n\nmatchedtab1&lt;-CreateTableOne(vars=xvars, strata=\"treat\", data=greedymatched, test=FALSE)\nprint(matchedtab1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.06 (0.24)       0.06 (0.24)    &lt;0.001\n  black (mean (SD))       0.84 (0.37)       0.84 (0.36)     0.015\n  white (mean (SD))       0.10 (0.30)       0.10 (0.30)     0.018\n  age (mean (SD))        25.34 (8.30)      25.82 (7.16)     0.062\n  educ (mean (SD))       10.45 (1.96)      10.35 (2.01)     0.054\n  married (mean (SD))     0.19 (0.39)       0.19 (0.39)    &lt;0.001\n  nodegree (mean (SD))    0.71 (0.46)       0.71 (0.46)    &lt;0.001\n  re74 (mean (SD))     2159.92 (4240.18) 2095.57 (4886.62)  0.014\n  re75 (mean (SD))     1119.08 (2442.29) 1532.06 (3219.25)  0.145\n\n\nGreedy pairwise matching yields, as expected, a reduced data set with 185 subjects in each group. This time all but the variable \\(re75\\) have corresponding \\(SMD&lt;0.1\\). This is not entirely satisfactory and I will attempt to balance the data set using propensity scores next"
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html#propensity-score-matching",
    "href": "blog/causailty_in_observation_data/index.html#propensity-score-matching",
    "title": "Causality in observational data",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nA propensity score denoted here \\(\\pi\\) is defined as the probability that a subject \\(i\\) received treatment conditioned on the covariates \\(X\\). I.e. \\(\\pi_i = P(T=1|X_i)\\). A \\(\\pi_i=0.4\\) means there’s a 40% chance the corresponding subject will receive treatment. Covariates can increase or decrease the probability of receiving treatment. For example, if \\(X\\), simplistically the only covariate, is a boolean variable for smoking and smokers are more likely to get a particular treatment then \\(P(T=1|X=1) \\gt P(T=1|X=0)\\).\nInterestingly, two subjects may have the same propensity score in spite of having different covariate values \\(X\\), meaning they are equally likely to receive treatment. Thus reducing the data to a subset of subjects with the same \\(\\pi\\) should balance the treated and control groups. In doing so a crucial assumption in causality, ignorability i.e. how a subject ended in one or the other group can be safely ignored. In a randomized trial, the propensity score is known. In an observational study \\(\\pi\\) is unknown. However because both \\(X\\) and \\(T\\) are collected, \\(\\pi\\) can be estimated. For this I will fit a logistic regression, where the covariates are the input and the treatment variable is the output. Using this model I can get the predicted probability of treatment for each subject; i.e. the estimated \\(\\pi\\).\n\npsmodel &lt;- glm(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75,family=binomial(), data=mydata)\n\nThe model fit is summarized below.\n\n# show fit summary\nsummary(psmodel)\n\n\nCall:\nglm(formula = treat ~ hispan + white + black + age + educ + married + \n    nodegree + re74 + re75, family = binomial(), data = mydata)\n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.663e+00  9.709e-01  -1.713  0.08668 .  \nhispan      -2.082e+00  3.672e-01  -5.669 1.44e-08 ***\nwhite       -3.065e+00  2.865e-01 -10.699  &lt; 2e-16 ***\nblack               NA         NA      NA       NA    \nage          1.578e-02  1.358e-02   1.162  0.24521    \neduc         1.613e-01  6.513e-02   2.477  0.01325 *  \nmarried     -8.321e-01  2.903e-01  -2.866  0.00415 ** \nnodegree     7.073e-01  3.377e-01   2.095  0.03620 *  \nre74        -7.178e-05  2.875e-05  -2.497  0.01253 *  \nre75         5.345e-05  4.635e-05   1.153  0.24884    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 751.49  on 613  degrees of freedom\nResidual deviance: 487.84  on 605  degrees of freedom\nAIC: 505.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nNext I extract the propensity scores from the model object.\n\n# create propensity score\npscore&lt;-psmodel$fitted.values\n\nFinally, I use the MatchIt package to match subjects based on their propensity scores. Note that I set a seed for reproducibility, since matching randomizes data as a first step.\n\nset.seed(42)\nm.out&lt;-matchit(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75, data=mydata, method=\"nearest\")\n\nThe matching results are summarized below.\n\n# summarize the matching outcome\nsummary(m.out)\n\n\nCall:\nmatchit(formula = treat ~ hispan + white + black + age + educ + \n    married + nodegree + re74 + re75, data = mydata, method = \"nearest\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.1822          1.7941     0.9211    0.3774\nhispan          0.0595        0.1422         -0.3498          .    0.0827\nwhite           0.0973        0.6550         -1.8819          .    0.5577\nblack           0.8432        0.2028          1.7615          .    0.6404\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\ndistance   0.6444\nhispan     0.0827\nwhite      0.5577\nblack      0.6404\nage        0.1577\neduc       0.1114\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.3629          0.9739     0.7566    0.1321\nhispan          0.0595        0.2162         -0.6629          .    0.1568\nwhite           0.0973        0.3135         -0.7296          .    0.2162\nblack           0.8432        0.4703          1.0259          .    0.3730\nage            25.8162       25.3027          0.0718     0.4568    0.0847\neduc           10.3459       10.6054         -0.1290     0.5721    0.0239\nmarried         0.1892        0.2108         -0.0552          .    0.0216\nnodegree        0.7081        0.6378          0.1546          .    0.0703\nre74         2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75         1532.0553     1614.7451         -0.0257     1.4956    0.0452\n         eCDF Max Std. Pair Dist.\ndistance   0.4216          0.9740\nhispan     0.1568          1.0743\nwhite      0.2162          0.8390\nblack      0.3730          1.0259\nage        0.2541          1.3938\neduc       0.0757          1.2474\nmarried    0.0216          0.8281\nnodegree   0.0703          1.0106\nre74       0.2757          0.7965\nre75       0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nThe above is more intuitively approached with some plotting as shown below.\n\n# propensity score plots\nplot(m.out, type=\"hist\")\n\n\n\n\n\n\n\n\nWhat I am looking for with the plot above is an improvement in the overlap between the distribution of propensity scores of matched control and treated groups, relative to the raw. There is obviously some improvement and I’ll next check more concretely below how good the match is using SMD.\n\n# --&gt; MATCHHING WITH & WITHOUT A CALIPER\n# Matching without a caliper\n# -&gt; do greedy matching on logit (PS)\nset.seed(42)\npsmatch &lt;- Match(Tr=mydata$treat, M=1, X=pscore, replace=FALSE)\nps_matched &lt;- mydata[unlist(psmatch[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_pscore &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=ps_matched,\n                              test=FALSE)\nprint(matchedtab1_pscore, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.21 (0.41)       0.06 (0.24)     0.453\n  black (mean (SD))       0.47 (0.50)       0.84 (0.36)     0.852\n  white (mean (SD))       0.32 (0.47)       0.10 (0.30)     0.566\n  age (mean (SD))        25.34 (10.53)     25.82 (7.16)     0.053\n  educ (mean (SD))       10.59 (2.63)      10.35 (2.01)     0.106\n  married (mean (SD))     0.21 (0.41)       0.19 (0.39)     0.041\n  nodegree (mean (SD))    0.64 (0.48)       0.71 (0.46)     0.139\n  re74 (mean (SD))     2455.47 (4352.86) 2095.57 (4886.62)  0.078\n  re75 (mean (SD))     1731.45 (2813.88) 1532.06 (3219.25)  0.066\n\n\nThe table above shows marked improvement relative to the raw data. However, variables like education (\\(educ\\)) and lack of high school degree (\\(nodegree\\)) are marginal, while race-related variables (\\(hispan\\), \\(black\\), \\(white\\)) are still too high to safely avoid confounding. Overall it is quite a bit worse than the first attempt with greedy matching I did earlier using the Mahalanobis distance; though \\(re75\\) is markedly better here.\n\nMatching with a caliper\nOne way to try to improve on the matching is to use a caliper; i.e. a threshold (maximum) distance beyond which matching is not allowed. In practice, though somewhat arbitrarily, (1) the propensity scores are logit-transformed, (2) the standard deviation (SD) is calculated, and (3) the caliper is set to 0.2 times the SD, finally (4) the matching is performed subject to the caliper. A smaller caliper, which results in fewer but better pairs, trades more variance for less bias. The code below performs the aforementioned steps.\n\nset.seed(42)\nlogit_pscore = qlogis(pscore)\npsmatch_calip &lt;-Match(Tr=mydata$treat, M=1, X=logit_pscore, replace=FALSE,\n                caliper=.2)\n# Note that the caliper is in St.Dev. units\nmatched_calip &lt;- mydata[unlist(psmatch_calip[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_calip &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=matched_calip,\n                              test=FALSE)\nprint(matchedtab1_calip, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        114               114                 \n  hispan (mean (SD))      0.11 (0.32)       0.10 (0.30)     0.057\n  black (mean (SD))       0.73 (0.45)       0.75 (0.44)     0.040\n  white (mean (SD))       0.16 (0.37)       0.16 (0.37)    &lt;0.001\n  age (mean (SD))        26.35 (10.83)     25.82 (6.93)     0.059\n  educ (mean (SD))       10.53 (2.63)      10.30 (2.29)     0.092\n  married (mean (SD))     0.26 (0.44)       0.24 (0.43)     0.061\n  nodegree (mean (SD))    0.61 (0.49)       0.64 (0.48)     0.054\n  re74 (mean (SD))     2858.97 (4816.95) 2168.29 (5590.28)  0.132\n  re75 (mean (SD))     1969.43 (3027.44) 1053.23 (2597.71)  0.325\n\n# NOTE the smaller number of subjects for each treatment categories, resultng\n# from dropping previously matched subjects.\n\nUsing the caliper has reduced the number of matched pairs down to 114. The high SMDs seen previously (without the caliper). However, for 1974 and 1975 real incomes \\(SMD &gt; 0.1\\). Thus I cannot be certain that the treatment is the only cause of the outcome. The table above suggests subjects’ earning history is still a causal factor. With this in mind, I next run an outcome analysis for all the approaches described previously. I do this at the end rather than after each outcome as a habit to prevent p-hacking.\n\n\nOutcome analyses:\nTo analyze whether the difference in outcome between the treatment and the control groups are different, I run a paired t-test on the various matched data. But first a quick function to avoid some repetition.\n\n# function that accepts a matched data table and runs the paired t-test.\nrun_matched_ttest&lt;- function(matched_table){\n  # get outcome data for both groups\n  treated_outcome &lt;- matched_table$outcome[matched_table$treat==1]\n  control_outcome &lt;- matched_table$outcome[matched_table$treat==0]\n  \n  # compute pairwise difference between both groups\n  diff_outcome &lt;- treated_outcome - control_outcome\n  \n  # paired t-test\n  t.test(diff_outcome)\n}\n\n\\(\\rightarrow\\) Greedy Mahalanobis distance matching:\n\nrun_matched_ttest(greedymatched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.12179, df = 184, p-value = 0.9032\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1276.271  1444.203\nsample estimates:\nmean of x \n 83.96634 \n\n\n\\(\\rightarrow\\) Propensity score matching (no caliper)\n\nrun_matched_ttest(ps_matched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.94827, df = 184, p-value = 0.3442\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -708.6934 2020.4023\nsample estimates:\nmean of x \n 655.8544 \n\n\n\\(\\rightarrow\\) Propensity score matching with caliper\n\nrun_matched_ttest(matched_calip)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 1.2773, df = 113, p-value = 0.2041\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -607.6641 2812.9263\nsample estimates:\nmean of x \n 1102.631 \n\n\nThus, from greedy Mahalanobis distance matching to propensity score matching with caliper, the statistical significance of the difference between groups does increase, with propensity score matching with caliper resulting the in the smallest p-value (0.20). This is still conventionally quite high, so that it is impossible to reject the null hypothesis, i.e. that there are no difference between the groups. Moreover, there is still a potential confounding problem in all cases. E.g. the SMD for real income in `74 and `75 remains above 0.1 suggesting I was not able to remove the confounding effect. Note though that I worked for a subset of the original data. Thus I would next re-run the analysis on the entire data set, which could lead to better matching. But that is a story for another day.\nPeaceful coding!\n\n\n\n\nimage source"
  },
  {
    "objectID": "blog/zerosumnormal/on_zero_sum_normal.html",
    "href": "blog/zerosumnormal/on_zero_sum_normal.html",
    "title": "The ZeroSumNormal Distribution",
    "section": "",
    "text": "Overview\nCategorical Regression is a powerful tool when dealing with outcomes that fall seveeral distinct categories. This can be seen as an extension of the Logistic Regression where the goal is the prediction of one of two possible outcomes. An example of categorical regression is voter preference prediction in multi-party elections. This is closely related to Frequentist Multivariate testing in marketing, A/B/n testing. Regardless of the school of thought one subscribes to or the field one operates in categorical regression models can suffer from non-identifiability issues due to overparameterization.\n\n\nNon-identifiability and Overparameterization\nIn categorical regression, each category is typically assigned its own set of parameter; e.g., intercepts or coefficients for each level of a predictor. Without proper constraints, these parameters can play off of each other, their values can shift collectively in a way that leaves the model’s predictions unchanged. This is analogous to making relative measurements without defined axes; you can measure the distance between different points but without a fixed reference, the absolute positions are arbitrary. In other words, the model ends up making relative measurements wihtout an establised baseline, leading to redundancy and ambiguity in parameter estimates. There are two common strategies to address the problem; Pivoting and the ZerosumNormal distibution.\nPivoting involves using one of the categories as a reference as a reference or baseline so that the effects of the remaining categories are measured relative to this reference. Pivoting is useful in scenarios where there is a natural or meaningful control group - for example, in A/B/n testing where one version, usually A, is the baseline, oft referred to as the control group.\nZeroSumNormal Distibution is an approach that imposes a zero-sum constraint on the parameters, effectively anchoring them by ensuring that their sum equal zero. This constraint allows for relative comparisions without arbitrarily selecting one category as the baseline; particularly beneficial in settings where all categories are of equal interest.\nThis post focuses on the ZeroSumNormal approach. In a future post, I will disucss pivoting and its applications.\n\n\nThe ZeroSumnormal Dsitribbution: How it Works\nIF you’re a Bayesian modeller like me you’re in luck - and if not it’s never too late to convert. The ZeroSumNormal distribution is a Bayesian prior designed to mitigate overparameterization and non-identifiability by enforcing a zero-sum constraint on the parameters. This means that the sum of the effects across all categories is forced to equal zero, anchoring the estimates in a way that ensures they are uniquely identifiable.\nConsider a model that predicts product choice baed on demographic predictors such as age, eduation, nationality, etc. Without constraints the model might assign an arbitrary set of intercepts or coefficients to each category, making the parameter estimates mabiguous. The ZeroSumNormal prior anchors these enstimates, ensuring that while the differences between categories remain informative, their absolute values are constrained to a common scale.\nBelow is a code snippet that illustrates the use of the ZeroSumNormal prior by doing the following: * It defines a simple model where \\(zs\\) is a parameter drawn from a \\(ZeroSumNormal\\) distribution over 5 categories. * It sets up the coordinates for the category dimension * It draws prior predictive samples (a process used to assess modeling assumptions before fitting data) * Finally, it computes the sum of the samples across the \\(category\\) axis.\n\n\nCode\nimport pymc as pm\nimport matplotlib.pyplot as pp\n\n# Define the dimensions and a simple model to sample from ZeroSumNormal\nwith pm.Model() as model:\n    # For demonstration, assume we have 5 categories.\n    # The ZeroSumNormal is defined over the 'category' dimension.\n    categories = ['A', 'B', 'C', 'D', 'E']\n    # Set up coordinates so PyMC knows about the dimension.\n    model.add_coord(\"category\", categories)\n    zs = pm.ZeroSumNormal(\"zs\", sigma=1, dims=\"category\")\n    \n    trace = pm.sample_prior_predictive(1000)\n\n\nSampling: [zs]\n\n\nNow I can gather the results of the prior predictive sampling, sum the samples across the categories and plot their distribution. The resulting histogram shows that the sum are centered very close to zero; the zero-sum constraint.\n\n\nCode\n# Extract samples and verify that they sum to nearly zero across categories.\nsamples = trace.prior[\"zs\"].values  # shape: (n_samples, n_categories)\nsums = samples.sum(axis=1)\n\n# Plot the distribution of the sums\nf, ax = pp.subplots(figsize=(8, 4))\nax.hist(sums, bins=100, edgecolor=\"k\", alpha=0.7)\nax.axvline(\n    sums.sum(), color='red', linestyle='--', label=r'$Σ_{categories}$ = ' + f'{sums.sum():.4f}')\nax.set(xlabel=\"Categories\", ylabel=\"Frequency\", title=\"Distribution of ZeroSumNormal Samples\", ylim=(0, 1.1))\nax.legend(fontsize=12, loc='best')\nfor s, cat in zip(sums.ravel(), categories):\n    ax.text(s, 1.05, cat)\n\n\n\n\n\n\n\n\n\n\n\nWhy Choose ZeroSumNormal over Pivoting?\nAs I indicated earlier, pivoting is a practical method when there’s a clear reference outcome. However when each category is of equal interest, selecting an arbitrary baseline can bias the interpretation of the results. The ZeroSumNormal approach avoids this by treating all categories symmetrically, ensuring that each effect is measured relative to the collective group rather than an arbitrary reference.\n\n\nIn Conclusion…\nOverparameterization and the resulting non-identifiability problem in categorical regression can lead to models that lack interpretability and stability. By using the ZeroSumNormal prior, we effectively anchor the parameters through a zero-sum constraint, avoiding the pitfalls of relative measurements without fixed axes. This leads to more robust and interpretable models, especially in contexts like multiparty elections where every category is of equal importance.\n\n\nReferences and links:\n\nThe ZeroSumNormal is discussed in this podcast.\nHere is the documentation of the implementation of the ZeroSumNormal distribution in PyMC.\nHere is a project I’m working on where I use ZeroSumNormal priors in my models."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html",
    "href": "portfolio/pfg_xgb/index.html",
    "title": "Retrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost",
    "section": "",
    "text": "📂 Code & Notebooks\n\n\n\nAll code and demo notebooks are available in the GitHub repository."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#introduction",
    "href": "portfolio/pfg_xgb/index.html#introduction",
    "title": "Retrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost",
    "section": "1 Introduction",
    "text": "1 Introduction\nPhytoplankton functional groups (PFGs) play a fundamental role in marine biogeochemical cycles, influencing carbon sequestration, nutrient fluxes, and global climate feedbacks. Different functional groups contribute uniquely to these processes; for example, diatoms facilitate carbon export through rapid sinking, cyanobacteria fix atmospheric nitrogen, and coccolithophores regulate carbonate chemistry via calcification (Marañón et al. 2015; Boyd and Doney 2010). Identifying and quantifying these groups from space is crucial for understanding their ecological functions, detecting environmental changes, and improving ocean biogeochemical models (Bopp et al. 2005; Laufkötter et al. 2015). However, current satellite ocean-color products primarily provide total chlorophyll a concentrations, which do not directly indicate community composition. To address this gap, various remote sensing algorithms have been developed to infer phytoplankton diversity, each with limitations in distinguishing certain groups and quantifying their biomass accurately (Mouw et al. 2017).\n\n1.1 Remote Sensing Approaches for PFG Retrieval\nPhytoplankton classification from satellite remote sensing has traditionally relied on empirical and semi-analytical methods. Empirical band-ratio techniques, such as PHYSAT, classify dominant phytoplankton groups based on anomalies in spectral reflectance but are often region-specific and limited to broad functional classes (Alvain et al. 2005, 2008). Semi-analytical models, in contrast, use inherent optical properties (IOPs) to infer phytoplankton composition from satellite reflectance, providing a more mechanistic approach (Hirata et al. 2011). Hybrid models incorporate additional environmental variables, such as sea surface temperature and total chlorophyll, to infer community structure (Brewin et al. 2010).\nMore recently, hyperspectral ocean-color sensors, such as NASA’s Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) mission, may have the potential to improve PFG retrieval by capturing finer spectral features associated with phytoplankton pigments (Dierssen et al. 2023). That is not to say that hyperspectral resolution is sufficient on its own. Optical similarity between different groups, depth-related biases in surface measurements, as well as associated measurement uncertainties inherenth to the noisy marine environment will likely hinder retrieval accuracy (IOCCG 2014). Most current models either estimate phytoplankton size classes or assign a single dominant group per pixel, often failing to capture the complexity of mixed communities (Ciotti, Lewis, and Cullen 2002).\n\n\n1.2 Study Contribution and Approach\nIn this study, we introduce extreme gradient boosting (XGBoost) (Chen and Guestrin 2016) as a novel approach for retrieving phytoplankton functional groups from satellite ocean color data. XGBoost is a scalable ensemble learning algorithm that has demonstrated high performance in complex classification tasks but has not yet been widely applied to PFG retrieval. XGBoost is also less opaque than neural networks, and can deal better with highly correlated (spectral) data of varying scales.\nRecent studies have demonstrated the potential of XGBoost in related applications, such as harmful algal bloom detection (Izadi et al. 2021) and phytoplankton biomass estimation (Yan et al. 2025), highlighting its suitability for remote sensing applications. Our work aligns with the objectives of the PACE mission by contributing an advanced classification algorithm that enhances hyperspectral monitoring of phytoplankton diversity (Zhang et al. 2024). To our knowledge, this is the first application of XGBoost for PFG classification in ocean color remote sensing, offering a robust alternative to traditional retrieval methods.\nOur approach was to leverage a large dataset of simulated hyperspectral TOA radiance and associated environmental variables to improve both the discrimination of functional groups and the quantification of their biomass. Previous remote sensing algorithms often classified only a dominant PFG or broad size class (Mouw et al. 2017) and relied on empirical band relationships that lacked generalizability (Hirata et al. 2011). By utilizing a machine learning framework capable of integrating multiple features, our approach reduces classification errors and enhances retrieval precision. Moreover the application of eXplainable AI (XAI) techniques to relate predictions to their input may further guide future efforts to improve PFG quantification.\n\n\n\n\n\n\nMethods\n\n\n\n\n\n\n1.3 Data Preparation and Feature Selection\nWe utilized a simulated dataset representing the world ocean over 31 days, corresponding to December 2021. The simulation generated hyperspectral remote sensing TOA radiance data, emulating a sensor configuration akin to that of the PACE instrument. Due to the high dimensionality of the original spectral data, we conducted an initial exploratory analysis and observed strong correlations among many of the channels. To reduce redundancy while preserving essential spectral information, we retained 51 channels by selecting one channel every ten. Note that despite this feature subsampling, spectral features are characterized by high degree of correlation we opted against applying principal component analysis for two principal reasons. The first reason is to avoid overemphasizing blue water signal contributed by the extensive oceanic regions present in a global satellite scene, and which could mask coastal processes of interest. The second reason is that tree-based algorithms such as XGBoost are resiliant to input multicollinearity. To further contextualize the ocean color signal, we also included auxiliary environmental variables such as temperature and latitude. Though not available from actual PACE measurements, climatology including temperature could be readily sourced elsewhere to augment observations on hand.\nThe dataset was divided into training and test sets using an 80/20 split. The training set was exclusively used for model development and hyperparameter optimization (see next section), while the test set was set aside until the final validation of model performance.\n\n\n1.4 Model Choice\nWe employed an XGBoost Regressor model with a multi-output regression head to predict simultaneously multiple phytoplankton functional groups as well as total chlorophyll-a concentration. XGBoost is a high-performance, scalable implementation of gradient boosting that has become a popular choice for a wide range of regression and classification tasks (Chen and Guestrin 2016). This approach consists in building an ensemble of decision trees sequentially, where each new tree attempts to correct the errors made by the previous trees. By optimizing a regularized objective function, XGBoost effectively controls overfitting while enhancing prediction accuracy. Its efficient handling highly correlated data, support for parallel computation, and flexible regularization mechanisms make it particularly well-suited for complex modeling tasks.\n\n\n\n\n\n\nHyperparameter Optimization and Model Training\n\n\n\n\n\nGiven the complexity of the problem and the high dimensionality of the input features, it was critical to optimize the hyperparameters to achieve robust performance and prevent overfitting. To this end, we conducted hyperparameter optimization using the Optuna version 4.2.1 library (Akiba et al. 2019). Specifically, we employed the efficient Tree-structured Parzen Estimator (TPE) algorithm (Bergstra et al. 2011). TPE is a Bayesian optimization method that iteratively builds probabilistic models of the hyperparameter space based on past evaluation results. By modeling the distributions of promising and less promising hyperparameter configurations, TPE suggests new parameter sets to explore, focusing the search on regions likely to yield improved performance. To further enhance the efficiency of the optimization process, we utilized Optuna’s MedianPruner with n_warmup_steps=5. This pruner automatically stops unpromising trials during the early stages of training (after at least 5 steps) if their intermediate results indicate they are unlikely to outperform the median performance of completed trials. The optimization step used an objective function to minimize the root mean squared error (RMSE) computed via three-fold cross-validation on the training set. The hyperparameters under investigation are lthe learning rate, maximum tree depth, number of estimators, subsample ratio, column subsample ratio, and gamma (the minimum loss reduction required to make a further partition on a leaf node); cf Table 1 for further details. The Bayesian optimization procedure allowed us to efficiently explore the hyperparameter space by leveraging past trial information to prune unpromising candidate parameter sets early, thereby reducing overall computational cost.\nOnce the optimization step complete, we instantiated the XGBoost model with the best set of hyperparameters and trained it on the full training set.\n\n\n\nTable 1: Hyperparameter ranges and their corresponding sampling strategy used in optimization.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter\nLow bound\nHigh bound\nSampling Distribution\n\n\n\n\nLearning rate\n\\(10^{-3}\\)\n\\(0.3\\)\nLog Uniform\n\n\nMax. tree depth\n\\(3\\)\n\\(10\\)\nUniform Integer\n\n\nEstimator number\n\\(50\\)\n\\(500\\)\nUniform Integer\n\n\nRow sample fraction\n\\(0.5\\)\n\\(1.0\\)\nUniform Float\n\n\nColumn sample frac.\n\\(0.5\\)\n\\(1.0\\)\nUniform Float\n\n\nGamma\n\\(10^{-8}\\)\n\\(1.0\\)\nLog Uniform\n\n\n\n\n\n\n\n\n\n\n\n1.5 Model Evaluation and eXplainable AI (XAI)\nOnce the optimal hyperparameter combination was identified, we retrained the final XGBoost model on the full training set using these optimized settings. Finally, we evaluated the performance of the retrained model on the held-out test set to assess its generalizability.\n\n1.5.1 Prediction Explainability\nTo enhance interpretability and gain insights into how different input features influence model predictions, we employed Shapley Additive Explanations (SHAP), a widely used explainable AI (XAI) framework for interpreting complex machine learning models. SHAP is named after the concept of Shapley values, which consists in assigning importance values to each input feature by estimating its contribution to the model’s predictions across different samples. The method is rooted in cooperative game theory, and guarantees a fair distribution of importance scores among features (Lundberg and Lee 2017).\nGiven the computational complexity of our XGBoost model and the high dimensionality of the dataset, we conducted SHAP analysis on a random subsample of 10,000 observations from the test set. This subset was selected to balance computational feasibility while maintaining a representative sample of phytoplankton spectral diversity.\nWe generated SHAP summary plots, which provide a comprehensive visualization of feature importance and the directionality of their influence on model outputs. These plots display the magnitude of each feature’s impact across all predictions, helping to identify the most influential spectral and environmental variables in determining phytoplankton functional group composition. The insights gained from SHAP analysis aid in validating model behavior and ensuring its ecological plausibility.\n\n1.5.1.1 Code Availability\nAll analysis and modeling code used in this study was written in Python 3.12. This code is publicly available on GitHub."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#results",
    "href": "portfolio/pfg_xgb/index.html#results",
    "title": "Retrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost",
    "section": "2 Results",
    "text": "2 Results\n\n2.1 Hyperparameter Optimization (HPO)\nWe performed hyperparameter optimization using a Bayesian optimization framework implemented with Optuna. The metric used for optimization was the average RMSE (in units of \\(mgL^{-1} Chl_a\\) ) computed over the cross-validation folds and across all target compartments. The “full HPO run” best parameters indicate a relatively aggressive model, characterized by deep trees with many estimators, a moderate learning rate, and little regularization via gamma.\nThe best trial finished with an RMSE of \\(0.116mgL^{-1} Chl_a\\). Below is the list of hyperparameters researched, the optimal values found, and an interpretation of these values:\n\nLearning Rate (learning_rate): \\(0.083\\) - This moderate learning rate suggests the model takes reasonably sized steps when updating that are neither too aggressive (which might lead to overshooting the optimum) nor too conservative (which could slow down convergence).\nMax Depth (max_depth): \\(10\\) - A depth of 10 allows the trees to capture complex interactions. This may indicate that the data has non-linear relationships that benefit from deeper trees. Such a depth can be associated with overfitting. The cross-validation process during HPO should minimize this however.\nNumber of Estimators (n_estimators): \\(466\\) -Building around 466 trees indicates the ensemble haa to tackle inherent complexity in the data that was not apparetn during the Exploratory Data Analysis phase. A larger number of trees generally improves performance—up to a point before overfitting becomse a risk. This number in conjunction with the cross validation process suggest this number strikes a balance between performance and overfitting.\nSubsampling (subsample): \\(0.658\\) - This indicates each of the 466 trees is using roughly 66% of the data. This introduces randomness that helps prevent overfitting as not all samples in any cross-validation fold are used to build every tree.\nFeatures used per tree (colsample_bytree): \\(0.894\\) - Using about 89% of the features per tree indicates that most features are informative, and the model is allowed to consider almost the full feature set at each split. - See features used in the Methods section.\nGamma (gamma): \\(8.63e-06\\) - An extremely low gamma value means that almost no minimum loss reduction is required to make a split. This implies that the algorithm will split more readily, potentially capturing fine details. Awareness of this hyperparameter values is important as low gamma can risk overfitting.\n\n\n\n2.2 Optimized Model Validation\nThe next step was to load the best set of hyperparameter (listed above) into the model and retrain the model on the entire training set. The optimized and trained model was then validated using the test set, which prior to the HPO process and until this step had been set aside.\n\n\n\n\n\n\nFigure 1: Goodness-of-fit plots for all groups and total chorophyll a, measured on out-of-sample data set. THe model is able to predict with very good accuracy. Dinoflagellates are the notable exception.\n\n\n\nA more complete set of metrics are summarized in table Table 2 See further below for metrics explanation.\n\n\n\nTable 2: Performance metrics of optimized and trained model on hold-out set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDiatom\nChloroph.\nCyanobac\nCoccolith.\nDinoflag.\nPhaeo\nTot. Chl_a\n\n\n\n\nMSE\n0.00034\n0.00010\n2.89e-06\n8.59e-05\n1.96e-05\n0.00011\n0.000193\n\n\nRMSE\n0.0184\n0.0100\n0.0017\n0.00927\n0.00443\n0.0105\n0.0139\n\n\nMAE\n0.00878\n0.0042\n0.00078\n0.0042\n0.000637\n0.00313\n0.00728\n\n\nR-squared\n0.979\n0.958\n0.996\n0.985\n0.530\n0.999\n0.999\n\n\nMAE/StDev\n0.0691\n0.0858\n0.0302\n0.0563\n0.0986\n0.00754\n0.0182\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of metrics\n\n\n\n\n\n\nMean Squared Error (MSE):\nMSE is the average of the squared differences between the predicted and true values. Squaring the errors emphasizes larger deviations, making MSE sensitive to outliers. In our context, MSE is expressed in units of (mg L\\(^{-1}\\) Chl\\(_a\\))\\(^2\\). Lower MSE values indicate better model performance.\nRoot Mean Squared Error (RMSE):\nRMSE is the square root of the MSE, bringing the error metric back to the original units (mg L\\(^{-1}\\) Chl\\(_a\\)). It provides a direct measure of the average prediction error magnitude. Lower RMSE values suggest that the model’s predictions are closer to the true values.\nMean Absolute Error (MAE):\nMAE calculates the average absolute difference between predicted and true values. Unlike MSE, it does not square the errors, so it is less sensitive to large outliers. MAE is also expressed in the same units as the target variable (mg L\\(^{-1}\\) Chl\\(_a\\)). A lower MAE indicates better predictive accuracy.\nCoefficient of Determination (R-squared):\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where a value closer to 1 indicates that the model explains a high proportion of the variance in the data. In our results, high R-squared values generally indicate strong model performance, although lower values (e.g., for dinoflagellates) suggest room for improvement.\nMAE/StDev\\(_{true}\\):\nThis ratio compares the mean absolute error to the standard deviation of the true values. It provides a relative measure of error by indicating how the average error compares to the inherent variability in the data. A lower ratio implies that the model’s prediction error is small relative to the natural variability of the observations.\n\n\n\n\n\n\n2.3 XAI with Shapley Values\nThe SHAP summary plots provides insights into feature importance and their effects on model predictions for phytoplankton functional groups.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\nFigure 2: Shapley values are shown for each functional group and for total chlorophyll. Features are ranked by most to least impactful, from top to bottom. Only the top 6 predictive features are shown. Along the x-axis, positive SHAP values indicate a positive relationship with the predicted; negative values, a negative one. Wider sections indicate greater variability. The color gradient represents feature values, with red for high values and blue for low values. The midpoint of the color bar reflects a percentile-based central value, not necessarily the mean, median, or mode, as it depends on the feature’s distribution.\n\n\n\nMost saliently, temperature was the top factor for all phytoplankton groups but is not a primordial feature in quantifying dinoflagellates."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#discussion",
    "href": "portfolio/pfg_xgb/index.html#discussion",
    "title": "Retrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost",
    "section": "3 Discussion",
    "text": "3 Discussion\nDinoflagellates do not show a single clear biogeographical zone of dominance the way cyanobacteria (tropics), diatoms (high latitudes and upwelling regions), or coccolithophores (subpolar summer blooms) do (Boyd and Doney 2010; Buitenhuis 2013; Gregg and Rousseaux 2019). Instead, they are considered ecological opportunists, often thriving in stratified, stable water columns typical of tropical and temperate regions (Smayda and Reynolds 2001; Kibler 2015). Their distribution is governed more by factors such as nutrient availability (particularly high N:P ratios), light regime, and water-column stability, along with their ability to exploit mixotrophic nutrition strategies, than by temperature alone (Jeong 2010; Glibert 2001; Flynn 2013). Recent studies suggest that ongoing ocean warming and increased stratification may promote a shift from diatom- to dinoflagellate-dominated phytoplankton communities—not because dinoflagellates prefer higher temperatures per se, but because warming tends to suppress vertical mixing and nutrient resupply, creating conditions more favorable to dinoflagellates (Glibert, 2020; Peperzak, 2003; Fu et al., 2012)."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#conclusion",
    "href": "portfolio/pfg_xgb/index.html#conclusion",
    "title": "Retrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis study demonstrates the potential of machine learning, specifically XGBoost, for retrieving phytoplankton functional groups (PFGs) from hyperspectral ocean color data. Using simulated data designed to emulate NASA’s upcoming PACE mission, our model achieved high predictive accuracy for most PFGs, with R² values exceeding 0.95 for diatoms, cyanobacteria, coccolithophores, and phaeophytes. However, retrievals for dinoflagellates were less reliable, highlighting challenges in distinguishing this group based on spectral data alone. Hyperparameter optimization with Optuna improved model performance, and SHAP-based explainability analysis revealed that temperature was among the top six predictive features for all functional groups except dinoflagellates, emphasizing the need to integrate external sea surface temperature (SST) data in real-world applications. Furthermore, our findings confirm that only a subset of hyperspectral bands drive PFG retrieval, suggesting that future algorithm development should prioritize feature selection to optimize computational efficiency. These results contribute to ongoing efforts to enhance phytoplankton classification methods and provide insights for future ocean color missions seeking to leverage hyperspectral capabilities for improved ecological monitoring."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#references",
    "href": "portfolio/pfg_xgb/index.html#references",
    "title": "Retrieving Phytoplankton Functional Groups from Hyperspectral Ocean Color using XGBoost",
    "section": "References",
    "text": "References\n\n\nAkiba, Takuya, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. “Optuna: A Next-Generation Hyperparameter Optimization Framework.” In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2623–31. ACM. https://doi.org/10.1145/3292500.3330701.\n\n\nAlvain, S., C. Moulin, Y. Dandonneau, and F. M. Bréon. 2005. “Remote Sensing of Phytoplankton Groups in Case 1 Waters from Global SeaWiFS Imagery.” Deep-Sea Research I 52 (11): 1989–2004. https://doi.org/10.1016/j.dsr.2005.06.015.\n\n\nAlvain, S., C. Moulin, Y. Dandonneau, and H. Loisel. 2008. “Seasonal Distribution and Succession of Dominant Phytoplankton Groups in the Global Ocean: A Satellite View.” Global Biogeochemical Cycles 22 (3): GB3S04. https://doi.org/10.1029/2007GB003154.\n\n\nBergstra, James, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. “Algorithms for Hyper-Parameter Optimization.” In Advances in Neural Information Processing Systems, edited by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf.\n\n\nBopp, L., O. Aumont, P. Cadule, S. Alvain, and M. Gehlen. 2005. “Response of Diatoms Distribution to Global Warming and Potential Implications: A Global Model Study.” Geophysical Research Letters 32 (19): L19606. https://doi.org/10.1029/2005GL023653.\n\n\nBoyd, P. W., and S. C. Doney. 2010. “Modelling Regional Responses by Marine Pelagic Ecosystems to Global Climate Change.” Geophysical Research Letters 32 (19): L19606. https://doi.org/10.1029/2005GL023653.\n\n\nBrewin, R. J. W., S. Sathyendranath, T. Hirata, S. J. Lavender, R. Barciela, and N. J. Hardman-Mountford. 2010. “A Three-Component Model of Phytoplankton Size Class for the Atlantic Ocean.” Ecological Modelling 221 (11): 1472–83. https://doi.org/10.1016/j.ecolmodel.2010.02.014.\n\n\nBuitenhuis, Erik T. et al. 2013. “Biogeochemical Fluxes Through Microzooplankton.” Global Biogeochemical Cycles 27 (3): 847–58. https://doi.org/10.1002/gbc.20059.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 785–94. https://doi.org/10.1145/2939672.2939785.\n\n\nCiotti, A. M., M. R. Lewis, and J. J. Cullen. 2002. “Assessment of the Relationships Between Dominant Cell Size in Natural Phytoplankton Communities and the Spectral Shape of the Absorption Coefficient.” Limnology and Oceanography 47 (2): 404–17. https://doi.org/10.4319/lo.2002.47.2.0404.\n\n\nDierssen, H. M., M. M. Gierach, L. S. Guild, A. Mannino, J. Salisbury, S. Schollaert Uz, J. Scott, et al. 2023. “Synergies Between NASA’s Hyperspectral Aquatic Missions PACE, GLIMR, and SBG: Opportunities for New Science and Applications.” Journal of Geophysical Research: Biogeosciences 128 (5): e2023JG007574. https://doi.org/10.1029/2023JG007574.\n\n\nFlynn, Kevin J. et al. 2013. “Misuse of the Phytoplankton–Zooplankton Dichotomy: The Need to Assign Organisms as Mixotrophs Within Plankton Functional Types.” Journal of Plankton Research 35 (1): 3–11. https://doi.org/10.1093/plankt/fbs062.\n\n\nGlibert, Patricia M. et al. 2001. “The Role of Eutrophication in the Global Proliferation of Harmful Algal Blooms.” Oceanography 14 (2): 66–74. https://doi.org/10.5670/oceanog.2001.47.\n\n\nGregg, Watson W., and Cecile S. Rousseaux. 2019. “Decadal Changes in Global Phytoplankton Composition: Observations and Modeling.” Journal of Geophysical Research: Oceans 124 (2): 983–1003. https://doi.org/10.1029/2018JC014173.\n\n\nHirata, T., N. J. Hardman-Mountford, R. J. W. Brewin, J. Aiken, R. G. Barlow, K. Suzuki, T. Isada, et al. 2011. “Synoptic Relationships Between Surface Chlorophyll-a and Diagnostic Pigments Specific to Phytoplankton Functional Types.” Biogeosciences 8 (2): 311–27. https://doi.org/10.5194/bg-8-311-2011.\n\n\nIOCCG. 2014. “Phytoplankton Functional Types from Space.” No. 15. Edited by S. Sathyendranath. IOCCG Reports. Dartmouth, Canada: International Ocean-Colour Coordinating Group (IOCCG).\n\n\nIzadi, Moein, Mohamed Sultan, Racha El Kadiri, Amin Ghannadi, and Karem Abdelmohsen. 2021. “A Remote Sensing and Machine Learning-Based Approach to Forecast the Onset of Harmful Algal Bloom.” Remote Sensing 13 (19): 3863. https://doi.org/10.3390/rs13193863.\n\n\nJeong, Hae Jin et al. 2010. “Mixotrophy in the Marine Dinoflagellate Population: Physiological Roles, Relationships, and Regulation.” Harmful Algae 9 (2): 154–65. https://doi.org/10.1016/j.hal.2009.08.005.\n\n\nKibler, Sarah R. et al. 2015. “Geographic and Vertical Distribution of Dinoflagellates in the Gulf of Mexico.” Journal of Phycology 51 (4): 606–18. https://doi.org/10.1111/jpy.12302.\n\n\nLaufkötter, C., M. Vogt, N. Gruber, O. Aumont, L. Bopp, S. C. Doney, J. P. Dunne, et al. 2015. “Projected Decreases in Future Marine Export Production: The Role of the Carbon Flux Through the Upper Ocean Ecosystem.” Biogeosciences 13 (13): 4023–47. https://doi.org/10.5194/bg-13-4023-2016.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS), 30:4768–77. Curran Associates, Inc. https://doi.org/10.48550/arXiv.1705.07874.\n\n\nMarañón, E., P. Cermeno, M. Latasa, and R. D. Tadonléké. 2015. “Resource Supply Overrides Temperature as a Controlling Factor of Marine Phytoplankton Growth.” PLoS ONE 10 (6): e0130093. https://doi.org/10.1371/journal.pone.0130093.\n\n\nMouw, Colleen B., Neil J. Hardman-Mountford, Sylvie Alvain, Astrid Bracher, Robert J. W. Brewin, Annick Bricaud, Aurea M. Ciotti, et al. 2017. “A Consumer’s Guide to Satellite Remote Sensing of Multiple Phytoplankton Groups in the Global Ocean.” Frontiers in Marine Science 4: 41. https://doi.org/10.3389/fmars.2017.00041.\n\n\nSmayda, Theodore J., and Colin S. Reynolds. 2001. “Community Ecology of Harmful Algal Blooms in Coastal Upwelling Ecosystems.” ICES Journal of Marine Science 58 (2): 374–76. https://doi.org/10.1006/jmsc.2001.1034.\n\n\nYan, Zhaojiang, Chong Fang, Kaishan Song, Xiangyu Wang, Zhidan Wen, Yingxin Shang, Hui Tao, and Yunfeng Lyu. 2025. “Spatiotemporal Variation in Biomass Abundance of Different Algal Species in Lake Hulun Using Machine Learning and Sentinel-3 Images.” Scientific Reports 15: 2739. https://doi.org/10.1038/s41598-025-87338-4.\n\n\nZhang, Yuan, Fang Shen, Renhu Li, Mengyu Li, Zhaoxin Li, Songyu Chen, and Xuerong Sun. 2024. “AIGD-PFT: The First AI-Driven Global Daily Gap-Free 4 Km Phytoplankton Functional Type Data Product from 1998 to 2023.” Earth System Science Data 16: 4793–4816. https://doi.org/10.5194/essd-16-4793-2024."
  },
  {
    "objectID": "assets/elections_estonia.html",
    "href": "assets/elections_estonia.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "Preamble: This is a data analysis and modeling project to predict voter party preference in Estonia on the basis of demographics. The intended process involves initial data exploration, followed by the construction, fitting, and analysis of a series of increasingly complex Bayesian categorical models. This Bayesian approach offers a robust alternative to traditional frequentist Analysis of Variance (ANOVA) methods commonly employed in scientific research and A/B/n testing in fields such as marketing, user experience (UX), and web design. A significant benefit of the Bayesian model is its inherent ability to quantify predictive uncertainty through the calculation of posterior (inferential) probability distributions, contrasting with the reliance on sampling distributions in frequentist procedures\nContents:\n\nExploratory Data Analysis\nModeling  \\(→\\) Model Building and, Prior Elicitation and Prior Predictive Checks  \\(→\\) Model Fitting and Goodness-of-Fit Analysis  \\(→\\) Posterior Predictive Checks  \\(→\\) Summary and Conclusion\n\nData Provenance: This data is courtesy of SALK. SALK refers to the Liberal Citizen Foundation in Estonia; a political organization aimed at influencing Estonian policy and parliamentary elections. This foundation was established with the (ultimately attained) goal of helping liberal forces gain a majority in the 2023 Estonian parliamentary elections.\nAcknowledgments: I thank Alex Andorra and the folks at Intuitive Bayes for helping me understand non-identifiability and overparameterization in statistical modeling.\n\nimport arviz as az\nimport matplotlib.pyplot as pp\nfrom matplotlib import rcParams\nimport numpy as np\nimport pandas as pd\n\nimport pymc as pm\n\n\nrcParams['font.size'] = 12\n\n\n1. Exploratory Data Analysis\n\ndata = pd.read_csv('./data/estonian-data.csv')\n\n\ndata.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nage_group\n16-24\n16-24\n16-24\n16-24\n16-24\n\n\neducation\nBasic education\nBasic education\nBasic education\nBasic education\nBasic education\n\n\ngender\nMale\nMale\nMale\nMale\nMale\n\n\nnationality\nEstonian\nEstonian\nEstonian\nEstonian\nEstonian\n\n\nelectoral_district\nHaabersti, Põhja-Tallinn ja Kristiine\nHarju- ja Raplamaa\nHarju- ja Raplamaa\nHarju- ja Raplamaa\nHarju- ja Raplamaa\n\n\nunit\nKristiine\nHarjumaa\nHarjumaa\nHarjumaa\nHarjumaa\n\n\nEKRE\n0\n0\n0\n1\n0\n\n\nEesti 200\n0\n0\n0\n0\n0\n\n\nHard to say\n0\n0\n0\n0\n0\n\n\nIsamaa\n0\n0\n0\n0\n0\n\n\nKeskerakond\n0\n0\n0\n0\n1\n\n\nMitte ükski erakond\n1\n0\n0\n0\n0\n\n\nOther\n0\n0\n0\n0\n0\n\n\nParempoolsed\n0\n0\n0\n0\n0\n\n\nReformierakond\n0\n1\n0\n0\n0\n\n\nRohelised\n0\n0\n0\n0\n0\n\n\nSDE\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\nI don’t like the “Hard to say” column name; replace below with the clearer “Undecided” label.\n\ndata.rename(columns={'Hard to say': 'Undecided'}, inplace=True)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5206 entries, 0 to 5205\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   age_group            5206 non-null   object\n 1   education            5206 non-null   object\n 2   gender               5206 non-null   object\n 3   nationality          5206 non-null   object\n 4   electoral_district   5206 non-null   object\n 5   unit                 5206 non-null   object\n 6   EKRE                 5206 non-null   int64 \n 7   Eesti 200            5206 non-null   int64 \n 8   Undecided            5206 non-null   int64 \n 9   Isamaa               5206 non-null   int64 \n 10  Keskerakond          5206 non-null   int64 \n 11  Mitte ükski erakond  5206 non-null   int64 \n 12  Other                5206 non-null   int64 \n 13  Parempoolsed         5206 non-null   int64 \n 14  Reformierakond       5206 non-null   int64 \n 15  Rohelised            5206 non-null   int64 \n 16  SDE                  5206 non-null   int64 \ndtypes: int64(11), object(6)\nmemory usage: 691.6+ KB\n\n\n\ndata.columns.tolist()\n\n['age_group',\n 'education',\n 'gender',\n 'nationality',\n 'electoral_district',\n 'unit',\n 'EKRE',\n 'Eesti 200',\n 'Undecided',\n 'Isamaa',\n 'Keskerakond',\n 'Mitte ükski erakond',\n 'Other',\n 'Parempoolsed',\n 'Reformierakond',\n 'Rohelised',\n 'SDE']\n\n\n\ndata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nEKRE\n5206.0\n0.124856\n0.330587\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nEesti 200\n5206.0\n0.082981\n0.275880\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nUndecided\n5206.0\n0.127353\n0.333400\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nIsamaa\n5206.0\n0.054552\n0.227126\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nKeskerakond\n5206.0\n0.119285\n0.324155\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nMitte ükski erakond\n5206.0\n0.181521\n0.385487\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nOther\n5206.0\n0.007876\n0.088403\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nParempoolsed\n5206.0\n0.002305\n0.047960\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nReformierakond\n5206.0\n0.215136\n0.410956\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nRohelised\n5206.0\n0.017480\n0.131063\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nSDE\n5206.0\n0.066654\n0.249446\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\ndata.loc[:, 'EKRE':].sum(axis=1).describe().loc[['min', 'max']]\n\nmin    1.0\nmax    1.0\ndtype: float64\n\n\n\ndata.loc[:, 'EKRE':].sum(axis=0).plot(kind='bar', title='Party Choice', ylabel='Counts', color='black', alpha=0.6);\n\n\n\n\n\n\n\n\n\ndata.electoral_district.unique().size, data.unit.unique().size\n\n(12, 24)\n\n\n\nf, (left, right) = pp.subplots(ncols=2, figsize=(12, 6), sharey=True)\ndata.electoral_district.value_counts(normalize=True).plot(kind='bar', color=['black', 'darkgray'], ax=left)\ndata.unit.value_counts(normalize=True).plot(kind='bar', color=['darkgray', 'black'])\nleft.set_xticklabels(left.get_xticklabels(), rotation=90)\nf.tight_layout();\n\n\n\n\n\n\n\n\n\nf, (left, right) = pp.subplots(ncols=2, figsize=(10, 4), sharey=True)\ndata.age_group.value_counts(normalize=True, sort=False).plot(kind='bar', color=[f'C{i}' for i in range(7)], ax=left);\ndata.gender.value_counts(normalize=True).plot(kind='bar', color= ['C7', 'C8'], ax=right)\nleft.set(ylabel='Fraction', xlabel='age group')\nf.tight_layout()\n\n\n\n\n\n\n\n\n\ndata.education.value_counts()\n\neducation\nSecondary education    2684\nHigher education       1948\nBasic education         574\nName: count, dtype: int64\n\n\n\nf, (left, right) = pp.subplots(ncols=2, figsize=(10, 4), sharey=True)\ndata.education.value_counts(normalize=True).plot(kind='bar', color=[f'C{i}' for i in range(0, 3)], ax=left)\ndata.nationality.value_counts(normalize=True).plot(kind='bar', color=['C3', 'C4'])\nleft.set_xticklabels(left.get_xticklabels(), rotation=50)\nf.tight_layout();\n\n\n\n\n\n\n\n\n\nPreliminary summary:\nThis table registers voter preference. Each row is a voter. Columns include voter demographics, the election district and unit. The rest of the columns are each dedicated to a party. Data are in the thousands with no apparent missing value. There are two types of data; (1) strings that lend themselves to categorization, and (2) binary data indicating whether a given party has received the vote of that row’s voter. There were no irregularities, i.e. more than 1 vote registered for each person; there were no abstentions noted in this dataset either. All the data is categorical with the following number of categories: * Party choice \\(→\\) 11 * Electoral district \\(\\rightarrow\\) 12 * (Electoral) Unit \\(→\\) 24 * AGe group \\(→\\) 7 * Gender \\(→\\) 2 * Education \\(→\\) 3 * Nationality \\(→\\) 2\n\n\nPreliminary summary, part 2:\nThere is some imbalance in education level, with secondary education being the more proeminent. There is notable imbalance in nationality, with predictable majority of Estonian representation. Party choice, electoral district and unit are also quite imbalanced. Next is to examine party representation breakdown at the group level. Note that I will not consider electoral district and unit hereafter as the emphasis is on demographics as a driver of party choice.\n\ndef make_group_percentage(df: pd.DataFrame, category: 'str') -&gt; pd.DataFrame:\n    \"\"\"Computes percentage for each column in a group\"\"\"\n    group = df.groupby(category).sum(numeric_only=True)\n    group_percent = group.div(group.sum(axis=1), axis=0)*100\n    return group_percent\n\n\nparty_names = data.loc[:, 'EKRE':].columns.tolist()\nparty_names\n\n['EKRE',\n 'Eesti 200',\n 'Undecided',\n 'Isamaa',\n 'Keskerakond',\n 'Mitte ükski erakond',\n 'Other',\n 'Parempoolsed',\n 'Reformierakond',\n 'Rohelised',\n 'SDE']\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\nage_group\neducation\ngender\nnationality\nelectoral_district\nunit\nEKRE\nEesti 200\nUndecided\nIsamaa\nKeskerakond\nMitte ükski erakond\nOther\nParempoolsed\nReformierakond\nRohelised\nSDE\n\n\n\n\n0\n16-24\nBasic education\nMale\nEstonian\nHaabersti, Põhja-Tallinn ja Kristiine\nKristiine\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n16-24\nBasic education\nMale\nEstonian\nHarju- ja Raplamaa\nHarjumaa\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ndata.groupby('gender').sum(\nnumeric_only=True\n)\n\n\n\n\n\n\n\n\nEKRE\nEesti 200\nUndecided\nIsamaa\nKeskerakond\nMitte ükski erakond\nOther\nParempoolsed\nReformierakond\nRohelised\nSDE\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFemale\n271\n237\n412\n119\n374\n507\n18\n7\n620\n55\n225\n\n\nMale\n379\n195\n251\n165\n247\n438\n23\n5\n500\n36\n122\n\n\n\n\n\n\n\n\nf, (left, right) = pp.subplots(ncols=2, figsize=(11, 4), sharey=True)\n#gender = data.groupby('gender').sum(numeric_only=True)\n#(gender.div(gender.sum(axis=1), axis=0)*100).plot(kind='bar', colormap='tab20c', ax=left, legend=False)\n#data.groupby('nationality').sum(numeric_only=True).plot(kind='bar', colormap='tab20c', ax=right, legend=False)\nmake_group_percentage(data, 'gender').plot(kind='bar', colormap='tab20c', ax=left, legend=False)\nmake_group_percentage(data, 'nationality').plot(kind='bar', colormap='tab20c', ax=right)\nleft.set(title='Gender', ylabel='Fraction(%)', xlim=(-0.3, 1.3), xlabel='')\nright.set(xlim=(-0.3, 1.3), title='Nationality', xlabel='')\nright.legend(frameon=False, loc=(0.34 ,0.04), fontsize=10)\nright.grid(axis='y', alpha=0.5, ls=':')\nleft.grid(axis='y', alpha=0.5, ls=':')\nf.tight_layout();\n\n\n\n\n\n\n\n\n\nf, (top, bottom) = pp.subplots(nrows=2, figsize=(12, 8))\n#data.groupby('age_group').sum(numeric_only=True)\nmake_group_percentage(data, 'age_group').plot(kind='bar', colormap='tab20c', ax=top, legend=False)\n#data.groupby('education').sum(numeric_only=True)\nmake_group_percentage(data, 'education').plot(kind='bar', colormap='tab20c', ax=bottom, legend=False, )\ntop.set(title='Age Group', ylabel='Fraction(%)', xlabel='')\ntop.set_xticklabels(top.get_xticklabels(), rotation=0)\nbottom.legend(frameon=False,  loc=(0.24, 0.04), fontsize=10)\nbottom.set_xticklabels(bottom.get_xticklabels(), rotation=0)\nbottom.set(ylabel='Fraction(%)', title='Education', xlim=(-0.3, 2.3), xlabel='')\nbottom.grid(axis='y', alpha=0.5, ls=':')\ntop.grid(axis='y', alpha=0.5, ls=':')\nf.tight_layout();\n\n\n\n\n\n\n\n\n\n\n\nGrouped data summary\n\nOverall\n\nReformierakond (a liberal party) is pretty popular across the board except with non-Estonians;\nNon-Estonians - I’m told - is almost entirely composed up of Russians;\nThere is a fair amount of undecided voters;\nEKRE and Keskeradon are populist parties - EKRE is right-wing, Keskerakond is center-left;\nParempoolsed, center-right party is the underdog across the board.\n\nGroup-specific\n\nGender: there are some subtle differences.\n\nReformierakond is slightly more popular with female voters;\nThere are more undecided among female voters;\nThe right-wing EKRE is more popular with male voters.\n\nNationality:\n\nReformierakond, EKRE are more popular with Estonians;\nKeskerakond, Mitte ukski erakond are favored by ethnic Russians who are also more numerous to be undecided.\n\nAge group:\n\nMitte ukski erakond and Reformierakond are the two prevaiing parties among each age group except the two older tranches;\nAmong 65 and over Reformierakond retains the top but Mitte recedes in favor of Keskeradond.\n\nEducation:\n\nEKRE, Undecided, Mitte ukski erakond are the top 3 party choices for those with a basic education;\nReformierakond and Mitte are the top 2 party choices among those with a college degree.\nHigh school graduates exhibit a preference for Reformierakond, Mitte, and EKRE.\n\n\n\n\n\n2. Modeling\nThe goal is to predict party preference on the basis of available demographic data. I will first start with looking at two categories; education and nationality. The below is some data encoding to ease model fitting and clarity of plotted results. In particular note the model coordinate names; ‘party_choice’, ‘education’, ‘nationality’, ‘obs_idx’. The latter is just the row index of the dataframe containing the observations; each row corresponding to one observation.\n\n# response variable encoding\nparty_choice = pd.Categorical(data[party_names].idxmax(axis=1), categories=party_names, ordered=True)\nparty_choice_label = party_choice.categories.to_list()\nparty_choice_code = party_choice.codes\n\n# input variables encoding\neducation_code, education_label = data.education.factorize(sort=True) \nnationality_code, nationality_label = data.nationality.factorize(sort=True)\n\n# setting model coordinates\n\ncoords = {\n    'party_choice': party_choice_label,\n    'education': education_label,\n    'nationality': nationality_label,\n    'obs_idx': data.index} # observation index, the row location of an observation in the dataframe.\n\nBelow I write a first model using a baseline intercept and I use nationality and education as predictors and baseline offsets. The corresponding coefficients have ‘party choice’ as dimension (11 possibilities) and in the case of education and nationality, have also a dimension of ‘education’ (3 possibilities) and ‘nationality’ (2 possibilities), respectively. These are linearly combined and passed to a softmax function to scale the output to the \\((0-1)\\) interval, as shown below. \\[ p = softmax(α_{baseline} + α_{nationality}[\\small{nationality\\_category}] + α_{education}[\\small{education\\_category}]) \\]\nAs prior distribution for all three coefficients, instead of using the \\(\\mathcal{Normal}\\) distribution, I use below the \\(\\mathcal{Zero-sum\\ Normal}\\). This distribution imposes zero sum constraints on specified axes of the fitted coefficients, thereby addressing the problem of overparameterization and identifiability that often plague categorical regression models; a subject of my blog post found here.\nFinally I use for likelihood the \\(\\mathcal{Categorical}\\) distribution, which is a n-dimensional \\(\\mathcal{Bernoulli}\\) process, and through which I expose the model to the data.\n\ndef build_model1(α_sigma=1, α_nat_sigma=1, α_edu_sigma=1):\n    with pm.Model(coords=coords) as model1:\n        # Data containers\n        party_choice_idx = pm.Data('party_choice_index', party_choice_code, dims='obs_idx')\n        nationality_idx = pm.Data('nationality_index', nationality_code, dims='obs_idx')\n        education_idx = pm.Data('eductation_index', education_code, dims='obs_idx')\n        \n        # Model priors\n        # baseline\n        α = pm.ZeroSumNormal('α', sigma=α_sigma, dims='party_choice') \n        # baseline offset due to nationality\n        α_nationality = pm.ZeroSumNormal('α_nationality', sigma=α_nat_sigma, dims=('nationality', 'party_choice'), n_zerosum_axes=2) \n        # baseline offset due to education\n        α_education = pm.ZeroSumNormal('α_education', sigma=α_edu_sigma, dims=('education', 'party_choice'), n_zerosum_axes=2)\n\n        # Link function for choice probability\n        p = pm.math.softmax(α + α_nationality[nationality_idx] + α_education[education_idx], axis=-1)\n        \n        # Likelihood\n        _ = pm.Categorical('y', p=p, observed=party_choice_idx, dims='obs_idx')\n    return model1\n\n\nmodel1 = build_model1()\n\nBelow is the model’s diagram with dimension information included.\n\nmodel1.to_graphviz()\n\n\n\n\n\n\n\n\nNext is to sample model priors to see the soundness of assumptions made\n\nwith model1:\n    idata1 = pm.sample_prior_predictive(model=model1)\n    idata1.extend(pm.sample(chains=4, draws=2000, random_seed=42))\n    idata1.extend(pm.sample_posterior_predictive(idata1))\n\nSampling: [y, α, α_education, α_nationality]\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, α_nationality, α_education]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 96 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\ndef plot_forward_samples(\n        idata: az.InferenceData, categories:list, categories_name:str, \n        figsize=(12, 6), plot_prior=True, plot_posterior=True):\n    \"\"\"A quick function to plot model predictives. \"\"\"\n    f, (left, right) = pp.subplots(1, 2, figsize=figsize, sharey=True)\n    xticks = [i + 0.5 for i in range(len(categories))]\n    if plot_prior:\n        az.plot_ppc(idata, group=\"prior\", ax=left)\n        left.set(\n            xticks=xticks,\n        )\n        left.set_xticklabels(categories, fontsize=10, rotation=30)\n        left.set_xlabel(categories_name, fontsize=16)\n        left.legend(frameon=True, fontsize=11)\n    else:\n        left.set_visible(False)\n\n    if plot_posterior:\n        az.plot_ppc(idata, ax=right)\n        right.set(\n            xticks=xticks,\n        )\n        right.set_xticklabels(categories, fontsize=10, rotation=30)\n        right.set_xlabel(categories_name, fontsize=16)\n        right.legend(frameon=True, fontsize=11)\n    else:\n        right.set_visible(False)\n    f.tight_layout()\n    return f, (left, right)\n\n\nmean_win = pd.Series(party_choice_code).value_counts(normalize=True).round(3).sort_index()\nf, (axl, axr) = plot_forward_samples(idata1, categories=party_choice_label, categories_name='party choice', figsize=(12, 6))\naxl.set_xticklabels(axl.get_xticklabels(), rotation=60);\naxl.bar(x=axl.get_xticks(), height=mean_win.values,  color='k', fill=False, lw=3, width=1, zorder=13, label='data');\naxr.set_xticklabels(axr.get_xticklabels(), rotation=60);\n\n\n\n\n\n\n\n\nLeft, in blue lines are sample outcomes for each party choice, before the model has seen the data. Right, model fit results (posterior distribution for each category). The model seems to have learned the posterior distribution well. The model did take some time to fit though and in spite of the lack of difergences, one wonders if sampling could be more efficient. Domain experts have suggested a tighter variance on the coefficient priors could indeed ease sampling. Model1b integrates this domain knowledge as follows:\n\nmodel1b = build_model1(α_sigma=0.5, α_edu_sigma=0.2, α_nat_sigma=0.3)\n\n\nwith model1b:\n    idata1b = pm.sample_prior_predictive()\n    idata1b.extend(pm.sample(chains=4, draws=2000, random_seed=42))\n    idata1b.extend(pm.sample_posterior_predictive(idata1b))\n\nSampling: [y, α, α_education, α_nationality]\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, α_nationality, α_education]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 88 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\nThere was some speed improvement (~ 18 seconds on a Macbook air M2.) #### Assessing goodness of fit\n\naz.plot_trace(idata1, backend_kwargs={'tight_layout': True});\n\n\n\n\n\n\n\n\n\naz.plot_trace(idata1b, backend_kwargs=dict(tight_layout=True));\n\n\n\n\n\n\n\n\nGood mixing in the chains and lack of divergences suggest a good fit. The second model (bottom panel) has tighter posteriors, suggesting a slightly better fit.\n\naz.summary(idata1, var_names='α')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα[EKRE]\n0.906\n0.072\n0.772\n1.042\n0.001\n0.001\n6612.0\n6756.0\n1.0\n\n\nα[Eesti 200]\n0.377\n0.087\n0.222\n0.549\n0.001\n0.001\n6715.0\n6112.0\n1.0\n\n\nα[Undecided]\n1.153\n0.067\n1.021\n1.277\n0.001\n0.001\n6441.0\n6223.0\n1.0\n\n\nα[Isamaa]\n-0.710\n0.160\n-1.000\n-0.405\n0.002\n0.001\n8356.0\n6113.0\n1.0\n\n\nα[Keskerakond]\n1.302\n0.063\n1.178\n1.415\n0.001\n0.001\n6106.0\n6320.0\n1.0\n\n\nα[Mitte ükski erakond]\n1.599\n0.059\n1.486\n1.709\n0.001\n0.001\n5842.0\n5910.0\n1.0\n\n\nα[Other]\n-1.575\n0.180\n-1.900\n-1.225\n0.002\n0.001\n10091.0\n6406.0\n1.0\n\n\nα[Parempoolsed]\n-2.961\n0.326\n-3.584\n-2.365\n0.004\n0.003\n5747.0\n6096.0\n1.0\n\n\nα[Reformierakond]\n0.716\n0.084\n0.552\n0.865\n0.001\n0.001\n8024.0\n6563.0\n1.0\n\n\nα[Rohelised]\n-1.054\n0.156\n-1.340\n-0.752\n0.002\n0.001\n8262.0\n6252.0\n1.0\n\n\nα[SDE]\n0.246\n0.093\n0.072\n0.419\n0.001\n0.001\n11979.0\n6935.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata1b, var_names='α')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα[EKRE]\n0.781\n0.064\n0.660\n0.899\n0.001\n0.000\n12336.0\n6688.0\n1.0\n\n\nα[Eesti 200]\n0.320\n0.073\n0.176\n0.452\n0.001\n0.001\n10479.0\n6316.0\n1.0\n\n\nα[Undecided]\n1.020\n0.058\n0.907\n1.126\n0.001\n0.000\n10735.0\n6572.0\n1.0\n\n\nα[Isamaa]\n-0.509\n0.109\n-0.723\n-0.310\n0.001\n0.001\n10705.0\n5825.0\n1.0\n\n\nα[Keskerakond]\n1.155\n0.055\n1.049\n1.253\n0.001\n0.000\n10455.0\n6907.0\n1.0\n\n\nα[Mitte ükski erakond]\n1.464\n0.051\n1.370\n1.563\n0.001\n0.000\n9276.0\n6427.0\n1.0\n\n\nα[Other]\n-1.609\n0.147\n-1.889\n-1.333\n0.001\n0.001\n12469.0\n6006.0\n1.0\n\n\nα[Parempoolsed]\n-2.461\n0.202\n-2.849\n-2.089\n0.002\n0.001\n10961.0\n5924.0\n1.0\n\n\nα[Reformierakond]\n0.698\n0.071\n0.563\n0.830\n0.001\n0.000\n10962.0\n6083.0\n1.0\n\n\nα[Rohelised]\n-1.065\n0.121\n-1.295\n-0.843\n0.001\n0.001\n10573.0\n6503.0\n1.0\n\n\nα[SDE]\n0.205\n0.075\n0.073\n0.357\n0.001\n0.001\n11252.0\n6818.0\n1.0\n\n\n\n\n\n\n\nLooking at the model fit summary tables for the baseline \\(α\\) parameter, in both cases the \\(\\hat{R}\\) (aka Gelman-Rubin statistic) is 1.0. This indicates good convergence of MCMC chains (I fired up 4 of them for sampling). Interestingly however, in the case of the second model with tighter variance constraints on the priors, the Effective Sample Size corresponding to the sampling of the central portion of the posterior (ess_bulk) shows more efficient sampling in most cases. The posterior is therefore likelier to be better characterized. A similar pattern can be observed in the next 4 tables for \\(α_{nationality}\\) and \\(α_{education}\\). Note that in contrast, the tails of the posterior are sampled with similar efficiency in both cases, as noted by ess_tail.\n\naz.summary(idata1, var_names='α_education')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_education[Basic education, EKRE]\n0.234\n0.088\n0.072\n0.400\n0.001\n0.001\n8337.0\n6406.0\n1.0\n\n\nα_education[Basic education, Eesti 200]\n-0.359\n0.125\n-0.602\n-0.134\n0.001\n0.001\n8978.0\n6077.0\n1.0\n\n\nα_education[Basic education, Undecided]\n0.110\n0.091\n-0.052\n0.290\n0.001\n0.001\n8579.0\n6639.0\n1.0\n\n\nα_education[Basic education, Isamaa]\n-0.110\n0.129\n-0.352\n0.129\n0.001\n0.001\n12807.0\n6272.0\n1.0\n\n\nα_education[Basic education, Keskerakond]\n0.385\n0.091\n0.212\n0.555\n0.001\n0.001\n7640.0\n6648.0\n1.0\n\n\nα_education[Basic education, Mitte ükski erakond]\n0.226\n0.082\n0.067\n0.376\n0.001\n0.001\n6905.0\n6441.0\n1.0\n\n\nα_education[Basic education, Other]\n0.148\n0.276\n-0.357\n0.665\n0.003\n0.003\n9166.0\n5883.0\n1.0\n\n\nα_education[Basic education, Parempoolsed]\n0.332\n0.382\n-0.402\n1.037\n0.004\n0.003\n9250.0\n6320.0\n1.0\n\n\nα_education[Basic education, Reformierakond]\n-0.486\n0.092\n-0.658\n-0.317\n0.001\n0.001\n9580.0\n6485.0\n1.0\n\n\nα_education[Basic education, Rohelised]\n-0.054\n0.205\n-0.446\n0.317\n0.002\n0.002\n10698.0\n6133.0\n1.0\n\n\nα_education[Basic education, SDE]\n-0.426\n0.144\n-0.696\n-0.155\n0.001\n0.001\n11192.0\n6408.0\n1.0\n\n\nα_education[Higher education, EKRE]\n-0.399\n0.072\n-0.530\n-0.259\n0.001\n0.001\n8977.0\n6374.0\n1.0\n\n\nα_education[Higher education, Eesti 200]\n0.348\n0.084\n0.200\n0.515\n0.001\n0.001\n8692.0\n5956.0\n1.0\n\n\nα_education[Higher education, Undecided]\n-0.216\n0.071\n-0.343\n-0.075\n0.001\n0.001\n8929.0\n6048.0\n1.0\n\n\nα_education[Higher education, Isamaa]\n-0.110\n0.098\n-0.292\n0.076\n0.001\n0.001\n13473.0\n6687.0\n1.0\n\n\nα_education[Higher education, Keskerakond]\n-0.347\n0.072\n-0.481\n-0.212\n0.001\n0.001\n8540.0\n6733.0\n1.0\n\n\nα_education[Higher education, Mitte ükski erakond]\n-0.278\n0.064\n-0.395\n-0.158\n0.001\n0.001\n7817.0\n6747.0\n1.0\n\n\nα_education[Higher education, Other]\n-0.259\n0.219\n-0.655\n0.164\n0.002\n0.002\n10544.0\n6202.0\n1.0\n\n\nα_education[Higher education, Parempoolsed]\n0.414\n0.304\n-0.160\n0.981\n0.003\n0.003\n8511.0\n6058.0\n1.0\n\n\nα_education[Higher education, Reformierakond]\n0.397\n0.064\n0.279\n0.516\n0.001\n0.000\n8557.0\n6525.0\n1.0\n\n\nα_education[Higher education, Rohelised]\n0.021\n0.152\n-0.253\n0.314\n0.001\n0.002\n11264.0\n5934.0\n1.0\n\n\nα_education[Higher education, SDE]\n0.428\n0.094\n0.258\n0.610\n0.001\n0.001\n11307.0\n6711.0\n1.0\n\n\nα_education[Secondary education, EKRE]\n0.164\n0.067\n0.039\n0.290\n0.001\n0.000\n9386.0\n6782.0\n1.0\n\n\nα_education[Secondary education, Eesti 200]\n0.011\n0.085\n-0.150\n0.169\n0.001\n0.001\n11057.0\n6734.0\n1.0\n\n\nα_education[Secondary education, Undecided]\n0.106\n0.069\n-0.025\n0.234\n0.001\n0.001\n10110.0\n6960.0\n1.0\n\n\nα_education[Secondary education, Isamaa]\n0.220\n0.092\n0.041\n0.388\n0.001\n0.001\n13016.0\n6300.0\n1.0\n\n\nα_education[Secondary education, Keskerakond]\n-0.038\n0.070\n-0.167\n0.094\n0.001\n0.001\n10080.0\n6738.0\n1.0\n\n\nα_education[Secondary education, Mitte ükski erakond]\n0.052\n0.062\n-0.063\n0.170\n0.001\n0.001\n9273.0\n6817.0\n1.0\n\n\nα_education[Secondary education, Other]\n0.111\n0.201\n-0.277\n0.476\n0.002\n0.002\n11753.0\n6409.0\n1.0\n\n\nα_education[Secondary education, Parempoolsed]\n-0.745\n0.350\n-1.389\n-0.074\n0.004\n0.003\n9203.0\n6561.0\n1.0\n\n\nα_education[Secondary education, Reformierakond]\n0.088\n0.065\n-0.038\n0.207\n0.001\n0.001\n9345.0\n7035.0\n1.0\n\n\nα_education[Secondary education, Rohelised]\n0.033\n0.144\n-0.235\n0.302\n0.001\n0.002\n15300.0\n6318.0\n1.0\n\n\nα_education[Secondary education, SDE]\n-0.002\n0.097\n-0.187\n0.176\n0.001\n0.001\n9201.0\n7201.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata1b, var_names='α_education')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_education[Basic education, EKRE]\n0.191\n0.070\n0.055\n0.320\n0.001\n0.000\n13539.0\n6375.0\n1.0\n\n\nα_education[Basic education, Eesti 200]\n-0.235\n0.090\n-0.400\n-0.062\n0.001\n0.001\n11990.0\n7072.0\n1.0\n\n\nα_education[Basic education, Undecided]\n0.083\n0.073\n-0.060\n0.214\n0.001\n0.001\n12012.0\n6297.0\n1.0\n\n\nα_education[Basic education, Isamaa]\n-0.050\n0.093\n-0.231\n0.118\n0.001\n0.001\n13607.0\n6170.0\n1.0\n\n\nα_education[Basic education, Keskerakond]\n0.295\n0.074\n0.156\n0.434\n0.001\n0.000\n12671.0\n6260.0\n1.0\n\n\nα_education[Basic education, Mitte ükski erakond]\n0.189\n0.066\n0.060\n0.309\n0.001\n0.000\n12424.0\n6586.0\n1.0\n\n\nα_education[Basic education, Other]\n0.065\n0.135\n-0.184\n0.323\n0.001\n0.001\n13651.0\n6458.0\n1.0\n\n\nα_education[Basic education, Parempoolsed]\n0.097\n0.144\n-0.174\n0.365\n0.001\n0.001\n13581.0\n5944.0\n1.0\n\n\nα_education[Basic education, Reformierakond]\n-0.393\n0.070\n-0.517\n-0.256\n0.001\n0.000\n11899.0\n6095.0\n1.0\n\n\nα_education[Basic education, Rohelised]\n0.009\n0.122\n-0.223\n0.228\n0.001\n0.001\n13491.0\n6665.0\n1.0\n\n\nα_education[Basic education, SDE]\n-0.251\n0.097\n-0.426\n-0.063\n0.001\n0.001\n11271.0\n6692.0\n1.0\n\n\nα_education[Higher education, EKRE]\n-0.318\n0.059\n-0.430\n-0.208\n0.000\n0.000\n14386.0\n6470.0\n1.0\n\n\nα_education[Higher education, Eesti 200]\n0.297\n0.065\n0.177\n0.419\n0.001\n0.000\n12723.0\n5482.0\n1.0\n\n\nα_education[Higher education, Undecided]\n-0.155\n0.057\n-0.261\n-0.047\n0.001\n0.000\n10573.0\n6068.0\n1.0\n\n\nα_education[Higher education, Isamaa]\n-0.091\n0.076\n-0.235\n0.051\n0.001\n0.001\n14480.0\n6157.0\n1.0\n\n\nα_education[Higher education, Keskerakond]\n-0.249\n0.060\n-0.367\n-0.139\n0.001\n0.000\n12318.0\n6111.0\n1.0\n\n\nα_education[Higher education, Mitte ükski erakond]\n-0.214\n0.053\n-0.316\n-0.116\n0.000\n0.000\n11808.0\n6247.0\n1.0\n\n\nα_education[Higher education, Other]\n-0.080\n0.121\n-0.308\n0.149\n0.001\n0.001\n15069.0\n6587.0\n1.0\n\n\nα_education[Higher education, Parempoolsed]\n0.080\n0.137\n-0.178\n0.335\n0.001\n0.001\n12957.0\n6326.0\n1.0\n\n\nα_education[Higher education, Reformierakond]\n0.373\n0.050\n0.276\n0.462\n0.000\n0.000\n11685.0\n6693.0\n1.0\n\n\nα_education[Higher education, Rohelised]\n0.014\n0.105\n-0.181\n0.212\n0.001\n0.001\n16191.0\n6541.0\n1.0\n\n\nα_education[Higher education, SDE]\n0.344\n0.071\n0.214\n0.479\n0.001\n0.000\n10646.0\n5829.0\n1.0\n\n\nα_education[Secondary education, EKRE]\n0.127\n0.053\n0.031\n0.229\n0.000\n0.000\n12694.0\n6755.0\n1.0\n\n\nα_education[Secondary education, Eesti 200]\n-0.062\n0.065\n-0.185\n0.060\n0.001\n0.001\n11771.0\n6722.0\n1.0\n\n\nα_education[Secondary education, Undecided]\n0.072\n0.054\n-0.027\n0.177\n0.000\n0.000\n12113.0\n7153.0\n1.0\n\n\nα_education[Secondary education, Isamaa]\n0.140\n0.072\n0.007\n0.277\n0.001\n0.000\n14944.0\n6205.0\n1.0\n\n\nα_education[Secondary education, Keskerakond]\n-0.046\n0.055\n-0.144\n0.062\n0.000\n0.000\n12197.0\n6874.0\n1.0\n\n\nα_education[Secondary education, Mitte ükski erakond]\n0.025\n0.049\n-0.068\n0.115\n0.000\n0.000\n12274.0\n6695.0\n1.0\n\n\nα_education[Secondary education, Other]\n0.015\n0.119\n-0.205\n0.239\n0.001\n0.001\n14454.0\n6886.0\n1.0\n\n\nα_education[Secondary education, Parempoolsed]\n-0.177\n0.136\n-0.437\n0.075\n0.001\n0.001\n14120.0\n6326.0\n1.0\n\n\nα_education[Secondary education, Reformierakond]\n0.020\n0.050\n-0.073\n0.112\n0.000\n0.000\n12133.0\n7066.0\n1.0\n\n\nα_education[Secondary education, Rohelised]\n-0.023\n0.100\n-0.209\n0.162\n0.001\n0.001\n13259.0\n5827.0\n1.0\n\n\nα_education[Secondary education, SDE]\n-0.092\n0.070\n-0.223\n0.041\n0.001\n0.001\n11114.0\n6683.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata1, var_names='α_nationality')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_nationality[Estonian, EKRE]\n0.091\n0.064\n-0.031\n0.210\n0.001\n0.001\n7281.0\n6421.0\n1.0\n\n\nα_nationality[Estonian, Eesti 200]\n0.081\n0.072\n-0.059\n0.213\n0.001\n0.001\n8736.0\n6470.0\n1.0\n\n\nα_nationality[Estonian, Undecided]\n-0.287\n0.058\n-0.394\n-0.182\n0.001\n0.001\n6742.0\n6367.0\n1.0\n\n\nα_nationality[Estonian, Isamaa]\n0.935\n0.151\n0.660\n1.228\n0.002\n0.001\n8963.0\n5827.0\n1.0\n\n\nα_nationality[Estonian, Keskerakond]\n-0.791\n0.056\n-0.888\n-0.677\n0.001\n0.000\n6695.0\n6262.0\n1.0\n\n\nα_nationality[Estonian, Mitte ükski erakond]\n-0.422\n0.052\n-0.520\n-0.327\n0.001\n0.000\n6279.0\n6162.0\n1.0\n\n\nα_nationality[Estonian, Other]\n-0.407\n0.155\n-0.703\n-0.120\n0.001\n0.001\n12316.0\n6225.0\n1.0\n\n\nα_nationality[Estonian, Parempoolsed]\n0.095\n0.292\n-0.427\n0.655\n0.004\n0.003\n5923.0\n5253.0\n1.0\n\n\nα_nationality[Estonian, Reformierakond]\n0.780\n0.076\n0.639\n0.921\n0.001\n0.001\n8745.0\n6390.0\n1.0\n\n\nα_nationality[Estonian, Rohelised]\n0.039\n0.135\n-0.209\n0.290\n0.001\n0.001\n10870.0\n6037.0\n1.0\n\n\nα_nationality[Estonian, SDE]\n-0.113\n0.070\n-0.236\n0.024\n0.001\n0.001\n10342.0\n6071.0\n1.0\n\n\nα_nationality[Other, EKRE]\n-0.091\n0.064\n-0.210\n0.031\n0.001\n0.001\n7281.0\n6421.0\n1.0\n\n\nα_nationality[Other, Eesti 200]\n-0.081\n0.072\n-0.213\n0.059\n0.001\n0.001\n8736.0\n6470.0\n1.0\n\n\nα_nationality[Other, Undecided]\n0.287\n0.058\n0.182\n0.394\n0.001\n0.001\n6742.0\n6367.0\n1.0\n\n\nα_nationality[Other, Isamaa]\n-0.935\n0.151\n-1.228\n-0.660\n0.002\n0.001\n8963.0\n5827.0\n1.0\n\n\nα_nationality[Other, Keskerakond]\n0.791\n0.056\n0.677\n0.888\n0.001\n0.000\n6695.0\n6262.0\n1.0\n\n\nα_nationality[Other, Mitte ükski erakond]\n0.422\n0.052\n0.327\n0.520\n0.001\n0.000\n6279.0\n6162.0\n1.0\n\n\nα_nationality[Other, Other]\n0.407\n0.155\n0.120\n0.703\n0.001\n0.001\n12316.0\n6225.0\n1.0\n\n\nα_nationality[Other, Parempoolsed]\n-0.095\n0.292\n-0.655\n0.427\n0.004\n0.003\n5923.0\n5253.0\n1.0\n\n\nα_nationality[Other, Reformierakond]\n-0.780\n0.076\n-0.921\n-0.639\n0.001\n0.001\n8745.0\n6390.0\n1.0\n\n\nα_nationality[Other, Rohelised]\n-0.039\n0.135\n-0.290\n0.209\n0.001\n0.001\n10870.0\n6037.0\n1.0\n\n\nα_nationality[Other, SDE]\n0.113\n0.070\n-0.024\n0.236\n0.001\n0.001\n10342.0\n6071.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata1b, var_names='α_nationality')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_nationality[Estonian, EKRE]\n0.144\n0.057\n0.039\n0.251\n0.001\n0.000\n12637.0\n6667.0\n1.0\n\n\nα_nationality[Estonian, Eesti 200]\n0.112\n0.062\n-0.006\n0.229\n0.001\n0.000\n11945.0\n6603.0\n1.0\n\n\nα_nationality[Estonian, Undecided]\n-0.223\n0.049\n-0.315\n-0.136\n0.000\n0.000\n11503.0\n7039.0\n1.0\n\n\nα_nationality[Estonian, Isamaa]\n0.669\n0.101\n0.476\n0.854\n0.001\n0.001\n11064.0\n6258.0\n1.0\n\n\nα_nationality[Estonian, Keskerakond]\n-0.707\n0.046\n-0.795\n-0.621\n0.000\n0.000\n11305.0\n6511.0\n1.0\n\n\nα_nationality[Estonian, Mitte ükski erakond]\n-0.357\n0.043\n-0.439\n-0.277\n0.000\n0.000\n10679.0\n6792.0\n1.0\n\n\nα_nationality[Estonian, Other]\n-0.260\n0.121\n-0.489\n-0.035\n0.001\n0.001\n12611.0\n6075.0\n1.0\n\n\nα_nationality[Estonian, Parempoolsed]\n-0.091\n0.153\n-0.378\n0.192\n0.001\n0.001\n11387.0\n6724.0\n1.0\n\n\nα_nationality[Estonian, Reformierakond]\n0.758\n0.065\n0.630\n0.875\n0.001\n0.000\n12215.0\n6441.0\n1.0\n\n\nα_nationality[Estonian, Rohelised]\n0.026\n0.105\n-0.156\n0.236\n0.001\n0.001\n12021.0\n6444.0\n1.0\n\n\nα_nationality[Estonian, SDE]\n-0.071\n0.062\n-0.186\n0.043\n0.001\n0.000\n11030.0\n6307.0\n1.0\n\n\nα_nationality[Other, EKRE]\n-0.144\n0.057\n-0.251\n-0.039\n0.001\n0.000\n12637.0\n6667.0\n1.0\n\n\nα_nationality[Other, Eesti 200]\n-0.112\n0.062\n-0.229\n0.006\n0.001\n0.000\n11945.0\n6603.0\n1.0\n\n\nα_nationality[Other, Undecided]\n0.223\n0.049\n0.136\n0.315\n0.000\n0.000\n11503.0\n7039.0\n1.0\n\n\nα_nationality[Other, Isamaa]\n-0.669\n0.101\n-0.854\n-0.476\n0.001\n0.001\n11064.0\n6258.0\n1.0\n\n\nα_nationality[Other, Keskerakond]\n0.707\n0.046\n0.621\n0.795\n0.000\n0.000\n11305.0\n6511.0\n1.0\n\n\nα_nationality[Other, Mitte ükski erakond]\n0.357\n0.043\n0.277\n0.439\n0.000\n0.000\n10679.0\n6792.0\n1.0\n\n\nα_nationality[Other, Other]\n0.260\n0.121\n0.035\n0.489\n0.001\n0.001\n12611.0\n6075.0\n1.0\n\n\nα_nationality[Other, Parempoolsed]\n0.091\n0.153\n-0.192\n0.378\n0.001\n0.001\n11387.0\n6724.0\n1.0\n\n\nα_nationality[Other, Reformierakond]\n-0.758\n0.065\n-0.875\n-0.630\n0.001\n0.000\n12215.0\n6441.0\n1.0\n\n\nα_nationality[Other, Rohelised]\n-0.026\n0.105\n-0.236\n0.156\n0.001\n0.001\n12021.0\n6444.0\n1.0\n\n\nα_nationality[Other, SDE]\n0.071\n0.062\n-0.043\n0.186\n0.001\n0.000\n11030.0\n6307.0\n1.0\n\n\n\n\n\n\n\nGiven the above, I hereafter drop the first model with the more naive priors and continue with the second model with the more informative priors.\nNow to check the impact of each predictor on voter preference by examining the posterior distribution, its central tendency and spread and how it relates to the reference value of 0, which corresponds to to effect. Note that: * The orange number situates probabilistically the reference value (0=neither favorable nor unfavorable.) * The black line corresponds to the 94% Highest Density Interval (HDI) - the narrowest interval containing 94% of the posterior distribution. * The choice of 94% is a reminder that there is nothing magical about this number. Analysts should determine the appropriate size of the credibility interval that works best for their use case.\n* This corresponds to a proabilistic statement; i.e. there is a 94% chance that given this data and this model, a voter correponding to this category would have an an offset within the HDI.\n\nBaseline - average voter tendencies\n\nOn average there is a overwhelmingly positive outlook on EKRE, Eesti, Mitte, Reformierakond, and to a lesser extent, SDE. The average voter has also a high probability of being Undecided.\nOn the other hand, Isamaa, Parempoolsed, Rohelised and smaller parties grouped under the catch-all Other are not likely to be appealing to the average voter.\n\n\n\naz.plot_posterior(idata1b, var_names='α', ref_val=0, textsize=16);\n\n\n\n\n\n\n\n\n\nEducation:\n\nBasic Education:\n\nThere is a very high probability that a voter belonging ot this category will be in favor of EKRE, Keskerakond, or Mitte\nThere is a very high probability against such a voter favoring Reformierakond, and to a lesser extent, SDE.\n\nSecondary Education:\n\nVoter have high probability of favoring EKRE and to a lesser extent Isamaa\nThough not quite significant, voter are likely to be most hostile to SDE.\n\nHigher Education:\n\nVoters in this category are very hostile to EKRE, Mitte and Keskerakond and are unlikely to be undecided\nVoters are quite sympathetic to Eesti and Reformierakond.\n\n\n\n\naz.plot_posterior(idata1b, var_names='α_education', ref_val=0, backend_kwargs=dict(tight_layout=True), textsize=20);\n\n\n\n\n\n\n\n\n\nNationality\n\n\nEstonians are:\n\npredominantly favorable to EKRE, Isamaa, Reformierakond;\nlargely unfavorable to Keskerakond and Mitte, and are less likely to be Undecided.\n\nEthnic Russians (Other) on the other hand:\n\nfavor Keskerakond, Mitte, and are more likely to be Undecided;\nare hostile to Reformierakond and to a lesser extent EKRE.\n\n\n\nf, axs = pp.subplots(ncols=3, nrows=8, figsize=(15, 40))\nfor ax in axs.ravel()[-2:]:\n    ax.set_visible(False)\naz.plot_posterior(idata1b, var_names='α_nationality', backend_kwargs=dict(tight_layout=True), ref_val=0, ax=axs, textsize=10);\n                \n\n\n\n\n\n\n\n\n\n\nAdding gender and age group\nIn accordance with the Bayesian workflow, the next step is to build a slightly more complex model, in this case by adding gender and vote age group as predictors to nationality and education, and see if can extract further insight. The model type is the same, just with a total of 4 offsets to the baseline instead of 2. I will also use stronger informative priors as I did in the second model that I ended up using for my analysis. Finally, the prior distribution for all coefficients is again the ZeroSumNormal distribution that I use to avoid non-identifiability due to overparameterization. But first, to encode the additional predictors…\n\ngender_code, gender_label = data.gender.factorize(sort=True)\nage_gp_code, age_group_label = data.age_group.factorize(sort=True)\n\n\ncoords = {\n    'age group': age_group_label,\n    'education': education_label,\n    'gender': gender_label,\n    'nationality': nationality_label,\n    'party choice': party_choice_label,\n    'obs_idx': data.index\n}\n\n\ndef build_model2(coords, σ_baseline=1, σ_age=1, σ_edu=1, σ_gen=1, σ_nat=1):\n    with pm.Model(coords=coords) as model2:\n        age_gp_idx = pm.Data('age_gp', age_gp_code, dims='obs_idx')\n        educ_idx = pm.Data('education', education_code, dims='obs_idx')\n        gender_idx = pm.Data('gender', gender_code, dims='obs_idx')\n        nat_idx = pm.Data('nationality', nationality_code, dims='obs_idx')\n        party_choice_idx = pm.Data('party choice', party_choice_code, dims='obs_idx') \n        \n        α_baseline = pm.ZeroSumNormal('α_baseline', sigma=σ_baseline, dims='party choice')\n        α_age_group = pm.ZeroSumNormal('α_age_group', sigma=σ_age, dims=('age group', 'party choice'), n_zerosum_axes=2)\n        α_education = pm.ZeroSumNormal('α_education', sigma=σ_edu, dims=('education', 'party choice'), n_zerosum_axes=2)\n        α_gender = pm.ZeroSumNormal('α_gender', sigma=σ_gen, dims=('gender', 'party choice'), n_zerosum_axes=2)\n        α_nationality = pm.ZeroSumNormal('α_nationality', sigma=σ_nat, dims=('nationality', 'party choice'), n_zerosum_axes=2)\n\n        μ = α_baseline + α_age_group[age_gp_idx] + α_education[educ_idx] + α_gender[gender_idx] + α_nationality[nat_idx]\n        p = pm.math.softmax(μ, axis=-1)\n\n        _ = pm.Categorical('y', p=p, observed=party_choice_idx, dims='obs_idx')\n        \n    return model2\n\nmodel2 = build_model2(coords=coords, σ_baseline=0.3, σ_age=0.4, σ_edu=0.3, σ_gen=0.2, σ_nat=0.2)\nmodel2.to_graphviz()\n\n\n\n\n\n\n\n\n\nwith model2:\n    idata2 = pm.sample_prior_predictive()\n    idata2.extend(pm.sample(chains=4, draws=2000, random_seed=42))\n    idata2.extend(pm.sample_posterior_predictive(idata2))\n\nSampling: [y, α_age_group, α_baseline, α_education, α_gender, α_nationality]\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α_baseline, α_age_group, α_education, α_gender, α_nationality]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 140 seconds.\nSampling: [y, α_age_group, α_baseline, α_education, α_gender, α_nationality]\n\n\n\n\n\n\n\n\n\naz.plot_trace(idata2, backend_kwargs=dict(tight_layout=True));\n\n\n\n\n\n\n\n\n\naz.summary(idata2, var_names='α_baseline')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_baseline[EKRE]\n0.713\n0.061\n0.598\n0.826\n0.001\n0.000\n13176.0\n6608.0\n1.0\n\n\nα_baseline[Eesti 200]\n0.168\n0.074\n0.030\n0.305\n0.001\n0.000\n12135.0\n6429.0\n1.0\n\n\nα_baseline[Undecided]\n0.882\n0.059\n0.772\n0.995\n0.001\n0.000\n13437.0\n6458.0\n1.0\n\n\nα_baseline[Isamaa]\n-0.416\n0.093\n-0.586\n-0.239\n0.001\n0.001\n13360.0\n6345.0\n1.0\n\n\nα_baseline[Keskerakond]\n1.044\n0.056\n0.944\n1.157\n0.000\n0.000\n14123.0\n6809.0\n1.0\n\n\nα_baseline[Mitte ükski erakond]\n1.358\n0.051\n1.260\n1.448\n0.000\n0.000\n12378.0\n7131.0\n1.0\n\n\nα_baseline[Other]\n-1.457\n0.129\n-1.695\n-1.215\n0.001\n0.001\n13204.0\n6783.0\n1.0\n\n\nα_baseline[Parempoolsed]\n-1.982\n0.151\n-2.258\n-1.695\n0.001\n0.001\n14895.0\n6244.0\n1.0\n\n\nα_baseline[Reformierakond]\n0.674\n0.066\n0.549\n0.797\n0.001\n0.000\n13091.0\n6590.0\n1.0\n\n\nα_baseline[Rohelised]\n-1.073\n0.113\n-1.290\n-0.866\n0.001\n0.001\n13350.0\n6760.0\n1.0\n\n\nα_baseline[SDE]\n0.089\n0.078\n-0.052\n0.237\n0.001\n0.001\n11862.0\n6514.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata2, var_names='α_age_group')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_age_group[16-24, EKRE]\n-0.307\n0.122\n-0.537\n-0.080\n0.001\n0.001\n16900.0\n5979.0\n1.0\n\n\nα_age_group[16-24, Eesti 200]\n0.465\n0.121\n0.240\n0.696\n0.001\n0.001\n16699.0\n6892.0\n1.0\n\n\nα_age_group[16-24, Undecided]\n0.011\n0.114\n-0.205\n0.225\n0.001\n0.001\n14880.0\n5789.0\n1.0\n\n\nα_age_group[16-24, Isamaa]\n-0.379\n0.167\n-0.684\n-0.070\n0.001\n0.001\n17991.0\n5771.0\n1.0\n\n\nα_age_group[16-24, Keskerakond]\n-0.685\n0.150\n-0.959\n-0.400\n0.001\n0.001\n17828.0\n5346.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nα_age_group[75+, Other]\n-0.044\n0.252\n-0.509\n0.438\n0.002\n0.003\n16116.0\n5912.0\n1.0\n\n\nα_age_group[75+, Parempoolsed]\n0.156\n0.279\n-0.350\n0.715\n0.002\n0.003\n17245.0\n6678.0\n1.0\n\n\nα_age_group[75+, Reformierakond]\n0.209\n0.087\n0.052\n0.378\n0.001\n0.001\n14009.0\n6339.0\n1.0\n\n\nα_age_group[75+, Rohelised]\n-0.412\n0.234\n-0.838\n0.033\n0.002\n0.002\n14955.0\n6332.0\n1.0\n\n\nα_age_group[75+, SDE]\n-0.017\n0.138\n-0.287\n0.228\n0.001\n0.002\n14714.0\n5968.0\n1.0\n\n\n\n\n77 rows × 9 columns\n\n\n\n\naz.summary(idata2, var_names='α_education')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_education[Basic education, EKRE]\n0.208\n0.076\n0.068\n0.353\n0.001\n0.000\n14723.0\n6608.0\n1.0\n\n\nα_education[Basic education, Eesti 200]\n-0.371\n0.102\n-0.553\n-0.172\n0.001\n0.001\n12680.0\n6619.0\n1.0\n\n\nα_education[Basic education, Undecided]\n0.053\n0.080\n-0.096\n0.203\n0.001\n0.001\n15074.0\n7250.0\n1.0\n\n\nα_education[Basic education, Isamaa]\n-0.031\n0.107\n-0.235\n0.164\n0.001\n0.001\n14127.0\n6301.0\n1.0\n\n\nα_education[Basic education, Keskerakond]\n0.321\n0.084\n0.165\n0.476\n0.001\n0.000\n15304.0\n6390.0\n1.0\n\n\nα_education[Basic education, Mitte ükski erakond]\n0.175\n0.069\n0.044\n0.305\n0.001\n0.000\n14289.0\n6947.0\n1.0\n\n\nα_education[Basic education, Other]\n0.187\n0.166\n-0.128\n0.491\n0.001\n0.001\n16358.0\n5678.0\n1.0\n\n\nα_education[Basic education, Parempoolsed]\n0.304\n0.177\n-0.022\n0.640\n0.001\n0.001\n16022.0\n6039.0\n1.0\n\n\nα_education[Basic education, Reformierakond]\n-0.466\n0.077\n-0.623\n-0.336\n0.001\n0.000\n15343.0\n6863.0\n1.0\n\n\nα_education[Basic education, Rohelised]\n0.001\n0.143\n-0.281\n0.261\n0.001\n0.002\n17370.0\n6133.0\n1.0\n\n\nα_education[Basic education, SDE]\n-0.381\n0.115\n-0.588\n-0.158\n0.001\n0.001\n11402.0\n6367.0\n1.0\n\n\nα_education[Higher education, EKRE]\n-0.336\n0.064\n-0.453\n-0.214\n0.000\n0.000\n16515.0\n6292.0\n1.0\n\n\nα_education[Higher education, Eesti 200]\n0.365\n0.073\n0.230\n0.508\n0.001\n0.000\n13045.0\n6551.0\n1.0\n\n\nα_education[Higher education, Undecided]\n-0.173\n0.063\n-0.291\n-0.055\n0.001\n0.000\n15164.0\n6155.0\n1.0\n\n\nα_education[Higher education, Isamaa]\n-0.104\n0.084\n-0.255\n0.054\n0.001\n0.001\n16875.0\n6313.0\n1.0\n\n\nα_education[Higher education, Keskerakond]\n-0.280\n0.066\n-0.407\n-0.161\n0.001\n0.000\n15162.0\n6066.0\n1.0\n\n\nα_education[Higher education, Mitte ükski erakond]\n-0.231\n0.056\n-0.339\n-0.129\n0.000\n0.000\n14425.0\n6429.0\n1.0\n\n\nα_education[Higher education, Other]\n-0.145\n0.143\n-0.417\n0.121\n0.001\n0.001\n17730.0\n5713.0\n1.0\n\n\nα_education[Higher education, Parempoolsed]\n0.050\n0.157\n-0.249\n0.353\n0.001\n0.002\n16159.0\n6270.0\n1.0\n\n\nα_education[Higher education, Reformierakond]\n0.400\n0.055\n0.302\n0.507\n0.000\n0.000\n14947.0\n6182.0\n1.0\n\n\nα_education[Higher education, Rohelised]\n0.029\n0.119\n-0.198\n0.255\n0.001\n0.001\n18364.0\n5719.0\n1.0\n\n\nα_education[Higher education, SDE]\n0.425\n0.081\n0.272\n0.577\n0.001\n0.001\n11128.0\n6702.0\n1.0\n\n\nα_education[Secondary education, EKRE]\n0.128\n0.057\n0.027\n0.242\n0.001\n0.000\n12401.0\n6724.0\n1.0\n\n\nα_education[Secondary education, Eesti 200]\n0.006\n0.072\n-0.125\n0.144\n0.001\n0.001\n11841.0\n6781.0\n1.0\n\n\nα_education[Secondary education, Undecided]\n0.121\n0.058\n0.010\n0.227\n0.001\n0.000\n12417.0\n7297.0\n1.0\n\n\nα_education[Secondary education, Isamaa]\n0.135\n0.080\n-0.013\n0.285\n0.001\n0.001\n12403.0\n6082.0\n1.0\n\n\nα_education[Secondary education, Keskerakond]\n-0.041\n0.060\n-0.155\n0.065\n0.001\n0.001\n13487.0\n6578.0\n1.0\n\n\nα_education[Secondary education, Mitte ükski erakond]\n0.056\n0.051\n-0.039\n0.150\n0.000\n0.000\n12410.0\n7802.0\n1.0\n\n\nα_education[Secondary education, Other]\n-0.042\n0.135\n-0.291\n0.218\n0.001\n0.001\n16671.0\n6445.0\n1.0\n\n\nα_education[Secondary education, Parempoolsed]\n-0.355\n0.159\n-0.647\n-0.057\n0.001\n0.001\n14857.0\n6236.0\n1.0\n\n\nα_education[Secondary education, Reformierakond]\n0.067\n0.053\n-0.036\n0.165\n0.000\n0.000\n12447.0\n6660.0\n1.0\n\n\nα_education[Secondary education, Rohelised]\n-0.030\n0.110\n-0.240\n0.172\n0.001\n0.001\n14568.0\n6149.0\n1.0\n\n\nα_education[Secondary education, SDE]\n-0.044\n0.079\n-0.185\n0.112\n0.001\n0.001\n10521.0\n6345.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata2, var_names='α_gender')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_gender[Female, EKRE]\n-0.228\n0.041\n-0.308\n-0.157\n0.000\n0.000\n17338.0\n6441.0\n1.0\n\n\nα_gender[Female, Eesti 200]\n0.031\n0.047\n-0.057\n0.118\n0.000\n0.000\n16743.0\n6071.0\n1.0\n\n\nα_gender[Female, Undecided]\n0.178\n0.041\n0.101\n0.253\n0.000\n0.000\n15502.0\n6196.0\n1.0\n\n\nα_gender[Female, Isamaa]\n-0.209\n0.053\n-0.306\n-0.110\n0.000\n0.000\n18282.0\n5863.0\n1.0\n\n\nα_gender[Female, Keskerakond]\n0.059\n0.043\n-0.023\n0.139\n0.000\n0.000\n17925.0\n6789.0\n1.0\n\n\nα_gender[Female, Mitte ükski erakond]\n0.005\n0.036\n-0.062\n0.072\n0.000\n0.000\n13068.0\n5882.0\n1.0\n\n\nα_gender[Female, Other]\n-0.089\n0.092\n-0.259\n0.088\n0.001\n0.001\n15853.0\n5529.0\n1.0\n\n\nα_gender[Female, Parempoolsed]\n-0.018\n0.101\n-0.206\n0.175\n0.001\n0.001\n17983.0\n6359.0\n1.0\n\n\nα_gender[Female, Reformierakond]\n0.009\n0.035\n-0.056\n0.074\n0.000\n0.000\n14718.0\n6247.0\n1.0\n\n\nα_gender[Female, Rohelised]\n0.087\n0.078\n-0.056\n0.237\n0.001\n0.001\n17240.0\n6054.0\n1.0\n\n\nα_gender[Female, SDE]\n0.174\n0.050\n0.082\n0.270\n0.000\n0.000\n12087.0\n6810.0\n1.0\n\n\nα_gender[Male, EKRE]\n0.228\n0.041\n0.157\n0.308\n0.000\n0.000\n17338.0\n6441.0\n1.0\n\n\nα_gender[Male, Eesti 200]\n-0.031\n0.047\n-0.118\n0.057\n0.000\n0.000\n16743.0\n6071.0\n1.0\n\n\nα_gender[Male, Undecided]\n-0.178\n0.041\n-0.253\n-0.101\n0.000\n0.000\n15502.0\n6196.0\n1.0\n\n\nα_gender[Male, Isamaa]\n0.209\n0.053\n0.110\n0.306\n0.000\n0.000\n18282.0\n5863.0\n1.0\n\n\nα_gender[Male, Keskerakond]\n-0.059\n0.043\n-0.139\n0.023\n0.000\n0.000\n17925.0\n6789.0\n1.0\n\n\nα_gender[Male, Mitte ükski erakond]\n-0.005\n0.036\n-0.072\n0.062\n0.000\n0.000\n13068.0\n5882.0\n1.0\n\n\nα_gender[Male, Other]\n0.089\n0.092\n-0.088\n0.259\n0.001\n0.001\n15853.0\n5529.0\n1.0\n\n\nα_gender[Male, Parempoolsed]\n0.018\n0.101\n-0.175\n0.206\n0.001\n0.001\n17983.0\n6359.0\n1.0\n\n\nα_gender[Male, Reformierakond]\n-0.009\n0.035\n-0.074\n0.056\n0.000\n0.000\n14718.0\n6247.0\n1.0\n\n\nα_gender[Male, Rohelised]\n-0.087\n0.078\n-0.237\n0.056\n0.001\n0.001\n17240.0\n6054.0\n1.0\n\n\nα_gender[Male, SDE]\n-0.174\n0.050\n-0.270\n-0.082\n0.000\n0.000\n12087.0\n6810.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(idata2, var_names='α_nationality')\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα_nationality[Estonian, EKRE]\n0.169\n0.053\n0.073\n0.271\n0.000\n0.000\n14414.0\n6430.0\n1.0\n\n\nα_nationality[Estonian, Eesti 200]\n0.130\n0.057\n0.018\n0.233\n0.000\n0.000\n15715.0\n7377.0\n1.0\n\n\nα_nationality[Estonian, Undecided]\n-0.159\n0.046\n-0.244\n-0.072\n0.000\n0.000\n14413.0\n7107.0\n1.0\n\n\nα_nationality[Estonian, Isamaa]\n0.518\n0.079\n0.366\n0.660\n0.001\n0.000\n13569.0\n6383.0\n1.0\n\n\nα_nationality[Estonian, Keskerakond]\n-0.675\n0.045\n-0.756\n-0.590\n0.000\n0.000\n14815.0\n6751.0\n1.0\n\n\nα_nationality[Estonian, Mitte ükski erakond]\n-0.294\n0.040\n-0.373\n-0.222\n0.000\n0.000\n13450.0\n6685.0\n1.0\n\n\nα_nationality[Estonian, Other]\n-0.225\n0.095\n-0.400\n-0.044\n0.001\n0.001\n16229.0\n5764.0\n1.0\n\n\nα_nationality[Estonian, Parempoolsed]\n-0.145\n0.106\n-0.340\n0.052\n0.001\n0.001\n15378.0\n6900.0\n1.0\n\n\nα_nationality[Estonian, Reformierakond]\n0.730\n0.057\n0.625\n0.837\n0.000\n0.000\n13500.0\n6581.0\n1.0\n\n\nα_nationality[Estonian, Rohelised]\n-0.015\n0.088\n-0.183\n0.145\n0.001\n0.001\n14598.0\n6308.0\n1.0\n\n\nα_nationality[Estonian, SDE]\n-0.033\n0.058\n-0.140\n0.080\n0.001\n0.001\n11830.0\n6737.0\n1.0\n\n\nα_nationality[Other, EKRE]\n-0.169\n0.053\n-0.271\n-0.073\n0.000\n0.000\n14414.0\n6430.0\n1.0\n\n\nα_nationality[Other, Eesti 200]\n-0.130\n0.057\n-0.233\n-0.018\n0.000\n0.000\n15715.0\n7377.0\n1.0\n\n\nα_nationality[Other, Undecided]\n0.159\n0.046\n0.072\n0.244\n0.000\n0.000\n14413.0\n7107.0\n1.0\n\n\nα_nationality[Other, Isamaa]\n-0.518\n0.079\n-0.660\n-0.366\n0.001\n0.000\n13569.0\n6383.0\n1.0\n\n\nα_nationality[Other, Keskerakond]\n0.675\n0.045\n0.590\n0.756\n0.000\n0.000\n14815.0\n6751.0\n1.0\n\n\nα_nationality[Other, Mitte ükski erakond]\n0.294\n0.040\n0.222\n0.373\n0.000\n0.000\n13450.0\n6685.0\n1.0\n\n\nα_nationality[Other, Other]\n0.225\n0.095\n0.044\n0.400\n0.001\n0.001\n16229.0\n5764.0\n1.0\n\n\nα_nationality[Other, Parempoolsed]\n0.145\n0.106\n-0.052\n0.340\n0.001\n0.001\n15378.0\n6900.0\n1.0\n\n\nα_nationality[Other, Reformierakond]\n-0.730\n0.057\n-0.837\n-0.625\n0.000\n0.000\n13500.0\n6581.0\n1.0\n\n\nα_nationality[Other, Rohelised]\n0.015\n0.088\n-0.145\n0.183\n0.001\n0.001\n14598.0\n6308.0\n1.0\n\n\nα_nationality[Other, SDE]\n0.033\n0.058\n-0.080\n0.140\n0.001\n0.001\n11830.0\n6737.0\n1.0\n\n\n\n\n\n\n\nThe fit took predictably longer; 2 minutes and 20 seconds on my macbook air m2. Moreover, * There were no divergences; * Posteriors look sound and the traces exhibit good mixing for all chains; * All \\(\\hat{R}\\) values were 1 * Bulk and tail ESS are about the same as the model with two predictors and relatively informative priors suggesting sampling efficiency has not deteriorated as a result of the increased model complexity."
  }
]