[
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html",
    "href": "blog/aws_ab_test/aws_ab_test.html",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "",
    "text": "This post walks through how I deployed a lightweight Streamlit demo app on AWS EC2 Free Tier. I made a series of intentional choices to keep things minimal, secure, and reproducible."
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#goal",
    "href": "blog/aws_ab_test/aws_ab_test.html#goal",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "Goal",
    "text": "Goal\nI wanted to:\n\nHost a public demo of a Streamlit A/B testing app\nUse the AWS Free Tier to avoid costs\nInclude default data loaded from S3\nAvoid heavyweight stack setups like Docker or full Conda stacks\nEnsure others could access the app from anywhere"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#why-ec2-and-not-streamlit-cloud",
    "href": "blog/aws_ab_test/aws_ab_test.html#why-ec2-and-not-streamlit-cloud",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "1. Why EC2 and Not Streamlit Cloud?",
    "text": "1. Why EC2 and Not Streamlit Cloud?\nWhile Streamlit Community Cloud is great for hobby projects, I wanted full control over the environment (especially for installing specific Python versions and libraries). EC2 gives you a real server to manage — and the Free Tier includes 750 hours/month of t2.micro time."
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#ec2-setup-overview",
    "href": "blog/aws_ab_test/aws_ab_test.html#ec2-setup-overview",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "2. EC2 Setup Overview",
    "text": "2. EC2 Setup Overview\nI launched an EC2 instance with the following config:\n\nAMI: Amazon Linux 2023 (stable, small footprint)\nInstance type: t2.micro (Free Tier)\nSecurity group settings:\n\nPort 22 (SSH): Open to my IP for secure shell access\nPort 8501 (Streamlit): Open to 0.0.0.0/0 so others can try the app"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#why-i-used-miniconda-instead-of-system-pip",
    "href": "blog/aws_ab_test/aws_ab_test.html#why-i-used-miniconda-instead-of-system-pip",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "3. Why I Used Miniconda Instead of System pip",
    "text": "3. Why I Used Miniconda Instead of System pip\nThe system Python on Amazon Linux is Python 3.9 — too old for some of my requirements (arviz&gt;=0.21.0, numpy&gt;=2.3). Rather than downgrade packages, I:\n\nInstalled Miniconda manually (lightweight and isolated)\nCreated a conda env with Python 3.12\nInstalled dependencies using pip inside the conda env\n\nThis gave me flexibility without modifying system Python."
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#folder-structure",
    "href": "blog/aws_ab_test/aws_ab_test.html#folder-structure",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "4. Folder Structure",
    "text": "4. Folder Structure\n.\n├── abtest-demo/\n│   ├── streamlit_app_1.py\n│   ├── requirements.txt\n│   └── ... (more files coming soon)\n└── miniconda3/"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#installing-dependencies",
    "href": "blog/aws_ab_test/aws_ab_test.html#installing-dependencies",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "5. Installing Dependencies",
    "text": "5. Installing Dependencies\nI generated a clean requirements.txt using pipreqs to only include the packages I actually import.\npip install -r requirements.txt"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#running-the-app-publicly",
    "href": "blog/aws_ab_test/aws_ab_test.html#running-the-app-publicly",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "6. Running the App Publicly",
    "text": "6. Running the App Publicly\nstreamlit run streamlit_app_1.py \\\n  --server.port 8501 \\\n  --server.address 0.0.0.0 \\\n  --server.enableCORS false \\\n  --server.enableXsrfProtection false\nThe CORS and XSRF flags prevent browser-side security errors when the app is accessed via IP. I then visited the app at:\nhttp://&lt;ec2-public-ip&gt;:8501"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#default-data-from-s3",
    "href": "blog/aws_ab_test/aws_ab_test.html#default-data-from-s3",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "7. Default Data from S3",
    "text": "7. Default Data from S3\nI uploaded a default dataset to an S3 bucket called ab-test-demo-data to demonstrate the app’s functionality:\nimport pandas as pd\ndf = pd.read_csv(\"https://ab-test-demo-data.s3.us-east-2.amazonaws.com/default_data.csv\")"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#security-cost-control",
    "href": "blog/aws_ab_test/aws_ab_test.html#security-cost-control",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "8. Security & Cost Control",
    "text": "8. Security & Cost Control\nTo avoid unnecessary exposure and costs:\n\nI keep SSH access limited to my IP\nI shut down the EC2 instance when not in use:\n\nsudo shutdown now"
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#in-conclusion",
    "href": "blog/aws_ab_test/aws_ab_test.html#in-conclusion",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "In Conclusion",
    "text": "In Conclusion\nThis setup is:\n\n✅ Minimal (no Docker, no system Python mods)\n✅ Secure (IP-locked SSH, no secrets exposed)\n✅ Publicly accessible (so others can interact with my work)\n✅ Cost-free within AWS Free Tier\n\nIt also gives me flexibility to grow the project — I can add analytics, a custom domain, even TLS later if needed."
  },
  {
    "objectID": "blog/aws_ab_test/aws_ab_test.html#source-code",
    "href": "blog/aws_ab_test/aws_ab_test.html#source-code",
    "title": "Deploying a Streamlit App on AWS EC2 (Free Tier)",
    "section": "Source Code",
    "text": "Source Code\nIf you’d like to see how I built the Bayesian A/B testing logic in the app itself, check out the source code on GitHub."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Musings & Crumbs",
    "section": "",
    "text": "A Bayesian Approach to Marine Modeling\n\n\nFitting data to a system of Differential Equations\n\n\n\nErdem Karaköylü\n\n\nMar 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Power Analysis for A/B Testing\n\n\nA bayesian approach to sample size planning for A/B testing.\n\n\n\nErdem Karaköylü\n\n\nMay 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausality in observational data\n\n\nMatching and propensity scores for causal inference\n\n\n\nErdem Karaköylü\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying a Streamlit App on AWS EC2 (Free Tier)\n\n\nA lightweight and secure Streamlit demo setup using Amazon Linux, Miniconda, and S3 for sharing A/B testing work publicly.\n\n\n\n\n\n\nJun 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Statistical Practices in A/B Testing\n\n\nThe inadequacy of p-values and what to do about it\n\n\n\nErdem Karaköylü\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeeking into the Black Box: Explaining Plankton Predictions from Space\n\n\nUsing SHAP values to uncover how a machine learning model predicts ocean life from satellite data\n\n\n\nErdem Karaköylü\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatellite Oceanography without Atmospheric Correction\n\n\nInferring Phytoplankton Absorption with BART\n\n\n\nErdem Karaköylü\n\n\nMay 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ZeroSumNormal Distribution\n\n\nA principled approach to avoinding overparameterization and non-identifiability in categorical regression\n\n\n\nErdem Karaköylü\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html",
    "href": "portfolio/pfg_xgb/index.html",
    "title": "Inferring PCC from Far Far Away",
    "section": "",
    "text": "📂 Code & Notebooks\n\n\n\nAll code and demo notebooks are available in the GitHub repository."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#introduction",
    "href": "portfolio/pfg_xgb/index.html#introduction",
    "title": "Inferring PCC from Far Far Away",
    "section": "1 Introduction",
    "text": "1 Introduction\nPhytoplankton community composition (PCCs) play a fundamental role in marine biogeochemical cycles, influencing carbon sequestration, nutrient fluxes, and global climate feedbacks. Different functional groups contribute uniquely to these processes; for example, diatoms facilitate carbon export through rapid sinking, cyanobacteria fix atmospheric nitrogen, and coccolithophores regulate carbonate chemistry via calcification (Marañón and Cermeño P. 2014; Boyd and Doney 2002). Identifying and quantifying these groups from space is crucial for understanding their ecological functions, detecting environmental changes, and improving ocean biogeochemical models (Bopp et al. 2005; Laufkötter et al. 2016). However, current satellite ocean-color products primarily provide total chlorophyll a concentrations, which do not directly indicate community composition. To address this gap, various remote sensing algorithms have been developed to infer phytoplankton diversity, each with limitations in distinguishing certain groups and quantifying their biomass accurately (Mouw et al. 2017).\n\n1.1 Remote Sensing Approaches for PCC Retrieval\nPhytoplankton classification from satellite remote sensing has traditionally relied on empirical and semi-analytical methods. Empirical band-ratio techniques, such as PHYSAT, classify dominant phytoplankton groups based on anomalies in spectral reflectance but are often region-specific and limited to broad functional classes (Alvain et al. 2005, 2008). Semi-analytical models, in contrast, use inherent optical properties (IOPs) to infer phytoplankton composition from satellite reflectance, providing a more mechanistic approach (Hirata et al. 2011). Hybrid models incorporate additional environmental variables, such as sea surface temperature and total chlorophyll, to infer community structure (Brewin et al. 2010).\nHyperspectral ocean-color sensors, such as NASA’s Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) mission, may have the potential to improve PCC retrieval by capturing finer spectral features associated with phytoplankton pigments (Dierssen et al. 2023). That is not to say that hyperspectral resolution is sufficient on its own. Optical similarity between different groups, depth-related biases in surface measurements, as well as associated measurement uncertainties inherenth to the noisy marine environment will likely hinder retrieval accuracy (IOCCG 2014). Most current models either estimate phytoplankton size classes or assign a single dominant group per pixel, often failing to capture the complexity of mixed communities (Ciotti, Lewis, and Cullen 2002).\n\n\n1.2 Study Contribution and Approach\nIn this study, we use a scalable, high-performance ensemble learning algorithm; extreme gradient boosting (XGBoost) (Chen and Guestrin 2016) to identify phytoplankton community composition from satellite ocean color data. The ensemble aspect of this algorithm makes it more resistant to overfitting. XGBoost is less opaque than neural networks. Finally, XGBoost is robust to data issues that are problematic for other machine learning models such as Neural Networks, including highly correlated (spectral) data or data of varying scales.\nXGBoost outperformed alternative approaches in applications such as harmful algal bloom detection (Izadi et al. 2021) and phytoplankton biomass estimation (Yan et al. 2025), which highlight the suitability of the approach1 for remote sensing applications. Our work aligns with the objectives of the PACE mission by contributing an advanced classification algorithm that enhances hyperspectral monitoring of phytoplankton diversity (Zhang et al. 2024). To our knowledge, this is the first application of XGBoost for PCC classification in ocean color remote sensing, offering a robust alternative to traditional retrieval methods.\nOur approach was to leverage a large dataset of simulated hyperspectral TOA radiance and associated environmental variables to improve both the discrimination of functional groups and the quantification of their biomass. Previous remote sensing algorithms often classified only a dominant PCC or broad size class (Mouw et al. 2017) and relied on empirical band relationships that lacked generalizability (Hirata et al. 2011). By utilizing a machine learning framework capable of integrating multiple features, our approach reduces classification errors and enhances retrieval precision. Moreover the application of eXplainable AI (XAI) techniques to relate predictions to their input may further guide future efforts to improve PCC quantification."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#methods",
    "href": "portfolio/pfg_xgb/index.html#methods",
    "title": "Inferring PCC from Far Far Away",
    "section": "2 Methods",
    "text": "2 Methods\n\n2.1 Data Preparation and Feature Selection\nWe utilized a simulated dataset representing the world ocean over 31 days, corresponding to December 2021. The simulation generated hyperspectral remote sensing TOA radiance data, emulating a sensor configuration akin to that of the PACE instrument. Due to the high dimensionality of the original spectral data, we conducted an initial exploratory analysis and observed strong correlations among many of the channels. To reduce redundancy while preserving essential spectral information, we retained 51 channels by selecting one channel every ten. Note that despite this feature subsampling, spectral features are characterized by high degree of correlation we opted against applying principal component analysis for two principal reasons. The first reason is to avoid overemphasizing blue water signal contributed by the extensive oceanic regions present in a global satellite scene, and which could mask coastal processes of interest. The second reason is that tree-based algorithms such as XGBoost are resiliant to input multicollinearity. To further contextualize the ocean color signal, we also included auxiliary environmental variables such as temperature and latitude. Though not available from actual PACE measurements, climatology including temperature could be readily sourced elsewhere to augment observations on hand.\nThe dataset was divided into training and test sets using an 80/20 split. The training set was exclusively used for model development and hyperparameter optimization (see next section), while the test set was set aside until the final validation of model performance.\n\n\n2.2 Model Choice\nWe employed an XGBoost Regressor model with a multi-output regression head to predict simultaneously multiple phytoplankton functional groups as well as total chlorophyll-a concentration. XGBoost is a high-performance, scalable implementation of gradient boosting that has become a popular choice for a wide range of regression and classification tasks (Chen and Guestrin 2016). This approach consists in building an ensemble of decision trees sequentially, where each new tree attempts to correct the errors made by the previous trees. By optimizing a regularized objective function, XGBoost effectively controls overfitting while enhancing prediction accuracy. Its efficient handling highly correlated data, support for parallel computation, and flexible regularization mechanisms make it particularly well-suited for complex modeling tasks.\n\n\n2.3 Hyperparameter Optimization and Model Training\nGiven the complexity of the problem and the high dimensionality of the input features, it was critical to optimize the hyperparameters to achieve robust performance and prevent overfitting. To this end, we conducted hyperparameter optimization using the Optuna library (Akiba et al. 2019). Specifically, we employed the efficient Tree-structured Parzen Estimator (TPE) algorithm (Bergstra et al. 2011). TPE is a Bayesian optimization method that iteratively builds probabilistic models of the hyperparameter space based on past evaluation results. By modeling the distributions of promising and less promising hyperparameter configurations, TPE suggests new parameter sets to explore, focusing the search on regions likely to yield improved performance. To further enhance the efficiency of the optimization process, we utilized Optuna’s MedianPruner with n_warmup_steps=5. This pruner automatically stops unpromising trials during the early stages of training (after at least 5 steps) if their intermediate results indicate they are unlikely to outperform the median performance of completed trials. The optimization step used an objective function to minimize the root mean squared error (RMSE) computed via three-fold cross-validation on the training set. The hyperparameters under investigation are lthe learning rate, maximum tree depth, number of estimators, subsample ratio, column subsample ratio, and gamma (the minimum loss reduction required to make a further partition on a leaf node); cf Table 1 for further details. The Bayesian optimization procedure allowed us to efficiently explore the hyperparameter space by leveraging past trial information to prune unpromising candidate parameter sets early, thereby reducing overall computational cost.\nOnce the optimization step complete, we instantiated the XGBoost model with the best set of hyperparameters and trained it on the full training set.\n\n\n\nTable 1: Hyperparameter ranges and their corresponding sampling strategy used in optimization.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter\nLow bound\nHigh bound\nSampling Distribution\n\n\n\n\nLearning rate\n\\(10^{-3}\\)\n\\(0.3\\)\nLog Uniform\n\n\nMax. tree depth\n\\(3\\)\n\\(10\\)\nUniform Integer\n\n\nEstimator number\n\\(50\\)\n\\(500\\)\nUniform Integer\n\n\nRow sample fraction\n\\(0.5\\)\n\\(1.0\\)\nUniform Float\n\n\nColumn sample frac.\n\\(0.5\\)\n\\(1.0\\)\nUniform Float\n\n\nGamma\n\\(10^{-8}\\)\n\\(1.0\\)\nLog Uniform\n\n\n\n\n\n\n\n\n2.4 Sensitiviy Analysis to Spectral Resolution through Band Downsampling\nTo assess the impact of spectral resolution on model performance, we conducted a sensitivity analysis by subsampling the hyperspectral input data to approximate the band configurations of MODIS and VIIRS sensors. Specifically, we selected the closest available channels in our simulated dataset to match the central wavelengths of MODIS and VIIRS ocean color bands (limited to &lt;750 nm), while retaining temperature as an auxiliary predictor. For both sensor configurations, we trained new models using the original set of hyperparameters optimized for the full hyperspectral dataset. This approach enabled a controlled comparison in which only the input features were varied, allowing us to isolate the impact of reduced spectral resolution on predictive skill. The same train/test split used in the initial model development was retained to ensure comparability of performance metrics across configurations, with the goal of measuring sensitivity rather than generalizability.\n\n\n2.5 Model Evaluation and eXplainable AI (XAI)\nOnce the optimal hyperparameter combination was identified, we retrained the final XGBoost model on the full training set using these optimized settings. Finally, we evaluated the performance of the retrained model on the held-out test set to assess its generalizability.\n\n2.5.1 Prediction Explainability\nTo enhance interpretability and gain insights into how different input features influence model predictions, we employed Shapley Additive Explanations (SHAP), a widely used explainable AI (XAI) framework for interpreting complex machine learning models. SHAP is named after the concept of Shapley values, which consists in assigning importance values to each input feature by estimating its contribution to the model’s predictions across different samples. The method is rooted in cooperative game theory, and guarantees a fair distribution of importance scores among features (Lundberg and Lee 2017).\nGiven the computational complexity of our XGBoost model and the high dimensionality of the dataset, we conducted SHAP analysis on a random subsample of 10,000 observations from the test set. This subset was selected to balance computational feasibility while maintaining a representative sample of phytoplankton spectral diversity.\nWe generated SHAP summary plots, which provide a comprehensive visualization of feature importance and the directionality of their influence on model outputs. These plots display the magnitude of each feature’s impact across all predictions, helping to identify the most influential spectral and environmental variables in determining phytoplankton functional group composition. The insights gained from SHAP analysis aid in validating model behavior and ensuring its ecological plausibility.\n\n2.5.1.1 Code Availability\nAll analysis and modeling code used in this study was written in Python 3.12. This code is publicly available on GitHub."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#results",
    "href": "portfolio/pfg_xgb/index.html#results",
    "title": "Inferring PCC from Far Far Away",
    "section": "3 Results",
    "text": "3 Results\n\n3.1 Hyperparameter Optimization (HPO)\nWe performed hyperparameter optimization using a Bayesian optimization framework implemented with Optuna. The metric used for optimization was the average RMSE (in units of \\(mgL^{-1} Chl_a\\) ) computed over the cross-validation folds and across all target compartments. The “full HPO run” best parameters indicate a relatively aggressive model, characterized by deep trees with many estimators, a moderate learning rate, and little regularization via gamma.\nThe best trial finished with an RMSE of \\(0.116mgL^{-1} Chl_a\\). Below is the list of hyperparameters researched, the optimal values found, and an interpretation of these values:\n\nLearning Rate (learning_rate): \\(0.083\\) - This moderate learning rate suggests the model takes reasonably sized steps when updating that are neither too aggressive (which might lead to overshooting the optimum) nor too conservative (which could slow down convergence).\nMax Depth (max_depth): \\(10\\) - A depth of 10 allows the trees to capture complex interactions. This may indicate that the data has non-linear relationships that benefit from deeper trees. Such a depth can be associated with overfitting. The cross-validation process during HPO should minimize this however.\nNumber of Estimators (n_estimators): \\(466\\) -Building around 466 trees indicates the ensemble haa to tackle inherent complexity in the data that was not apparetn during the Exploratory Data Analysis phase. A larger number of trees generally improves performance—up to a point before overfitting becomse a risk. This number in conjunction with the cross validation process suggest this number strikes a balance between performance and overfitting.\nSubsampling (subsample): \\(0.658\\) - This indicates each of the 466 trees is using roughly 66% of the data. This introduces randomness that helps prevent overfitting as not all samples in any cross-validation fold are used to build every tree.\nFeatures used per tree (colsample_bytree): \\(0.894\\) - Using about 89% of the features per tree indicates that most features are informative, and the model is allowed to consider almost the full feature set at each split. - See features used in the Methods section.\nGamma (gamma): \\(8.63e-06\\) - An extremely low gamma value means that almost no minimum loss reduction is required to make a split. This implies that the algorithm will split more readily, potentially capturing fine details. Awareness of this hyperparameter values is important as low gamma can risk overfitting.\n\n\n\n3.2 Optimized Model Validation\nThe next step was to load the best set of hyperparameter (listed above) into the model and retrain the model on the entire training set. The optimized and trained model was then validated using the test set, which prior to the HPO process and until this step had been set aside.\n\n\n\n\n\n\nFigure 1: Goodness-of-fit plots for all groups and total chorophyll a, measured on out-of-sample data set. THe model is able to predict with very good accuracy. Dinoflagellates are the notable exception.\n\n\n\nA more complete set of metrics are summarized in table Table 2 See further below for metrics explanation.\n\n\n\nTable 2: Performance metrics of optimized and trained model on hold-out set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDiatom\nChloroph.\nCyanobac\nCoccolith.\nDinoflag.\nPhaeo\nTot. Chl_a\n\n\n\n\nMSE\n0.00034\n0.00010\n2.89e-06\n8.59e-05\n1.96e-05\n0.00011\n0.000193\n\n\nRMSE\n0.0184\n0.0100\n0.0017\n0.00927\n0.00443\n0.0105\n0.0139\n\n\nMAE\n0.00878\n0.0042\n0.00078\n0.0042\n0.000637\n0.00313\n0.00728\n\n\nR-squared\n0.979\n0.958\n0.996\n0.985\n0.530\n0.999\n0.999\n\n\nMAE/StDev\n0.0691\n0.0858\n0.0302\n0.0563\n0.0986\n0.00754\n0.0182\n\n\n\n\n\n\n\n3.2.1 Explanation of metrics\n\nMean Squared Error (MSE):\nMSE is the average of the squared differences between the predicted and true values. Squaring the errors emphasizes larger deviations, making MSE sensitive to outliers. In our context, MSE is expressed in units of (mg L\\(^{-1}\\) Chl\\(_a\\))\\(^2\\). Lower MSE values indicate better model performance.\nRoot Mean Squared Error (RMSE):\nRMSE is the square root of the MSE, bringing the error metric back to the original units (mg L\\(^{-1}\\) Chl\\(_a\\)). It provides a direct measure of the average prediction error magnitude. Lower RMSE values suggest that the model’s predictions are closer to the true values.\nMean Absolute Error (MAE):\nMAE calculates the average absolute difference between predicted and true values. Unlike MSE, it does not square the errors, so it is less sensitive to large outliers. MAE is also expressed in the same units as the target variable (mg L\\(^{-1}\\) Chl\\(_a\\)). A lower MAE indicates better predictive accuracy.\nCoefficient of Determination (R-squared):\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where a value closer to 1 indicates that the model explains a high proportion of the variance in the data. In our results, high R-squared values generally indicate strong model performance, although lower values (e.g., for dinoflagellates) suggest room for improvement.\nMAE/StDev\\(_{true}\\):\nThis ratio compares the mean absolute error to the standard deviation of the true values. It provides a relative measure of error by indicating how the average error compares to the inherent variability in the data. A lower ratio implies that the model’s prediction error is small relative to the natural variability of the observations.\n\n\n\n\n3.3 XAI with Shapley Values\nThe SHAP summary plots provides insights into feature importance and their effects on model predictions for phytoplankton community composition.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\nFigure 2: Shapley values are shown for each functional group and for total chlorophyll. Features are ranked by most to least impactful, from top to bottom. Only the top 6 predictive features are shown. Along the x-axis, positive SHAP values indicate a positive relationship with the predicted; negative values, a negative one. Wider sections indicate greater variability. The color gradient represents feature values, with red for high values and blue for low values. The midpoint of the color bar reflects a percentile-based central value, not necessarily the mean, median, or mode, as it depends on the feature’s distribution.\n\n\n\nMost saliently, temperature was the top factor for all phytoplankton groups but is not a primordial feature in quantifying dinoflagellates.\n\n\n3.4 Spectral Resolution Sensitivity Analysis\nTo evaluate the sensitivity of model performance to spectral input resolution, we conducted additional experiments using reduced-band versions of the input data, corresponding to the band configurations of the MODIS and VIIRS ocean color sensors. For each sensor-specific dataset, we selected the closest matching channels from the original simulated hyperspectral inputs (limited to &lt;750 nm), while retaining temperature as an auxiliary predictor.\nAll modeling conditions—hyperparameters, training procedure, and the original 80/20 train-test split—were held constant to isolate the effect of spectral resolution. Model performance was evaluated using multiple regression metrics across PCCs, including diatoms, chlorophytes, cyanobacteria, coccolithophores, dinoflagellates, and prasinophytes, as well as total chlorophyll-a.\nFigure Figure 3 summarizes the RMSE values for each model configuration. As expected, both band-limited models showed increased prediction error relative to the full hyperspectral model. The MODIS-band model exhibited an RMSE increase of 1.5–2.5× across most groups, with declines in R² particularly evident for diatoms, coccolithophores, and total chlorophyll. The VIIRS-band model showed consistently lower errors and higher explained variance than the MODIS counterpart, indicating better preservation of model skill under reduced spectral input.\n\n\n\n\n\n\nFigure 3: RMSE across functional groups for three input configurations: full hyperspectral (PACE-like), MODIS-band subset, and VIIRS-band subset. The full model consistently outperforms both reduced-band versions.\n\n\n\nTo complement the aggregated metrics, Figures Figure 4 and Figure 5 show predicted versus true values for each group using the MODIS and VIIRS subsets, respectively. While both reduced models show reasonable alignment along the 1:1 line, greater scatter and bias are evident relative to the hyperspectral model, particularly for diatoms and dinoflagellates.\n\n\n\n\n\n\nFigure 4: Predicted vs. true phytoplankton concentrations using the MODIS-band subset. A dashed 1:1 line indicates perfect prediction. Wider spread around the diagonal reflects increased prediction error due to reduced spectral input.\n\n\n\n\n\n\n\n\n\nFigure 5: Predicted vs. true phytoplankton concentrations using the VIIRS-band subset. Despite limited input features, the model maintains good predictive performance across most groups, particularly compared to the MODIS configuration."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#discussion",
    "href": "portfolio/pfg_xgb/index.html#discussion",
    "title": "Inferring PCC from Far Far Away",
    "section": "4 Discussion",
    "text": "4 Discussion\nThe results presented here demonstrate that machine learning, particularly XGBoost, can effectively retrieve phytoplankton functional group (PCC) concentrations and total chlorophyll-a from hyperspectral top-of-atmosphere (TOA) data and auxiliary variables. The model performed well across most PCCs, with root mean squared error (RMSE) values under 0.02 mg m⁻³ and R² values exceeding 0.95. The exception was dinoflagellates, which exhibited significantly lower predictive accuracy (R² ≈ 0.53), a result also reflected in their higher normalized MAE and MAPE values.\nFeature importance analysis using SHAP values revealed that temperature was among the top six predictors for all groups except dinoflagellates, for which no strong dependence on any single environmental feature emerged. These patterns point to functional differences in ecological drivers between PCCs. In particular, the centrality of temperature for most groups highlights its role as a proxy for environmental gradients, biogeography, and metabolic scaling.\nDinoflagellates do not exhibit a single, well-defined biogeographical zone of dominance the way cyanobacteria (tropical oligotrophic waters), diatoms (high latitudes and upwelling systems), or coccolithophores (subpolar blooms) do (@ Buitenhuis 2013; Gregg and Rousseaux 2019). Instead, they are often considered ecological opportunists, occupying a broad range of regions, particularly stratified, nutrient-depleted, and temperate to tropical environments (Smayda and Reynolds 2001; Kibler 2015). Their distribution is governed less by temperature per se than by water column stability, nutrient availability (especially high N:P conditions), and their ability to exploit mixotrophic strategies (Jeong 2010; P. M. et al. Glibert 2001; Flynn 2013). Several studies suggest that warming-driven increases in stratification may promote shifts from diatom- to dinoflagellate-dominated systems—not because dinoflagellates prefer warmer temperatures, but because they thrive in the low-nutrient, low-turbulence conditions that warming often produces (P. M. Glibert 2020; Peperzak 2003; Fu 2012).\nThese ecological patterns are further supported by physiological observations. Anderson et al. (Anderson et al. 2021) demonstrated that dinoflagellates exhibit shallower thermal performance curves compared to other PCCs, with lower maximum growth rates and broader thermal tolerance. In contrast, diatoms and cyanobacteria display distinct thermal optima and strong growth responses to temperature. This generalist thermal profile offers a plausible interpretation for temperature not being a dominant feature for dinoflagellate prediction in our model. Other important factors linked to their distribution, such as stratification, irradiance, and nutrient regime are only partially, or not at all, captured by the input features used here. The SHAP-based feature interpretation supports this ecological understanding and underscores the need to include more nuanced environmental predictors in future modeling efforts.\nThe strength of the model, particularly for diatoms, coccolithophores, and phaeocystis, suggests that important spectral and environmental signals are being captured despite substantial dimensionality reduction. This supports the feasibility of operational PCC retrieval using compressed hyperspectral data, especially when paired with interpretable machine learning models.\nHowever, this study is based on simulated TOA radiance data, and real-world deployment will depend on atmospheric correction, instrument fidelity, and access to reliable ancillary predictors. Future work should validate the model on actual PACE observations and incorporate additional features—such as nutrient proxies, light attenuation, and mixed layer depth—to better capture ecological dynamics across all functional groups."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#conclusion",
    "href": "portfolio/pfg_xgb/index.html#conclusion",
    "title": "Inferring PCC from Far Far Away",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis study presents a novel, explainable machine learning framework for retrieving phytoplankton functional group concentrations from simulated hyperspectral top-of-atmosphere radiance data. Using an XGBoost model trained on reduced spectral features and auxiliary inputs such as temperature and latitude, we achieved high predictive performance across most functional groups and total chlorophyll-a. Model interpretation using SHAP values revealed that temperature was a key predictor for all groups except dinoflagellates, whose distribution appears to be driven by a broader suite of ecological factors such as stratification, nutrient limitation, and mixotrophy.\nThese findings reinforce the importance of tailoring remote sensing algorithms to the ecological and physiological diversity of phytoplankton groups. They also demonstrate that physically interpretable, high-performing models can be built even when using compressed hyperspectral inputs. However, the data tested is still simulated. Confirmation studies focusing on real sensor data (e.g., from the PACE mission) and incorporating additional oceanographic predictors to improve performance across all functional groups—especially those, like dinoflagellates, whose success is governed by indirect or emergent environmental conditions."
  },
  {
    "objectID": "portfolio/pfg_xgb/index.html#references",
    "href": "portfolio/pfg_xgb/index.html#references",
    "title": "Inferring PCC from Far Far Away",
    "section": "References",
    "text": "References\n\n\nAkiba, Takuya, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. “Optuna: A Next-Generation Hyperparameter Optimization Framework.” In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2623–31. ACM. https://doi.org/10.1145/3292500.3330701.\n\n\nAlvain, S., C. Moulin, Y. Dandonneau, and F. M. Bréon. 2005. “Remote Sensing of Phytoplankton Groups in Case 1 Waters from Global SeaWiFS Imagery.” Deep-Sea Research I 52 (11): 1989–2004. https://doi.org/10.1016/j.dsr.2005.06.015.\n\n\nAlvain, S., C. Moulin, Y. Dandonneau, and H. Loisel. 2008. “Seasonal Distribution and Succession of Dominant Phytoplankton Groups in the Global Ocean: A Satellite View.” Global Biogeochemical Cycles 22 (3): GB3S04. https://doi.org/10.1029/2007GB003154.\n\n\nAnderson, Thomas R, Erik T Buitenhuis, Corinne Le Quéré, and Andrew Yool. 2021. “Marine Phytoplankton Functional Types Exhibit Diverse Responses to Thermal Change.” Nature Communications 12 (1): 5126. https://doi.org/10.1038/s41467-021-25499-4.\n\n\nBergstra, James, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. “Algorithms for Hyper-Parameter Optimization.” In Advances in Neural Information Processing Systems, edited by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf.\n\n\nBopp, L., O. Aumont, P. Cadule, S. Alvain, and M. Gehlen. 2005. “Response of Diatoms Distribution to Global Warming and Potential Implications: A Global Model Study.” Geophysical Research Letters 32 (19): L19606. https://doi.org/10.1029/2005GL023653.\n\n\nBoyd, P. W., and S. C. Doney. 2002. “Modelling Regional Responses by Marine Pelagic Ecosystems to Global Climate Change.” Geophysical Research Letters 29 (19): 53-1-53-4. https://doi.org/10.1029/2001GL014130.\n\n\nBrewin, R. J. W., S. Sathyendranath, T. Hirata, S. J. Lavender, R. Barciela, and N. J. Hardman-Mountford. 2010. “A Three-Component Model of Phytoplankton Size Class for the Atlantic Ocean.” Ecological Modelling 221 (11): 1472–83. https://doi.org/10.1016/j.ecolmodel.2010.02.014.\n\n\nBuitenhuis, Erik T. et al. 2013. “Biogeochemical Fluxes Through Microzooplankton.” Global Biogeochemical Cycles 27 (3): 847–58. https://doi.org/10.1002/gbc.20059.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 785–94. https://doi.org/10.1145/2939672.2939785.\n\n\nCiotti, A. M., M. R. Lewis, and J. J. Cullen. 2002. “Assessment of the Relationships Between Dominant Cell Size in Natural Phytoplankton Communities and the Spectral Shape of the Absorption Coefficient.” Limnology and Oceanography 47 (2): 404–17. https://doi.org/10.4319/lo.2002.47.2.0404.\n\n\nDierssen, H. M., M. M. Gierach, L. S. Guild, A. Mannino, J. Salisbury, S. Schollaert Uz, J. Scott, et al. 2023. “Synergies Between NASA’s Hyperspectral Aquatic Missions PACE, GLIMR, and SBG: Opportunities for New Science and Applications.” Journal of Geophysical Research: Biogeosciences 128 (10): e2023JG007574. https://doi.org/10.1029/2023JG007574.\n\n\nFlynn, Kevin J. et al. 2013. “Misuse of the Phytoplankton–Zooplankton Dichotomy: The Need to Assign Organisms as Mixotrophs Within Plankton Functional Types.” Journal of Plankton Research 35 (1): 3–11. https://doi.org/10.1093/plankt/fbs062.\n\n\nFu, Feixue et al. 2012. “Interactions Between Changing pCO₂, n Availability, and Temperature on the Marine Nitrogen Fixer Trichodesmium.” Global Change Biology 18 (10): 3079–92. https://doi.org/10.1111/j.1365-2486.2012.02719.x.\n\n\nGlibert, Patricia M. 2020. “Harmful Algal Blooms: A Climate Change Co-Stressor in Marine and Freshwater Ecosystems.” Harmful Algae 91: 101590. https://doi.org/10.1016/j.hal.2019.03.008.\n\n\nGlibert, Patricia M. et al. 2001. “The Role of Eutrophication in the Global Proliferation of Harmful Algal Blooms.” Oceanography 14 (2): 66–74. https://doi.org/10.5670/oceanog.2001.47.\n\n\nGregg, Watson W., and Cecile S. Rousseaux. 2019. “Decadal Changes in Global Phytoplankton Composition: Observations and Modeling.” Journal of Geophysical Research: Oceans 124 (2): 983–1003. https://doi.org/10.1029/2018JC014173.\n\n\nHirata, T., N. J. Hardman-Mountford, R. J. W. Brewin, J. Aiken, R. Barlow, K. Suzuki, T. Isada, et al. 2011. “Synoptic Relationships Between Surface Chlorophyll-a and Diagnostic Pigments Specific to Phytoplankton Functional Types.” Biogeosciences 8 (2): 311–27. https://doi.org/10.5194/bg-8-311-2011.\n\n\nIOCCG. 2014. “Phytoplankton Functional Types from Space.” 15. Edited by S. Sathyendranath. IOCCG Reports. Dartmouth, Canada: International Ocean-Colour Coordinating Group (IOCCG).\n\n\nIzadi, Moein, Mohamed Sultan, Racha El Kadiri, Amin Ghannadi, and Karem Abdelmohsen. 2021. “A Remote Sensing and Machine Learning-Based Approach to Forecast the Onset of Harmful Algal Bloom.” Remote Sensing 13 (19): 3863. https://doi.org/10.3390/rs13193863.\n\n\nJeong, Hae Jin et al. 2010. “Mixotrophy in the Marine Dinoflagellate Population: Physiological Roles, Relationships, and Regulation.” Harmful Algae 9 (2): 154–65. https://doi.org/10.1016/j.hal.2009.08.005.\n\n\nKibler, Sarah R. et al. 2015. “Geographic and Vertical Distribution of Dinoflagellates in the Gulf of Mexico.” Journal of Phycology 51 (4): 606–18. https://doi.org/10.1111/jpy.12302.\n\n\nLaufkötter, C., M. Vogt, N. Gruber, O. Aumont, L. Bopp, S. C. Doney, J. P. Dunne, et al. 2016. “Projected Decreases in Future Marine Export Production: The Role of the Carbon Flux Through the Upper Ocean Ecosystem.” Biogeosciences 13 (13): 4023–47. https://doi.org/10.5194/bg-13-4023-2016.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS), 30:4768–77. Curran Associates, Inc. https://doi.org/10.48550/arXiv.1705.07874.\n\n\nMarañón, E., and Mouriño-Carballido B. Cermeño P. Huete-Ortega M. López-Sandoval D. C. 2014. “Resource Supply Overrides Temperature as a Controlling Factor of Marine Phytoplankton Growth.” PLoS ONE 10 (6). https://doi.org/10.1371/journal.pone.0099312.\n\n\nMouw, Colleen B., Neil J. Hardman-Mountford, Sylvie Alvain, Astrid Bracher, Robert J. W. Brewin, Annick Bricaud, Aurea M. Ciotti, et al. 2017. “A Consumer’s Guide to Satellite Remote Sensing of Multiple Phytoplankton Groups in the Global Ocean.” Frontiers in Marine Science 4: 41. https://doi.org/10.3389/fmars.2017.00041.\n\n\nPeperzak, Louis. 2003. “Climate Change and Harmful Algal Blooms in the North Sea.” ICES Journal of Marine Science 60 (2): 271–76. https://doi.org/10.1016/S1054-3139(03)00010-1.\n\n\nSmayda, Theodore J., and Colin S. Reynolds. 2001. “Community Ecology of Harmful Algal Blooms in Coastal Upwelling Ecosystems.” ICES Journal of Marine Science 58 (2): 374–76. https://doi.org/10.1006/jmsc.2001.1034.\n\n\nYan, Zhaojiang, Chong Fang, Kaishan Song, Xiangyu Wang, Zhidan Wen, Yingxin Shang, Hui Tao, and Yunfeng Lyu. 2025. “Spatiotemporal Variation in Biomass Abundance of Different Algal Species in Lake Hulun Using Machine Learning and Sentinel-3 Images.” Scientific Reports 15: 2739. https://doi.org/10.1038/s41598-025-87338-4.\n\n\nZhang, Yuan, Fang Shen, Renhu Li, Mengyu Li, Zhaoxin Li, Songyu Chen, and Xuerong Sun. 2024. “AIGD-PFT: The First AI-Driven Global Daily Gap-Free 4 Km Phytoplankton Functional Type Data Product from 1998 to 2023.” Earth System Science Data 16: 4793–4816. https://doi.org/10.5194/essd-16-4793-2024."
  },
  {
    "objectID": "blog/bayesian_ode/npz1986_blog.html",
    "href": "blog/bayesian_ode/npz1986_blog.html",
    "title": "A Bayesian Approach to Marine Modeling",
    "section": "",
    "text": "This is a summary of a first installment in a series that aims to demonstrate the use of Bayesian inference for marine modeling. Here, I demonstrate how parameters of a system of ordinary differential equations (ODE) used in a nitrogen-phytoplankton-zooplankton (NPZ) model can be recovered from synthetic data. These are created from the NPZ model using fixed parameters, the equations of which are as follows\n\\[\n\\begin{alignat}{3}\n&\\frac{dN}{dt} = -μ \\frac{N}{k_N + N} P + m_PP + m_ZZ \\\\\n&\\frac{dP}{dt} = μ \\frac{N}{k_N+N} P - g_{max} \\frac{P^2}{k_P^2 + P^2} Z - m_PP \\\\\n&\\frac{dZ}{dt} = τ g_{max} \\frac{P^2}{k_P^2 + P^2} Z - m_ZZ \\\\\n\\end{alignat}\n\\]\nThe above is the original mechanistic framework of Franks et al. (1986), which represents the dynamics of nutrients (N), phytoplankton (P), and zooplankton (Z). To recover the parameters of the simulation, I use a combination of informative priors sourced from the litterature an adaptive version of the Hamiltonian Monte Carlo (HMC), the No U-Turn Sampler (NUTS). The resulting high-dimensional posterior distribution is a rich probabilistic construct that can be mined for parameter estimation along with estimation uncertainty. The following represents the first few steps of a principled Bayesian workflow; cf. Gelman et al., 2020.\n\nPrior elicitation, model building.\nPrior predictive checks: aimed at verifying modeling assumptions before data collection occurs;\nData collection: simulated by running the NPZ model to account of 10 days of activity following which the signals of each compartments were then homoscedasticly noised up;\nSampler setting and model fitting;\nMarginalization and examination of each parameter’s posterior distribution to assess whether and how well the “true” parameters were recovered - recovery is deemed successful if the true value falls within a pre-specified level of significance;\nPosterior predictive checks: aimed at checking model fits and associated uncertainties with comparison with simulated data.\n\nA note on significance - unlike in the classical paradigm, there isn’t a pre-ordained conventional level of significane. Significance here is represented as a region within the posterior distribution known as the Highest Density Interval (HDI). HDI and other uncertainty tools are touched upon in the actual study referenced at the end.\nThe figures below summarize some of the process.\n\n\n\nFigure 1: Prior Predictive Time Series with 94% HDI. This is one of the steps in checking assumption validity in the model structure and prior choices.\n\n\n\n\n\nFigure 2: Simulated data. Top: mean field signal. Bottom: mean with compartment-wise homoscedastic noise added. This is the data from which the parameters will be recovered.\n\n\n\n\n\nFigure 3: Marginal posterior distributions for each model parameters. The orange line shows the “true” values used to generate the synthetic data. The probability of the “true value” to be within the posterior is also indicated in orange. The success criterion is for the orange line to fall within the region depicted by the black HDI line. While almost all parameters are found to be withing this HDI, one is outside and a few are close to the edges of the HDI line. This indicates potential problems and suggests revisions of the model.\n\n\n\n\n\nFigure 4: The mean modeled time series for each compartment, N, P, Z along with 94% HDI. The observed (synthetic) data is overlaid for reference. This may look adequate but as indicated by the probability of\n\n\nThis was a short demonstration of how to infer model parameters of a NPZ model given some data. The Bayesian workflow includes more steps than were shown here. In particular, more than one models are typically built with increasing complexity. The resulting high dimensional posterior distributions are rich constructs than can be mined for far greater insights than provided by alternative frameworks. Information theoretic and probabilistic machine learning approaches can then be leveraged to decide on the better model. Thus a model is not falsified per se, rather the difference in skill between models can be quantitatively measured. I will demonstrate this in a subsequent entry. One should remember however that model performance is conditional on the data observed and the model structure itself.\nThe code for this effort can be found here. For a deeper dive visit this portfolio entry."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html",
    "href": "blog/bart_toa/bart_toa.html",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "",
    "text": "Tired of wrestling with atmospheric models when all you want is ocean data? To oceanographers using satellite observations, the atmosphere is an obstacle to the development of good predictive models of marine processes. Bayesian Additive Regression Trees - BART - offers a robust solution, blending the flexibility of tree-based machine learning with Bayesian uncertainty to directly predict phytoplankton absorption from top-of-the-atmosphere (TOA) radiance."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#preamble",
    "href": "blog/bart_toa/bart_toa.html#preamble",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "",
    "text": "Tired of wrestling with atmospheric models when all you want is ocean data? To oceanographers using satellite observations, the atmosphere is an obstacle to the development of good predictive models of marine processes. Bayesian Additive Regression Trees - BART - offers a robust solution, blending the flexibility of tree-based machine learning with Bayesian uncertainty to directly predict phytoplankton absorption from top-of-the-atmosphere (TOA) radiance."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#atmospheric-correction-in-brief",
    "href": "blog/bart_toa/bart_toa.html#atmospheric-correction-in-brief",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "Atmospheric Correction in Brief",
    "text": "Atmospheric Correction in Brief\nAtmospheric correction is the process of removing the atmospheric contribution from satellite measurements of TOA radiance to isolate the weak oceanic signal. Sunlight reaching the sensor includes both direct atmospheric paths (scattered by air molecules and aerosols) and reflected sunlight from the ocean surface. Only a small fraction of this signal — the water-leaving radiance — actually carries information about oceanic properties like chlorophyll and suspended particles. While simpler components like Rayleigh scattering and surface reflection (Fresnel effects) can be corrected relatively easily, the remaining atmospheric signal, especially from aerosols, is far more complex. Being able to bypass or reduce the need for full atmospheric correction is a significant advancement, as it simplifies the retrieval pipeline and avoids introducing uncertainty from external assumptions and models."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#about-bart",
    "href": "blog/bart_toa/bart_toa.html#about-bart",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "About BART",
    "text": "About BART\nBART has been around for about 2 decades; it has gained significant traction for its robust predictive performance and ability to quantify uncertainty across diverse fields. Because of its nonparametric nature, BART can capture complex, nonlinear relationships and interactions without requiring explicit specification (Chipman, George, and McCulloch 2010). BART has been applied in variety of fields, including;\n\nCausal Inference: BART is widely used for estimating causal effects in observational studies due to its flexibility in modeling confounding relationships (J. L. Hill 2011).\nBiomedical Research: It has been applied in areas like biomarker discovery, predicting disease outcomes, and estimating individual treatment effects (Logan et al. 2019).\nEnvironmental Science: BART has found use in predicting air pollution concentrations, as well as in various ecological modeling tasks (Zhang et al. 2020).\nSocial Sciences and Economics: Its ability to handle high-dimensional data and provide uncertainty estimates makes it valuable for analyzing complex survey data and estimating policy impacts.\n\nBART’s building block is the regression tree, which partitions the covariate space into subgroups. As its name suggests BART’s formulation then consists in summing such trees. What makes it Bayesian is its regularization priors. These limit both depth and the fit of each tree, thus encouraging an ensemble of weak learners, with the goal of avoiding - or more realistically avoidingg overfitting; cf. J. Hill, Linero, and Murray (2020) for more.\n\n\n\n\n\n\nFigure 1: In search of a generalizable solution…\n\n\n\nThis post is a brief demonstration of work I am currently wrapping up, and that improves on previous work (Craig and Karaköylü 2020). The goal of the project is to assess the feasibility of simplifying the modeling process by avoiding most of atmospheric correction. It builds on work published previously."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#data-overview",
    "href": "blog/bart_toa/bart_toa.html#data-overview",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "Data Overview",
    "text": "Data Overview\nThe predictive features consist of six wavelengths of TOA (top-of-atmosphere) radiance measurements from satellite remote sensing. These have been corrected for Fresnel reflection and Rayleigh scattering. The Fresnel correction removes the specular component of sunlight reflected off a flat ocean surface, while the Rayleigh correction accounts for molecular scattering by air molecules in the atmosphere. Both are relatively straightforward, physics-based corrections that depend primarily on viewing geometry and standard atmospheric conditions, without requiring full atmospheric state data.\nHowever, the data is not corrected for more complex atmospheric interactions — most notably, aerosol scattering and absorption. These effects are typically handled by full atmospheric correction algorithms, which attempt to isolate the weak water-leaving radiance signal from the much stronger atmospheric contribution. One key aspect of this signal is that ocean color is shaped not just by what is reflected, but also by what is absorbed. For example, phytoplankton pigments like chlorophyll-a absorb strongly in the blue and red parts of the spectrum. In practice, this means we observe less radiance at those wavelengths than we otherwise would — so absorption often reveals itself as a missing or diminished signal where one would expect more. Interpreting these spectral “absences” is fundamental to retrieving information about oceanic constituents.\nAnother characteristic of these features is their high multicollinearity (Figure 2). Multicollinearity can pose challenges for many modeling approaches by producing unstable coefficient estimates; however, tree-based models — including BART — are generally more robust in such cases. Nonetheless, care should be taken when interpreting variable importance or partial dependence. This aspect will be explored in more depth in a future post.\n\n\n\n\n\n\nFigure 2: Pair plot demonstrating the multicollinear nature of the input features; Rayleigh- and Fresnel-corrected top-of-the-atmosphhere radiance.\n\n\n\nThe prediction targets are phytoplankton absorption (AΦ) at three wavelengths; 443, 555, 670nm; their distribution is seen in Figure 3. The present BART model was written to predicted all three targets simultaneously.\n\n\n\n\n\n\nFigure 3: Kernel density estimation of the output, phytoplankton absorption at 3 wavelengths; 443, 555, and 670 nm.\n\n\n\nOverall the only preprocessing step applied to the data in addition to the aforementioned corrections was to \\(log_{10}\\)-transformed them - both input features and prediction targets - as doing so stabilized their variance, which eases model fitting. Interpretation remains easy since data thus transformed refer to magnitude. All plots in this post will be shown in \\(log_{10}\\) scale."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#modeling",
    "href": "blog/bart_toa/bart_toa.html#modeling",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "Modeling",
    "text": "Modeling\nFigure 4 shows the BART model structure, written in PyMC and PyMC-BART. The full notebook with code can be found here.\n\n\n\n\n\n\nFigure 4: Model graph depicting priors, likelihood, data and how these are connected per the model formulation."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#abbreviated-bayesian-workflow",
    "href": "blog/bart_toa/bart_toa.html#abbreviated-bayesian-workflow",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "Abbreviated Bayesian Workflow",
    "text": "Abbreviated Bayesian Workflow\nPart of the Bayesian workflow is to simulate model output before fitting data. This is a verification step to make sure modeling assumptions, encoded as priors, produce reasonable target values. This is referred to as Prior Predictive Checks. After fitting the process is repeated, which shows whether the model has learned from the data, and how simulated data compares to actual observations\n\n\n\n\n\n\nFigure 5: Did the model learn from the data? Left - prior predictive checks, where model output (\\(log(aΦ)\\)) is simulated though not conditioned on the data yet. This plot shows Kernel Density Estimation (KDE) of multiple simulation draws, their mean and how the observation data. Right - posterior predictive checks, similar but now the simulated output is conditioned on the observations.\n\n\n\n\n\n\n\n\n\nFigure 6: Posterior Predictive Checks - The trained Bayesian model can now be used to simulate phytoplankton absorption at all 3 wavelengths. These can in turn be compared with observatiosn. From left to right and top to bottom; phytoplankton absorption at 443, 555, 670 nm, and all bands. Black line: distribution of observations; each grey line represents the distribution of a simulation outcome; orange dashed line is the simulation mean distribution."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#interpretable-machine-learning",
    "href": "blog/bart_toa/bart_toa.html#interpretable-machine-learning",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "Interpretable Machine Learning",
    "text": "Interpretable Machine Learning\nLike other tree-based algorithms, BART is relatively transparent and exploring variable importance in driving inference is relatively easy."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#model-generalization",
    "href": "blog/bart_toa/bart_toa.html#model-generalization",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "Model Generalization",
    "text": "Model Generalization\nApproximating Out-of-Sample Predictions - As summarized earlier in Figure 1, making sure the model will generalized well is paramount. Thus the model should avoid overfitting the training data, while learning a good enough representation of the signal contained within. E.g. the observation KDE suggest a bimodal distribution (two peaks). The likelihood on the other hand is monomodal (single peak). This is because I used a Normal likelihood. At this stage, this is a reasonable assumption, and the bimodality observed in the data could be artifact of the scarcity of the available data. Thus, until more data becomes available to add more weight to the bi- or even multi-modality of observations, a Normal likelihood is a reasonable assumption.\nThat said, the gold standard of model skill assessment is the out-of-sample prediction verification; i.e., using labeled data not used to train the model and compare its predictions to the observations. Given the small size of the data splitting it into training and validtion tiers is not a reasonable proposition. A better use of scarce data is Leave-one-out (LOO) cross-validation. This implies revolving around removing one sample."
  },
  {
    "objectID": "blog/bart_toa/bart_toa.html#references",
    "href": "blog/bart_toa/bart_toa.html#references",
    "title": "Satellite Oceanography without Atmospheric Correction",
    "section": "References",
    "text": "References\n\n\nChipman, Hugh A, Edward I George, and Robert E McCulloch. 2010. “BART: Bayesian Additive Regression Trees.” The Annals of Applied Statistics 4 (1): 266–98. https://doi.org/10.1214/09-AOAS285.\n\n\nCraig, Susanne E., and Erdem M. Karaköylü. 2020. “Bayesian Models for Deriving Biogeochemical Information from Satellite Ocean Color.” https://doi.org/10.31223/osf.io/shp6y.\n\n\nHill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for Causal Inference.” Journal of Computational and Graphical Statistics 20 (1): 217–40. https://doi.org/10.1198/jcgs.2010.08162.\n\n\nHill, Jennifer, Antonio Linero, and Jared Murray. 2020. “Bayesian Additive Regression Trees: A Review and Look Forward.” Annual Review of Statistics and Its Application 7: 251–78. https://doi.org/10.1146/annurev-statistics-031219-041110.\n\n\nLogan, Brent R., Rodney Sparapani, Robert E. McCulloch, and Purushottam W. Laud. 2019. “Decision Making and Uncertainty Quantification for Individualized Treatments Using Bayesian Additive Regression Trees.” Statistical Methods in Medical Research 28 (4): 1079–93. https://doi.org/10.1177/0962280217746191.\n\n\nZhang, Tong, Guannan Geng, Yang Liu, and Howard H. Chang. 2020. “Application of Bayesian Additive Regression Trees for Estimating Daily Concentrations of PM2.5 Components.” Atmosphere 11 (11): 1233. https://doi.org/10.3390/atmos11111233."
  },
  {
    "objectID": "blog/bayesian_power/bayesian_power.html",
    "href": "blog/bayesian_power/bayesian_power.html",
    "title": "Bayesian Power Analysis for A/B Testing",
    "section": "",
    "text": "Background\nA/B testing is a critical tool for informing decisions, whether comparing webpages, app features, or marketing campaigns. The goal is to identify which version performs best based on a chosen metric like conversion rate, click-through rate, or revenue. For frequentists, A/B testing often involves Null Hypothesis Significance Testing (NHST), where concepts such as \\(α\\), \\(p-\\text{value}\\), and power analysis are central. My focus here is on power analysis.\n\nFrequentist power analysis: a brief critique\nIn Frequentist Statistics, power analysis is essential during the planning phase of an experiment. Frequentist power is defined as the probability of correctly rejecting a false null hypothesis, which is often equated to the probability of correctly identifying a true effect. It’s used to determine the minimum sample size required for a study to have a reasonable chance (typically 80% or higher) of detecting a statistically significant effect of a given size at a predetermined significance level (α, often 0.05). This helps researchers avoid underpowered studies that might fail to detect real effects and informs decisions about resource allocation.\nThe standard definition of frequentist power (the probability of rejecting a false null hypothesis) is framed within a \\(P(D∣H)\\) perspective. It asks: “If the null hypothesis is indeed false (meaning a true effect exists), what is the probability that statistical testing will produce data that leads to rejecting that null hypothesis?” In my opinion, this is a risky and often flawed approach due to its incorrect use of probability for inference. This topic is beyond the scope of this post, but interested readers are invited to consult Aubrey Clayton’s “Bernoulli’s Fallacy” for a deeper technical and historical dive into this issue.\n\n\nThe Bayesian Alternative\nThe point is that Bayesian analysis offers a compelling alternative to the frequentist approach for A/B testing. It provides more direct and intuitive interpretations of results, without concerns about stopping issues, p-hacking, or needing multiple comparison corrections. Instead of focusing on the probability of observing data under a null hypothesis, Bayesian methods yield the probability distribution of the parameters of interest (e.g., the conversion rates of each variant and their difference). This allows for direct statements about the probability that one variant is better than the other, or the probability that the difference exceeds a practically significant threshold.\nNevertheless, power analysis can still be important in the Bayesian paradigm. While it takes on an altered meaning, it remains essential in experiment planning. Specifically, the goal is to determine the sample size, and consequently, adequate resource allocation to collect so that the experiment has a high probability of yielding informative results. Typically, this means assessing the probability of obtaining posterior distributions with sufficient precision (e.g., narrow credible intervals) or the probability that the effect size (the difference between the variants) exceeds a Minimum Detectable Effect (MDE). In practice, this means simulating potential experiment outcomes for a variety of sample sizes and analyzing the resulting posterior distributions.\n\n\n\nBrief Demonstration of Bayesian Power Analysis\n\n👉 Here is the notebook with all the code\n\n\nStep 1: Define Goal and Metric(s)\nCompany X wants to increase the conversion rate on its landing page. To do so a new landing page is proposed. Thus we have * Current landing page; hereafter referred to as A * Proposed landing page; hereafter referred to as B * A single primary metric, conversion rate (number of conversions / total landings )\nObviously this is an oversimplified example but will serve to illustrate the approach.\n\n\nStep 2: Eliciting Prior Beliefs\nTo compute posteriors I need to combine likelihoods obtained with data with priors. Unlike Frequentist settings, Bayesian experiments are not conducted in a vaccum. Instead the paradigm encourages the incorporation of prior knowledge. While it is often the case that data will swamp out priors, there is value in carefully constructing them.\nCompany X data scientists should therefore base their priors on any relevant existing knowledge. This could include:\n\nHistorical Data: If Company X has run similar A/B tests in the past on the same website or for similar features, the results of those tests can provide valuable information for setting priors. For example, if previous versions of the landing page had conversion rates consistently around 4-6%, this could inform the prior for variant A.\nIndustry Benchmarks: Depending on the industry, there might be typical conversion rate ranges that can inform the priors.\nExpert Opinions: Marketing experts or product managers within Company X might have intuitions or expectations about the performance of the new variant B. These subjective beliefs can be formalized into a prior distribution.\nA “Skeptical” or Weakly Informative Prior: If Company X has little to no prior information, they might choose a weakly informative prior. This is a prior that doesn’t strongly favor any particular outcome but still provides some regularization.\n\nThe outcome is binary, thus the natural likelihood to model the data is the Binomial distribution, which aggregates binary Bernoulli trials. There a few distributions that can be used to encode priors in this case. To keep things simple and avoid reaching for my favorite MCMC sampler, I define priors in terms of Beta distributions. A Beta distribution is a conjugate prior to the Binomial, meaning that updating the posterior has a closed form and does not require approximation. The parameters \\(α\\) and \\(β\\) of the Beta distribution represent prior “successes” and “failures,” respectively. A higher \\(α\\) relative to \\(β\\) shifts the distribution towards higher values, and vice versa. The sum of \\(α\\) and \\(β\\) influences the “strength” or concentration of the prior belief (higher sum means more certainty). Below I show what this might look like.\n\nLet’s assume historical data suggest a current conversion rate of 5% for variant \\(A\\).\nWithout any additional information and to keep things simple a reasonable prior for \\(A\\) is therefore \\(prior_A=Beta(α=5, β=95)\\).\nFor one reason or another, I also expect a lift of 1% from variant B so an acceptable prior is \\(prior_B=Beta(α=6, β=94)\\). Importantly, this does not discard the possibility that variant B is worse or better. The point is that my assumptions are made explicit, open to critique, and certainly revisable.\n\n\nThese priors can be seen in Figure 1.\n\n\n\nFigure 1: Beta Priors for A and B variants.\n\n\n\nUpdating the Beta prior to get the posterior is trivial as it boils down to adding new successes and new failures to the existing \\(α\\) and \\(β\\) parameters, respectively.\nThe function run_analytical_ab_test in the notebook simulates the Data Generation Process with a know conversion rate for variant A and a latent (not yet known) conversion rate for variant B.\n\n\n\nStep 3: Defining the Minimum Detectable Effect (MDE)\n\nQuestion: what is the smallest practically significant difference in conversion rates that Company X would want to detect?\nThe answer is the Minimum Detectable Effect, hereafter MDE.\nNote the actual effect would need to be greater than the MDE; the greater the actual effect relative to the MDE, the smaller the number of samples needed to detect a difference at the MDE level.\nLet’s assume Company X is interested in detecting an absolute difference of at least 1% in the conversion rate, that’s our MDE\nIf the baseline conversion rate is around 5%, stakeholders then want to be able to reliably detect if variant B increases it to 6% or more.\nGiven the above and to make it easier, let’s say that the latent true conversion rate of Variant B will be 7% - Obviously several scenarios with different conversion rates coould be simulated. To keep it short, however, we’ll just limit the simulation to a single case.\n\n\n\nStep 4: Prospective Sample Size Planning (Bayesian “Power Analysis”)\n\nWe are not trying to calculate the probability of rejecting a null hypothesis - this wouldn’t tell us much about the alternative anyway.\nThe focus is on the probability of achieving a desired level of certainty about the difference in conversion rates.\nThis often involves simulating potential outcomes for different sample sizes and evaluating the resulting posterior distribution.\nWe assess the following conditions\n\nThe posterior probability that variant B is better than variant A (i.e., conversion rate of B &gt; conversion rate of A) is above a certain threshold; set here at 95%.\nThe posterior probability that the difference in conversion rates (B - A) is greater than the MDE is above a certain threshold; set here at 80%.\n\nThe function evaluate_analytica_power in the notebook is used to evaluate the posterior difference for a given number of samples in view of these criteria. Now we can build a power curve against sample size\n\n\n\n\nFigure 2: Power curve showing showing expected power given the number of samples for each variant. Red line indicates desired threshold probability of detecting that stipulated MDE is satisfied; in this case 80%.\n\n\n\n\n\nPower Analysis Interpretation\n\nThe figure above suggests 3500 FOR EACH variant would be a sufficient size.\nI recommend running it a bunch of times and get uncertainty envelopes around these numbers for added credibility to threshold surpassing; this is skipped for expediency.\n\n\nRunning The Actual Experiment.\n\nArmed with this information we can actually run an A/B test and see what kind of posterior we get.\nNote that unlike Frequentists we don’t have traditionally accepted significance levels. How to come to a decision is an important discussion point between the data scientist and the stakeholder/decision maker.\nAs a decision guide as to whether the difference is significantly greater than 0, I propose a Region of Pratical Equivalence (ROPE).\nDetermining a good ROPE is beyond the scope of this post. But the power analysis had two criteria so for illustration, I will use these as follows\nFirst ROPE will be between (-1 and 0.005) and I will visually evaluate whether 95% of the Highest Density Interval (HDI) of the posterior of the difference (B-A) is greater\nSecond rope will be between (0 and 0.015) and I will visually evaluate whether 80% of the HDI of the same posterior is greater.\nNote that for both I left some wiggle room and bumped the ROPE’s upper limits by half a percent relative to the Power Analysis criteria\n\n\n\n\nFigure 3: Experiment outcome depicted as the posterior distribution of the difference, B-A, in terms of conversion between variants. Posterior statistics in black. Left panel includes Region of Practical Equivalence (ROPE) for absolute difference criterion, Right includes ROPE for minimum detectability criterion. See text for more.\n\n\nFigure above: * Both plots above show the same posterior with different ROPEs * In black, the chosen HDI - 94% in this case as a reminder to be practical, not religious about these numbers * Numbers in black are lower and upper HDI bounds, and posterior mean.\n\nLeft panel shows 0% of the posterior overlaps with the ROPE, which satisfies the condition that \\(P((B-A)&gt;0) &gt; 95\\%\\).\nRight panel shows shows 2.1% of the posterior overlaps with the ROPE, meaning \\(P((B-A)&gt;0.01)=97.1\\%&gt;80\\%\\)\n\n\n\n\nIn conclusion\nPower analysis is not a strict necessity for Bayesians. It’s OK for us to go and collect more data if the posterior is not what we want it to be - data snooping or p-hacking is not a thing here. Nevertheless it can still be a useful tool for experiment planning and adequate resource allocation.\nThanks for reading & happy (probabilistic) coding!"
  },
  {
    "objectID": "blog/pfg_xgb_xai/index.html",
    "href": "blog/pfg_xgb_xai/index.html",
    "title": "Peeking into the Black Box: Explaining Plankton Predictions from Space",
    "section": "",
    "text": "Can we trust a machine learning model that predicts phytoplankton from satellite data? Only if we understand how it thinks."
  },
  {
    "objectID": "blog/pfg_xgb_xai/index.html#background",
    "href": "blog/pfg_xgb_xai/index.html#background",
    "title": "Peeking into the Black Box: Explaining Plankton Predictions from Space",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\n🌊 Meet the Ocean’s Invisible Powerhouse\n\n\n\nPhytoplankton are microscopic algae that drift near the ocean’s surface — and they’re quietly responsible for producing at least half of the oxygen we breathe.\nThough tiny, they punch far above their weight: these photosynthetic drifters form the foundation of marine food webs, support global fisheries, and help regulate Earth’s climate by drawing down carbon dioxide.\nThink of them as invisible forests of the sea — floating, blooming, and even glowing in the dark.\n\n\nWe trained a machine learning model (XGBoost) to predict concentrations of phytoplankton functional groups — including diatoms, cyanobacteria, and dinoflagellates — using simulated satellite top-of-atmosphere (TOA) radiance data, similar to what NASA’s upcoming PACE mission will collect.\nThe model worked surprisingly well. But one challenge remained: Why was it working? And what was it learning?\n\n\n\nBioluminescent wave caused by dinoflagellates\n\n\nAbove: A wave glows blue at night from bioluminescent dinoflagellates — microscopic organisms that react to turbulence by lighting up the sea."
  },
  {
    "objectID": "blog/pfg_xgb_xai/index.html#enter-shap-a-window-into-model-logic",
    "href": "blog/pfg_xgb_xai/index.html#enter-shap-a-window-into-model-logic",
    "title": "Peeking into the Black Box: Explaining Plankton Predictions from Space",
    "section": "Enter SHAP: A Window into Model Logic",
    "text": "Enter SHAP: A Window into Model Logic\nWe used SHAP (SHapley Additive exPlanations), a method from game theory that assigns credit (or blame) to each input feature based on how it contributes to a model’s prediction.\nThink of it this way: if your model says “There are likely lots of diatoms here,” SHAP tells you what input features pushed it toward that conclusion.\nHere’s a summary plot of SHAP values for one phytoplankton group:\n\n\n\nSHAP Summary Plot for Diatoms\n\n\n\nThe further to the right, the more a feature increases the prediction; blue means low values of the feature, red means high."
  },
  {
    "objectID": "blog/pfg_xgb_xai/index.html#what-we-learned",
    "href": "blog/pfg_xgb_xai/index.html#what-we-learned",
    "title": "Peeking into the Black Box: Explaining Plankton Predictions from Space",
    "section": "What We Learned",
    "text": "What We Learned\n\nTemperature was a strong predictor — for almost all groups.\nNot for dinoflagellates. Their SHAP profile looked completely different.\n\nThis made ecological sense. Unlike other plankton that follow clear temperature gradients, dinoflagellates are mixotrophic opportunists. They dominate in nutrient-poor, stratified waters — which aren’t always tied directly to temperature. SHAP picked up on that.\n\n\n\nScanning electron microscope images of various dinoflagellate species\n\n\nAbove: SEM images of dinoflagellates — a highly diverse group with complex behaviors and forms."
  },
  {
    "objectID": "blog/pfg_xgb_xai/index.html#why-this-matters",
    "href": "blog/pfg_xgb_xai/index.html#why-this-matters",
    "title": "Peeking into the Black Box: Explaining Plankton Predictions from Space",
    "section": "Why This Matters",
    "text": "Why This Matters\nMachine learning in Earth science is powerful — but models aren’t useful if they’re black boxes. With SHAP, we not only got accurate predictions, but also insight into what’s driving them. In Biological Oceanography, explainable machine learning predictions can tell us how environmental features like temperature and light structure marine ecosystems. It’s not just about matching numbers — it’s about using models to discover and verify biological patterns.\nIn fact, recent research suggests that these tools can help predict how phytoplankton communities will shift as the ocean warms and stratifies in the coming decades. Understanding what drives groups like dinoflagellates today could help us anticipate how ocean life will respond tomorrow.\nWant to explore the model and data?\n👉 See the full project repo\n👉 See the manuscript we’re working on\n\nComing soon: Do we need hyperspectral sensors to resolve biological processes? How well will this model perform when we throw away most of the input data — and try to match older sensors like MODIS and VIIRS that resolve far fewer colors? Stay tuned for these and other cool insights…"
  },
  {
    "objectID": "blog/stats_ab_testing/index.html",
    "href": "blog/stats_ab_testing/index.html",
    "title": "On Statistical Practices in A/B Testing",
    "section": "",
    "text": "I just finished watching Dr. Ronny Kohavi’s “Ultimate Guide to A/B Testing” “Ultimate Guide to A/B Testing”. This was a lengthy presentation made enjoyable by his enthusiasm and wealth of practical experience. Dr Kohavi has had extensive experience leading A/B testing efforts in companies like Bing, Microsoft, Airbnb. During this seminar, he touched on the statistical approach and the use of p-values as a decision mechanism in A/B testing. P-values are an essential tool in Null Hypothesis Testing and other frequentist inference efforts. In my view, this potentially consequential decision support approach is the wrong framework, not just for this use case, but in fact most use cases. Rare are the instances where this approach is required. Below I justify this thinking and propose a better framework.\nProblems arising from the use of p-values - The p-value is often described as the probability of observing data that is as extreme or more extreme than what is on hand, given the null hypothesis is true. This is a subtle but crucial point. The P in p-value refers to a conditional probability about the data, not the hypothesis itself. This leads to several issues:\n\nSampling vs. Inference: P-values provide a sampling probability, not an inference probability. We’re interested in the probability that our hypothesis is true, given the data, not the other way around. This is not conducive to drawing definitive conclusions about whether variant A or B is truly better.\nP-hacking Vulnerability: The reliance on p-values can encourage “p-hacking”, where researchers manipulate data or analyses to achieve statistical significance, leading to unreliable results at best.\nCommon Misconception: As Dr. Kohavi correctly notes, many people mistakenly interpret (1-p) as the chance the alternative hypothesis is true. This misunderstanding is widespread and problematic.\n\nThe Importance of Priors - Dr. Kohavi touches on the concept of a “Prior”, in a nod to Bayes’ theorem, and says it is the way to get a probabilistic statement about the null hypothesis. However, he states, “we don’t know the prior.” This is incorrect. The Prior, to be thorough, the prior probability, is what we do know before seeing the data; it is the embodiment of our existing knowledge and assumption. In addition to the role they play in inference, priors are important even before running the model and fitting the data because they exposing modeling assumptions for all to see and critique. Once the model is run and the data is fit the resulting posterior distribution is a rich construct that provides greater understanding of the data, and potentially the data generation process. I also think it is a better underpinning for causal analysis, which is what A/B testing is - I’ll get into that in another post. For the A/B testing scenario specifically though Bayesian analysis can provide actual answers to the product manager’s questions.\nBayesian answers - Essentially, by using Bayesian statistics we can answer the question “what is the probability that B is better than A?” Contrast this with the question the frequentist p-value does answer; i.e. “what is the probability of the observed (or more extreme) data given that A and B are the same?”. Finally it renders unnecessary the dependence on an arbitrary significance level of 0.05 that hails from the obscurantism of the early 20th century. In essence, while A/B testing based on frequentist methods has been widely used, it’s essential to recognize its limitations. The frequentist approach, with its reliance on p-values, can lead to misinterpretations and unreliable conclusion, and has led to considerable damage in the social, political, judicial, and now with the awareness raised on the crisis of replication, scientific arenas.\nEspecially in today’s data-driven world, the Bayesian approach provides a more robust and informative alternative. Yes, it demands more effort, but its consistent framework stands in stark contrast to the hodgepodge of case-dependent recipes inherent in traditional statistics, a complexity that often proves difficult even for classically trained statisticians.\nI will provide an example of doing A/B testing properly in a next post. In the meantime, I would love to hear thoughts and experiences with A/B testing and statistical methods! Feel free to reach out if you’d like to dive deeper into these concepts or need help with your data.\nBe well!"
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html",
    "href": "blog/causailty_in_observation_data/index.html",
    "title": "Causality in observational data",
    "section": "",
    "text": "Preamble\nIn the real world, we often want to understand if something (a new medicine, a marketing campaign, a policy change) truly causes an outcome. In ideal experiments (like Randomized Control Trials or RCTs), we can control all factors, making it easier to determine cause and effect.\nBut what if we only have observational data, where things weren’t neatly controlled? Establishing a causal link becomes tricky. There might be hidden factors, called confounders, that influence both the treatment and the outcome, making it hard to tell if the treatment is the real cause. This post explores ways to tackle this challenge.\nExample: Does Exercise Cause Weight Loss?\nImagine a study on the impact of a new exercise program on weight loss. Participants with higher fitness levels might be more likely to choose the program. This becomes a confounder – fitness level influences both program participation and weight loss outcomes. Propensity score matching can help – it pairs individuals with similar fitness levels but different program participation, isolating the program’s true effect.\nUsing the Lalonde Dataset\nTo illustrate how matching and propensity scores can help with causal analysis, I’ll use a dataset from a study by Lalonde (1986) evaluating an employment and training program. The study aimed to understand if the program truly caused better employment outcomes. Let’s load the data from the ‘Matching’ R library and take a closer look:\nlibrary(MatchIt)\ndata(lalonde)\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html#matching",
    "href": "blog/causailty_in_observation_data/index.html#matching",
    "title": "Causality in observational data",
    "section": "Matching",
    "text": "Matching\n\nTo match or not to match?\nA first step is whether the data on hand is appropriate for causal inferrence, in particular, whether it should be balanced. A commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;-c(\"hispan\", \"black\", \"white\", \"age\", \"educ\", \"married\", \"nodegree\", \n              \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata, test=FALSE)\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        429               185                 \n  hispan (mean (SD))      0.14 (0.35)       0.06 (0.24)     0.277\n  black (mean (SD))       0.20 (0.40)       0.84 (0.36)     1.668\n  white (mean (SD))       0.66 (0.48)       0.10 (0.30)     1.406\n  age (mean (SD))        28.03 (10.79)     25.82 (7.16)     0.242\n  educ (mean (SD))       10.24 (2.86)      10.35 (2.01)     0.045\n  married (mean (SD))     0.51 (0.50)       0.19 (0.39)     0.719\n  nodegree (mean (SD))    0.60 (0.49)       0.71 (0.46)     0.235\n  re74 (mean (SD))     5619.24 (6788.75) 2095.57 (4886.62)  0.596\n  re75 (mean (SD))     2466.48 (3292.00) 1532.06 (3219.25)  0.287\n\n\nNote that an alternative would be to conduct two-tailed t-tests to assess difference between group (treated and control) means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size. I therefore use \\(SMD\\) in this post.\nBy convention, an \\(SMD\\) greater than 0.1 suggest an imbalance with respect to the corresponding covariate. Here \\(SMD&gt;0.1\\) for all covariates except education. Treated subjects need each to be match via greedy matching to as close as possible a control subject. Matching between subjects is done on the basis of a distance metric indicating how separated they are in the covariate space. The specific metric used in this case is the Mahalanobis distance, which is a kind of standardized difference, computed as follows: \\[d = \\sqrt{(X_i-X_j)^T C^{-1} (X_i-X_j) }\\] where \\(X\\) is a covariate, \\(i\\) and \\(j\\) are treated and control subjects, and \\(C\\) is the covariance matrix\n\nlibrary(Matching)\n# Below M=1 refers to pairwise matching. Even so if \"ties\" is left as TRUE (default)\n# multiple subjects within the tolerance threshold will all be matched. \n# In this case, e.g. not setting ties=TRUE yields 207 pairs, even though there are only # 185 treated subjects.\ngreedymatch&lt;-Match(Tr=treat, M=1, X=mydata[xvars], ties=FALSE) \ngreedymatched&lt;-mydata[unlist(greedymatch[c(\"index.treated\", \"index.control\")]), ]\n\nI create another Table 1 with the matched data check the SMDs.\n\nmatchedtab1&lt;-CreateTableOne(vars=xvars, strata=\"treat\", data=greedymatched, test=FALSE)\nprint(matchedtab1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.06 (0.24)       0.06 (0.24)    &lt;0.001\n  black (mean (SD))       0.84 (0.37)       0.84 (0.36)     0.015\n  white (mean (SD))       0.10 (0.30)       0.10 (0.30)     0.018\n  age (mean (SD))        25.34 (8.30)      25.82 (7.16)     0.062\n  educ (mean (SD))       10.45 (1.96)      10.35 (2.01)     0.054\n  married (mean (SD))     0.19 (0.39)       0.19 (0.39)    &lt;0.001\n  nodegree (mean (SD))    0.71 (0.46)       0.71 (0.46)    &lt;0.001\n  re74 (mean (SD))     2159.92 (4240.18) 2095.57 (4886.62)  0.014\n  re75 (mean (SD))     1119.08 (2442.29) 1532.06 (3219.25)  0.145\n\n\nGreedy pairwise matching yields, as expected, a reduced data set with 185 subjects in each group. This time all but the variable \\(re75\\) have corresponding \\(SMD&lt;0.1\\). This is not entirely satisfactory and I will attempt to balance the data set using propensity scores next"
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html#propensity-score-matching",
    "href": "blog/causailty_in_observation_data/index.html#propensity-score-matching",
    "title": "Causality in observational data",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nA propensity score denoted here \\(\\pi\\) is defined as the probability that a subject \\(i\\) received treatment conditioned on the covariates \\(X\\). I.e. \\(\\pi_i = P(T=1|X_i)\\). A \\(\\pi_i=0.4\\) means there’s a 40% chance the corresponding subject will receive treatment. Covariates can increase or decrease the probability of receiving treatment. For example, if \\(X\\), simplistically the only covariate, is a boolean variable for smoking and smokers are more likely to get a particular treatment then \\(P(T=1|X=1) \\gt P(T=1|X=0)\\).\nInterestingly, two subjects may have the same propensity score in spite of having different covariate values \\(X\\), meaning they are equally likely to receive treatment. Thus reducing the data to a subset of subjects with the same \\(\\pi\\) should balance the treated and control groups. In doing so a crucial assumption in causality, ignorability i.e. how a subject ended in one or the other group can be safely ignored. In a randomized trial, the propensity score is known. In an observational study \\(\\pi\\) is unknown. However because both \\(X\\) and \\(T\\) are collected, \\(\\pi\\) can be estimated. For this I will fit a logistic regression, where the covariates are the input and the treatment variable is the output. Using this model I can get the predicted probability of treatment for each subject; i.e. the estimated \\(\\pi\\).\n\npsmodel &lt;- glm(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75,family=binomial(), data=mydata)\n\nThe model fit is summarized below.\n\n# show fit summary\nsummary(psmodel)\n\n\nCall:\nglm(formula = treat ~ hispan + white + black + age + educ + married + \n    nodegree + re74 + re75, family = binomial(), data = mydata)\n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.663e+00  9.709e-01  -1.713  0.08668 .  \nhispan      -2.082e+00  3.672e-01  -5.669 1.44e-08 ***\nwhite       -3.065e+00  2.865e-01 -10.699  &lt; 2e-16 ***\nblack               NA         NA      NA       NA    \nage          1.578e-02  1.358e-02   1.162  0.24521    \neduc         1.613e-01  6.513e-02   2.477  0.01325 *  \nmarried     -8.321e-01  2.903e-01  -2.866  0.00415 ** \nnodegree     7.073e-01  3.377e-01   2.095  0.03620 *  \nre74        -7.178e-05  2.875e-05  -2.497  0.01253 *  \nre75         5.345e-05  4.635e-05   1.153  0.24884    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 751.49  on 613  degrees of freedom\nResidual deviance: 487.84  on 605  degrees of freedom\nAIC: 505.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nNext I extract the propensity scores from the model object.\n\n# create propensity score\npscore&lt;-psmodel$fitted.values\n\nFinally, I use the MatchIt package to match subjects based on their propensity scores. Note that I set a seed for reproducibility, since matching randomizes data as a first step.\n\nset.seed(42)\nm.out&lt;-matchit(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75, data=mydata, method=\"nearest\")\n\nThe matching results are summarized below.\n\n# summarize the matching outcome\nsummary(m.out)\n\n\nCall:\nmatchit(formula = treat ~ hispan + white + black + age + educ + \n    married + nodegree + re74 + re75, data = mydata, method = \"nearest\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.1822          1.7941     0.9211    0.3774\nhispan          0.0595        0.1422         -0.3498          .    0.0827\nwhite           0.0973        0.6550         -1.8819          .    0.5577\nblack           0.8432        0.2028          1.7615          .    0.6404\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\ndistance   0.6444\nhispan     0.0827\nwhite      0.5577\nblack      0.6404\nage        0.1577\neduc       0.1114\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.3629          0.9739     0.7566    0.1321\nhispan          0.0595        0.2162         -0.6629          .    0.1568\nwhite           0.0973        0.3135         -0.7296          .    0.2162\nblack           0.8432        0.4703          1.0259          .    0.3730\nage            25.8162       25.3027          0.0718     0.4568    0.0847\neduc           10.3459       10.6054         -0.1290     0.5721    0.0239\nmarried         0.1892        0.2108         -0.0552          .    0.0216\nnodegree        0.7081        0.6378          0.1546          .    0.0703\nre74         2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75         1532.0553     1614.7451         -0.0257     1.4956    0.0452\n         eCDF Max Std. Pair Dist.\ndistance   0.4216          0.9740\nhispan     0.1568          1.0743\nwhite      0.2162          0.8390\nblack      0.3730          1.0259\nage        0.2541          1.3938\neduc       0.0757          1.2474\nmarried    0.0216          0.8281\nnodegree   0.0703          1.0106\nre74       0.2757          0.7965\nre75       0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nThe above is more intuitively approached with some plotting as shown below.\n\n# propensity score plots\nplot(m.out, type=\"hist\")\n\n\n\n\n\n\n\n\nWhat I am looking for with the plot above is an improvement in the overlap between the distribution of propensity scores of matched control and treated groups, relative to the raw. There is obviously some improvement and I’ll next check more concretely below how good the match is using SMD.\n\n# --&gt; MATCHHING WITH & WITHOUT A CALIPER\n# Matching without a caliper\n# -&gt; do greedy matching on logit (PS)\nset.seed(42)\npsmatch &lt;- Match(Tr=mydata$treat, M=1, X=pscore, replace=FALSE)\nps_matched &lt;- mydata[unlist(psmatch[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_pscore &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=ps_matched,\n                              test=FALSE)\nprint(matchedtab1_pscore, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.21 (0.41)       0.06 (0.24)     0.453\n  black (mean (SD))       0.47 (0.50)       0.84 (0.36)     0.852\n  white (mean (SD))       0.32 (0.47)       0.10 (0.30)     0.566\n  age (mean (SD))        25.34 (10.53)     25.82 (7.16)     0.053\n  educ (mean (SD))       10.59 (2.63)      10.35 (2.01)     0.106\n  married (mean (SD))     0.21 (0.41)       0.19 (0.39)     0.041\n  nodegree (mean (SD))    0.64 (0.48)       0.71 (0.46)     0.139\n  re74 (mean (SD))     2455.47 (4352.86) 2095.57 (4886.62)  0.078\n  re75 (mean (SD))     1731.45 (2813.88) 1532.06 (3219.25)  0.066\n\n\nThe table above shows marked improvement relative to the raw data. However, variables like education (\\(educ\\)) and lack of high school degree (\\(nodegree\\)) are marginal, while race-related variables (\\(hispan\\), \\(black\\), \\(white\\)) are still too high to safely avoid confounding. Overall it is quite a bit worse than the first attempt with greedy matching I did earlier using the Mahalanobis distance; though \\(re75\\) is markedly better here.\n\nMatching with a caliper\nOne way to try to improve on the matching is to use a caliper; i.e. a threshold (maximum) distance beyond which matching is not allowed. In practice, though somewhat arbitrarily, (1) the propensity scores are logit-transformed, (2) the standard deviation (SD) is calculated, and (3) the caliper is set to 0.2 times the SD, finally (4) the matching is performed subject to the caliper. A smaller caliper, which results in fewer but better pairs, trades more variance for less bias. The code below performs the aforementioned steps.\n\nset.seed(42)\nlogit_pscore = qlogis(pscore)\npsmatch_calip &lt;-Match(Tr=mydata$treat, M=1, X=logit_pscore, replace=FALSE,\n                caliper=.2)\n# Note that the caliper is in St.Dev. units\nmatched_calip &lt;- mydata[unlist(psmatch_calip[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_calip &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=matched_calip,\n                              test=FALSE)\nprint(matchedtab1_calip, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        114               114                 \n  hispan (mean (SD))      0.11 (0.32)       0.10 (0.30)     0.057\n  black (mean (SD))       0.73 (0.45)       0.75 (0.44)     0.040\n  white (mean (SD))       0.16 (0.37)       0.16 (0.37)    &lt;0.001\n  age (mean (SD))        26.35 (10.83)     25.82 (6.93)     0.059\n  educ (mean (SD))       10.53 (2.63)      10.30 (2.29)     0.092\n  married (mean (SD))     0.26 (0.44)       0.24 (0.43)     0.061\n  nodegree (mean (SD))    0.61 (0.49)       0.64 (0.48)     0.054\n  re74 (mean (SD))     2858.97 (4816.95) 2168.29 (5590.28)  0.132\n  re75 (mean (SD))     1969.43 (3027.44) 1053.23 (2597.71)  0.325\n\n# NOTE the smaller number of subjects for each treatment categories, resultng\n# from dropping previously matched subjects.\n\nUsing the caliper has reduced the number of matched pairs down to 114. The high SMDs seen previously (without the caliper). However, for 1974 and 1975 real incomes \\(SMD &gt; 0.1\\). Thus I cannot be certain that the treatment is the only cause of the outcome. The table above suggests subjects’ earning history is still a causal factor. With this in mind, I next run an outcome analysis for all the approaches described previously. I do this at the end rather than after each outcome as a habit to prevent p-hacking.\n\n\nOutcome analyses:\nTo analyze whether the difference in outcome between the treatment and the control groups are different, I run a paired t-test on the various matched data. But first a quick function to avoid some repetition.\n\n# function that accepts a matched data table and runs the paired t-test.\nrun_matched_ttest&lt;- function(matched_table){\n  # get outcome data for both groups\n  treated_outcome &lt;- matched_table$outcome[matched_table$treat==1]\n  control_outcome &lt;- matched_table$outcome[matched_table$treat==0]\n  \n  # compute pairwise difference between both groups\n  diff_outcome &lt;- treated_outcome - control_outcome\n  \n  # paired t-test\n  t.test(diff_outcome)\n}\n\n\\(\\rightarrow\\) Greedy Mahalanobis distance matching:\n\nrun_matched_ttest(greedymatched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.54612, df = 184, p-value = 0.5856\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -965.9393 1705.3767\nsample estimates:\nmean of x \n 369.7187 \n\n\n\\(\\rightarrow\\) Propensity score matching (no caliper)\n\nrun_matched_ttest(ps_matched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.94827, df = 184, p-value = 0.3442\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -708.6934 2020.4023\nsample estimates:\nmean of x \n 655.8544 \n\n\n\\(\\rightarrow\\) Propensity score matching with caliper\n\nrun_matched_ttest(matched_calip)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 1.2773, df = 113, p-value = 0.2041\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -607.6641 2812.9263\nsample estimates:\nmean of x \n 1102.631 \n\n\nThus, from greedy Mahalanobis distance matching to propensity score matching with caliper, the statistical significance of the difference between groups does increase, with propensity score matching with caliper resulting the in the smallest p-value (0.20). This is still conventionally quite high, so that it is impossible to reject the null hypothesis, i.e. that there are no difference between the groups. Moreover, there is still a potential confounding problem in all cases. E.g. the SMD for real income in `74 and `75 remains above 0.1 suggesting I was not able to remove the confounding effect. Note though that I worked for a subset of the original data. Thus I would next re-run the analysis on the entire data set, which could lead to better matching. But that is a story for another day.\nPeaceful coding!\n\n\n\n\nimage source"
  },
  {
    "objectID": "blog/zerosumnormal/on_zero_sum_normal.html",
    "href": "blog/zerosumnormal/on_zero_sum_normal.html",
    "title": "The ZeroSumNormal Distribution",
    "section": "",
    "text": "Overview\nCategorical Regression is a powerful tool when dealing with outcomes that fall seveeral distinct categories. This can be seen as an extension of the Logistic Regression where the goal is the prediction of one of two possible outcomes. An example of categorical regression is voter preference prediction in multi-party elections. This is closely related to Frequentist Multivariate testing in marketing, A/B/n testing. Regardless of the school of thought one subscribes to or the field one operates in categorical regression models can suffer from non-identifiability issues due to overparameterization.\n\n\nNon-identifiability and Overparameterization\nIn categorical regression, each category is typically assigned its own set of parameter; e.g., intercepts or coefficients for each level of a predictor. Without proper constraints, these parameters can play off of each other, their values can shift collectively in a way that leaves the model’s predictions unchanged. This is analogous to making relative measurements without defined axes; you can measure the distance between different points but without a fixed reference, the absolute positions are arbitrary. In other words, the model ends up making relative measurements wihtout an establised baseline, leading to redundancy and ambiguity in parameter estimates. There are two common strategies to address the problem; Pivoting and the ZerosumNormal distibution.\nPivoting involves using one of the categories as a reference as a reference or baseline so that the effects of the remaining categories are measured relative to this reference. Pivoting is useful in scenarios where there is a natural or meaningful control group - for example, in A/B/n testing where one version, usually A, is the baseline, oft referred to as the control group.\nZeroSumNormal Distibution is an approach that imposes a zero-sum constraint on the parameters, effectively anchoring them by ensuring that their sum equal zero. This constraint allows for relative comparisions without arbitrarily selecting one category as the baseline; particularly beneficial in settings where all categories are of equal interest.\nThis post focuses on the ZeroSumNormal approach. In a future post, I will disucss pivoting and its applications.\n\n\nThe ZeroSumnormal Dsitribbution: How it Works\nIF you’re a Bayesian modeller like me you’re in luck - and if not it’s never too late to convert. The ZeroSumNormal distribution is a Bayesian prior designed to mitigate overparameterization and non-identifiability by enforcing a zero-sum constraint on the parameters. This means that the sum of the effects across all categories is forced to equal zero, anchoring the estimates in a way that ensures they are uniquely identifiable.\nConsider a model that predicts product choice baed on demographic predictors such as age, eduation, nationality, etc. Without constraints the model might assign an arbitrary set of intercepts or coefficients to each category, making the parameter estimates mabiguous. The ZeroSumNormal prior anchors these enstimates, ensuring that while the differences between categories remain informative, their absolute values are constrained to a common scale.\nBelow is a code snippet that illustrates the use of the ZeroSumNormal prior by doing the following: * It defines a simple model where \\(zs\\) is a parameter drawn from a \\(ZeroSumNormal\\) distribution over 5 categories. * It sets up the coordinates for the category dimension * It draws prior predictive samples (a process used to assess modeling assumptions before fitting data) * Finally, it computes the sum of the samples across the \\(category\\) axis.\n\n\nCode\nimport pymc as pm\nimport matplotlib.pyplot as pp\n\n# Define the dimensions and a simple model to sample from ZeroSumNormal\nwith pm.Model() as model:\n    # For demonstration, assume we have 5 categories.\n    # The ZeroSumNormal is defined over the 'category' dimension.\n    categories = ['A', 'B', 'C', 'D', 'E']\n    # Set up coordinates so PyMC knows about the dimension.\n    model.add_coord(\"category\", categories)\n    zs = pm.ZeroSumNormal(\"zs\", sigma=1, dims=\"category\")\n    \n    trace = pm.sample_prior_predictive(1000)\n\n\nSampling: [zs]\n\n\nNow I can gather the results of the prior predictive sampling, sum the samples across the categories and plot their distribution. The resulting histogram shows that the sum are centered very close to zero; the zero-sum constraint.\n\n\nCode\n# Extract samples and verify that they sum to nearly zero across categories.\nsamples = trace.prior[\"zs\"].values  # shape: (n_samples, n_categories)\nsums = samples.sum(axis=1)\n\n# Plot the distribution of the sums\nf, ax = pp.subplots(figsize=(8, 4))\nax.hist(sums, bins=100, edgecolor=\"k\", alpha=0.7)\nax.axvline(\n    sums.sum(), color='red', linestyle='--', label=r'$Σ_{categories}$ = ' + f'{sums.sum():.4f}')\nax.set(xlabel=\"Categories\", ylabel=\"Frequency\", title=\"Distribution of ZeroSumNormal Samples\", ylim=(0, 1.1))\nax.legend(fontsize=12, loc='best')\nfor s, cat in zip(sums.ravel(), categories):\n    ax.text(s, 1.05, cat)\n\n\n\n\n\n\n\n\n\n\n\nWhy Choose ZeroSumNormal over Pivoting?\nAs I indicated earlier, pivoting is a practical method when there’s a clear reference outcome. However when each category is of equal interest, selecting an arbitrary baseline can bias the interpretation of the results. The ZeroSumNormal approach avoids this by treating all categories symmetrically, ensuring that each effect is measured relative to the collective group rather than an arbitrary reference.\n\n\nIn Conclusion…\nOverparameterization and the resulting non-identifiability problem in categorical regression can lead to models that lack interpretability and stability. By using the ZeroSumNormal prior, we effectively anchor the parameters through a zero-sum constraint, avoiding the pitfalls of relative measurements without fixed axes. This leads to more robust and interpretable models, especially in contexts like multiparty elections where every category is of equal importance.\n\n\nReferences and links:\n\nThe ZeroSumNormal is discussed in this podcast.\nHere is the documentation of the implementation of the ZeroSumNormal distribution in PyMC.\nHere is a project I’m working on where I use ZeroSumNormal priors in my models."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Porfolio",
    "section": "",
    "text": "Bayesian A/B Testing: A Fully Interactive App\n\n\nHow to build an AWS-friendly Streamlit dashboard complete with session-aware data updates, posterior plots, and ROPE analysis for sound decision support\n\n\n\n\n\n\nJul 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferring PCC from Far Far Away\n\n\nUsing Satellite Data to Predict and Explain Phytoplankton Community Composition with XGBoost and SHAP\n\n\n\nErdem M. Karaköylü & Susanne E. Craig\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/bayesian_ab_testing/index.html",
    "href": "portfolio/bayesian_ab_testing/index.html",
    "title": "Bayesian A/B Testing: A Fully Interactive App",
    "section": "",
    "text": "“Bayesian methods make better use of uncertainty — so why not make them easier to explore?”\nThis post introduces a lightweight, interactive dashboard for Bayesian A/B testing built with Streamlit and ArviZ. It’s designed to help you experiment faster, think probabilistically, and get clear answers — without writing formulas or wrangling with p-values."
  },
  {
    "objectID": "portfolio/clv_case_study/index.html",
    "href": "portfolio/clv_case_study/index.html",
    "title": "BayEstonia!",
    "section": "",
    "text": "Project Summary\n\n\n\nBayesian categorical regression on Estonian voter data using PyMC. Explores how demographic variables predict party preference and leverages the ZeroSumNormal distribution to address identifiability."
  },
  {
    "objectID": "portfolio/clv_case_study/index.html#introduction",
    "href": "portfolio/clv_case_study/index.html#introduction",
    "title": "BayEstonia!",
    "section": "Introduction",
    "text": "Introduction\nThis portfolio project models voting preferences in Estonia using demographic data and a Bayesian categorical likelihood. The goal is not just to predict voter behavior, but to extract interpretable insights about how education, nationality, age, and gender influence party preference.\nWe leverage PyMC and the ZeroSumNormal prior to address overparameterization, a common challenge in categorical regression. The Bayesian framework allows us to work directly with posterior distributions, giving a probabilistic handle on model uncertainty and demographic effects."
  },
  {
    "objectID": "portfolio/clv_case_study/index.html#data-overview",
    "href": "portfolio/clv_case_study/index.html#data-overview",
    "title": "BayEstonia!",
    "section": "Data Overview",
    "text": "Data Overview\nThe dataset comes from SALK, the Liberal Citizen Foundation in Estonia. It contains individual-level voting data annotated with demographics. Each row represents one voter, and the columns include:\n\nDemographics: age group, gender, education, nationality\n\nGeography: electoral district and unit\n\nParty choices: 11 binary columns (one per party) marking vote intent\n\nWe renamed ambiguous columns for clarity (e.g., “Hard to say” → “Undecided”) and excluded geography from modeling to focus strictly on demographics."
  },
  {
    "objectID": "portfolio/clv_case_study/index.html#exploratory-analysis",
    "href": "portfolio/clv_case_study/index.html#exploratory-analysis",
    "title": "BayEstonia!",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nBefore modeling, we explore how voting preferences differ across demographic groups. Below are two representative plots.\n\nParty Preference by Nationality\nEstonians and non-Estonians (predominantly ethnic Russians) show clear divergences:\n\nEstonians tend to favor Reformierakond and EKRE.\n\nNon-Estonians show higher preference for Keskerakond and “Mitte ükski erakond” (“None of the above”).\n\nFigure: Distribution of party choices by nationality."
  },
  {
    "objectID": "portfolio/bayesian_ab_testing/index.html#why-bayesian",
    "href": "portfolio/bayesian_ab_testing/index.html#why-bayesian",
    "title": "Bayesian A/B Testing: A Fully Interactive App",
    "section": "Why Bayesian?",
    "text": "Why Bayesian?\nA quick refresher: in contrast to the frequentist approach, Bayesian A/B testing gives us the full distribution of possible outcomes. It lets us:\n\nCompute P(B &gt; A) directly\nVisualize the magnitude and uncertainty of effects\nIncorporate prior beliefs\nDefine a ROPE: a practical threshold for saying “this difference doesn’t matter”"
  },
  {
    "objectID": "portfolio/bayesian_ab_testing/index.html#app-features",
    "href": "portfolio/bayesian_ab_testing/index.html#app-features",
    "title": "Bayesian A/B Testing: A Fully Interactive App",
    "section": "App Features",
    "text": "App Features\nHere’s what the app supports:\n\nData input: sample, upload, or manual\nGroup mapping: assign any group labels to A and B\nFlexible priors: control the beta distribution for both arms\nHDI control: choose your desired confidence level\nROPE region: set a tolerance range for negligible effects\nCumulative updates: simulate sequential testing\nSession restart: start fresh anytime"
  },
  {
    "objectID": "portfolio/bayesian_ab_testing/index.html#interface-layout",
    "href": "portfolio/bayesian_ab_testing/index.html#interface-layout",
    "title": "Building an Interactive Bayesian A/B Testing App with Streamlit",
    "section": "🖥️ Interface Layout",
    "text": "🖥️ Interface Layout\n```{.noexecute} Sidebar: Main Area: [ data input ] [ Update Button ] [ group mapping ] [ Restart Button ] [ prior sliders ] [ Tabs: ] [ ROPE settings ] 📈 Posterior Plots 📋 Summary Table ✅ Decision Metrics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "Machine Learning Researcher – Probabilistic Programmer – All-around Bayesian Enthusiast!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Erdem Karaköylü – Data Scientist",
    "section": "",
    "text": "Erdem Karaköylü\nUniversity Park, MD\nerdemk@protonmail.com | erdemkarakoylu.github.io | LinkedIn\n\n\n💻 Data Scientist | Bayesian Modeling & Machine Learning Specialist\nPractical AI expert specializing in Bayesian methods, LLM pipelines (RAG), and predictive modeling across defense, climate, and environmental domains. Builds robust, uncertainty-aware systems to enhance decision-making.\n\n\n\n🛠 Core Skills\nBayesian & Statistical Modeling\nHierarchical Modeling (Regression & Classification) · Bayesian Additive Regression Trees (BART) · Monte Carlo Simulation · Probabilistic Programming · A/B Testing\nEmerging Interests\nCausal Inference (incl. do-calculus in PyMC) · Bayesian Decision Theory\nTechnical Stack\nPython · PyMC · Scikit-learn · XGBoost · PyTorch · Pandas · Git · Matplotlib · Seaborn · ArviZ · XArray · SQL\n\n\n\n🧪 Experience & Research Highlights\nFreelance Data Scientist – Marine Remote Sensing and Ecological Forecasting\nBayesian modeling and machine learning for environmental data\n\nDeveloped Bayesian Additive Regression Tree (BART) and hierarchical models to estimate marine optical properties and chlorophyll concentrations from satellite radiance data.\nBuilt predictive XGBoost models to infer phytoplankton community structure, outperforming baseline approaches.\nUsed probabilistic ODE parameter estimation to analyze nonlinear dynamics in marine ecological systems.\nPublished reproducible Bayesian modeling workflows as open-source Jupyter notebooks for environmental science.\n\nData Scientist – Research Innovations Inc. (Alexandria, VA)\nDOD and DOJ-focused machine learning and NLP systems\n\nContributed to the development of a Retrieval-Augmented Generation (RAG) system that improved information retrieval for military planners.\nLed Bayesian A/B testing to optimize system components and refine model selection for production environments.\nBuilt and iteratively refined an active-learning image classification pipeline to reduce manual annotation requirements.\nSupported targeted sentiment analysis using fine-tuned large language models for sensitive domains.\n\nMachine Learning Researcher – NASA Goddard Ocean Biology Processing Group / SAIC\nEarth observation and probabilistic modeling for ocean color remote sensing\n\nDeveloped Bayesian models to predict satellite-derived ocean color products, improving chlorophyll and particulate property estimates.\nConducted Monte Carlo simulations to quantify uncertainty and error propagation in remote sensing reflectance (Rrs) data.\nCreated climate data visualizations and analysis pipelines supporting scientific reports and satellite mission deliverables.\nAdvocated for probabilistic approaches and led internal discussions on Bayesian methods for biogeophysical modeling.\n\nResearcher – UC San Diego / Scripps Institution of Oceanography\nThesis: Foraging Sorties Hypothesis – Inferring Behavioral Rhythms in Marine Primary Consumers\n\nAdapted a planar laser-induced fluorescence (PLIF) imaging system to quantify real-time feeding states in individual marine zooplankton.\nCaptured high-resolution time series of gut pigment dynamics to infer behavioral state transitions (feeding, digestion, resting).\nBuilt an individual-based model linking physiological state to vertical foraging behavior under environmental constraints.\nCalibrated imaging measurements against chemical extraction to ensure accuracy and repeatability across individuals.\nFindings published in Limnology and Oceanography Methods (2009); follow-up research on temperature effects published in Journal of Plankton Research (2012), where a gut fluorescence image was featured on the journal cover.\n\n\n\n\n📚 Education\nPh.D. – Biological Oceanography & Marine Ecology\nScripps Institution of Oceanography, UC San Diego\nB.Sc. – Oceanography\nFlorida Institute of Technology\n\n\n🌍 Languages\n\nEnglish – Native/Trilingual\nFrench - Native/Trilingual\nTurkish – Native/Trilingual\nSpanish – Advanced\n\nDownload PDF file."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "Erdem Karaköylü\nUniversity Park, MD\nerdemk@protonmail.com | erdemkarakoylu.github.io | LinkedIn\n\n\n💻 Data Scientist | Bayesian Modeling & Machine Learning Specialist\nPractical AI expert specializing in Bayesian methods, LLM pipelines (RAG), and predictive modeling across defense, climate, and environmental domains. Builds robust, uncertainty-aware systems to enhance decision-making and operational automation.\n\n\n\n🛠 Core Skills\nBayesian & Statistical Modeling\nBayesian Inference · Hierarchical Modeling · Monte Carlo Simulation · Probabilistic Programming · A/B Testing · Satellite Remote Sensing\nMachine Learning\nBayesian Additive Regression Trees (BART) · XGBoost · Image Classification · Active Learning · Model Calibration\nEmerging Interests\nCausal Inference (incl. do-calculus in PyMC) · Decision Metrics · Experimental Design\nTechnical Stack\nPython · PyMC · Scikit-learn · PyTorch · Pandas · SQL · Git · Matplotlib · Seaborn · ArviZ · XArray\n\n\n\n🧪 Experience & Research Highlights\nFreelance Data Scientist – Marine Remote Sensing and Ecological Forecasting\nBayesian modeling and machine learning for environmental data\n\nDeveloped Bayesian Additive Regression Tree (BART) and hierarchical models to estimate marine optical properties and chlorophyll concentrations from satellite radiance data.\nBuilt predictive XGBoost models to infer phytoplankton community structure, outperforming baseline approaches.\nUsed probabilistic ODE parameter estimation to analyze nonlinear dynamics in marine ecological systems.\nPublished reproducible Bayesian modeling workflows as open-source Jupyter notebooks for environmental science.\n\nData Scientist – Research Innovations Inc. (Alexandria, VA)\nDOD and DOJ-focused machine learning and NLP systems\n\nContributed to the development of a Retrieval-Augmented Generation (RAG) system that improved information retrieval for military planners.\nLed Bayesian A/B testing to optimize system components and refine model selection for production environments.\nBuilt and iteratively refined an active-learning image classification pipeline to reduce manual annotation requirements.\nSupported targeted sentiment analysis using fine-tuned large language models for sensitive domains.\n\nMachine Learning Researcher – NASA Goddard Ocean Biology Processing Group / SAIC\nEarth observation and probabilistic modeling for ocean color remote sensing\n\nDeveloped Bayesian models to predict satellite-derived ocean color products, improving chlorophyll and particulate property estimates.\nConducted Monte Carlo simulations to quantify uncertainty and error propagation in remote sensing reflectance (Rrs) data.\nCreated climate data visualizations and analysis pipelines supporting scientific reports and satellite mission deliverables.\nAdvocated for probabilistic approaches and led internal discussions on Bayesian methods for biogeophysical modeling.\n\nResearcher – UC San Diego / Scripps Institution of Oceanography\nThesis: Foraging Sorties Hypothesis – Inferring Behavioral Rhythms in Marine Primary Consumers\n\nAdapted a planar laser-induced fluorescence (PLIF) imaging system to quantify real-time feeding states in individual marine zooplankton.\nCaptured high-resolution time series of gut pigment dynamics to infer behavioral state transitions (feeding, digestion, resting).\nBuilt an individual-based model linking physiological state to vertical foraging behavior under environmental constraints.\nCalibrated imaging measurements against chemical extraction to ensure accuracy and repeatability across individuals.\nFindings published in Limnology and Oceanography Methods (2009); follow-up research on temperature effects published in Journal of Plankton Research (2012), where a gut fluorescence image was featured on the journal cover.\n\n\n\n\n📚 Education\nPh.D. – Biological Oceanography & Marine Ecology\nScripps Institution of Oceanography, UC San Diego\nB.Sc. – Oceanography\nFlorida Institute of Technology\n\n\n🌍 Languages\n\nEnglish – Native/Trilingual\nFrench - Native/Trilingual\nTurkish – Native/Trilingual\nSpanish – Advanced\n\nDownload PDF file."
  }
]