[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "Machine Learning Researcher – Probabilistic Programmer – All-around Bayesian Enthusiast!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Musings & Crumbs",
    "section": "",
    "text": "Evaluating causality in observational data\n\n\n\n\n\n\nErdem Karaköylü\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I went from sitting to standing to a crouching desk solution.\n\n\n\n\n\n\nErdem Karaköylü\n\n\nAug 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html",
    "href": "blog/causailty_in_observation_data/index.html",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "Preamble\nIn the real world, we often want to understand if something (a new medicine, a marketing campaign, a policy change) truly causes an outcome. In ideal experiments (like Randomized Control Trials or RCTs), we can control all factors, making it easier to determine cause and effect.\nBut what if we only have observational data, where things weren’t neatly controlled? Establishing a causal link becomes tricky. There might be hidden factors, called confounders, that influence both the treatment and the outcome, making it hard to tell if the treatment is the real cause. This post explores ways to tackle this challenge.\nExample: Does Exercise Cause Weight Loss?\nImagine a study on the impact of a new exercise program on weight loss. Participants with higher fitness levels might be more likely to choose the program. This becomes a confounder – fitness level influences both program participation and weight loss outcomes. Propensity score matching can help – it pairs individuals with similar fitness levels but different program participation, isolating the program’s true effect.\nUsing the Lalonde Dataset\nTo illustrate how matching and propensity scores can help with causal analysis, I’ll use a dataset from a study by Lalonde (1986) evaluating an employment and training program. The study aimed to understand if the program truly caused better employment outcomes. Let’s load the data from the ‘Matching’ R library and take a closer look:\nlibrary(MatchIt)\ndata(lalonde)\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html#matching",
    "href": "blog/causailty_in_observation_data/index.html#matching",
    "title": "Evaluating causality in observational data",
    "section": "Matching",
    "text": "Matching\n\nTo match or not to match?\nA first step is whether the data on hand is appropriate for causal inferrence, in particular, whether it should be balanced. A commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;-c(\"hispan\", \"black\", \"white\", \"age\", \"educ\", \"married\", \"nodegree\", \n              \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata, test=FALSE)\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        429               185                 \n  hispan (mean (SD))      0.14 (0.35)       0.06 (0.24)     0.277\n  black (mean (SD))       0.20 (0.40)       0.84 (0.36)     1.668\n  white (mean (SD))       0.66 (0.48)       0.10 (0.30)     1.406\n  age (mean (SD))        28.03 (10.79)     25.82 (7.16)     0.242\n  educ (mean (SD))       10.24 (2.86)      10.35 (2.01)     0.045\n  married (mean (SD))     0.51 (0.50)       0.19 (0.39)     0.719\n  nodegree (mean (SD))    0.60 (0.49)       0.71 (0.46)     0.235\n  re74 (mean (SD))     5619.24 (6788.75) 2095.57 (4886.62)  0.596\n  re75 (mean (SD))     2466.48 (3292.00) 1532.06 (3219.25)  0.287\n\n\nNote that an alternative would be to conduct two-tailed t-tests to assess difference between group (treated and control) means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size. I therefore use \\(SMD\\) in this post.\nBy convention, an \\(SMD\\) greater than 0.1 suggest an imbalance with respect to the corresponding covariate. Here \\(SMD&gt;0.1\\) for all covariates except education. Treated subjects need each to be match via greedy matching to as close as possible a control subject. Matching between subjects is done on the basis of a distance metric indicating how separated they are in the covariate space. The specific metric used in this case is the Mahalanobis distance, which is a kind of standardized difference, computed as follows: \\[d = \\sqrt{(X_i-X_j)^T C^{-1} (X_i-X_j) }\\] where \\(X\\) is a covariate, \\(i\\) and \\(j\\) are treated and control subjects, and \\(C\\) is the covariance matrix\n\nlibrary(Matching)\n# Below M=1 refers to pairwise matching. Even so if \"ties\" is left as TRUE (default)\n# multiple subjects within the tolerance threshold will all be matched. \n# In this case, e.g. not setting ties=TRUE yields 207 pairs, even though there are only # 185 treated subjects.\ngreedymatch&lt;-Match(Tr=treat, M=1, X=mydata[xvars], ties=FALSE) \ngreedymatched&lt;-mydata[unlist(greedymatch[c(\"index.treated\", \"index.control\")]), ]\n\nI create another Table 1 with the matched data check the SMDs.\n\nmatchedtab1&lt;-CreateTableOne(vars=xvars, strata=\"treat\", data=greedymatched, test=FALSE)\nprint(matchedtab1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.06 (0.24)       0.06 (0.24)    &lt;0.001\n  black (mean (SD))       0.84 (0.37)       0.84 (0.36)     0.015\n  white (mean (SD))       0.10 (0.30)       0.10 (0.30)     0.018\n  age (mean (SD))        25.36 (8.29)      25.82 (7.16)     0.059\n  educ (mean (SD))       10.45 (1.96)      10.35 (2.01)     0.054\n  married (mean (SD))     0.19 (0.39)       0.19 (0.39)    &lt;0.001\n  nodegree (mean (SD))    0.71 (0.46)       0.71 (0.46)    &lt;0.001\n  re74 (mean (SD))     2159.92 (4240.18) 2095.57 (4886.62)  0.014\n  re75 (mean (SD))     1119.08 (2442.29) 1532.06 (3219.25)  0.145\n\n\nGreedy pairwise matching yields, as expected, a reduced data set with 185 subjects in each group. This time all but the variable \\(re75\\) have corresponding \\(SMD&lt;0.1\\). This is not entirely satisfactory and I will attempt to balance the data set using propensity scores next"
  },
  {
    "objectID": "blog/causailty_in_observation_data/index.html#propensity-score-matching",
    "href": "blog/causailty_in_observation_data/index.html#propensity-score-matching",
    "title": "Evaluating causality in observational data",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nA propensity score denoted here \\(\\pi\\) is defined as the probability that a subject \\(i\\) received treatment conditioned on the covariates \\(X\\). I.e. \\(\\pi_i = P(T=1|X_i)\\). A \\(\\pi_i=0.4\\) means there’s a 40% chance the corresponding subject will receive treatment. Covariates can increase or decrease the probability of receiving treatment. For example, if \\(X\\), simplistically the only covariate, is a boolean variable for smoking and smokers are more likely to get a particular treatment then \\(P(T=1|X=1) \\gt P(T=1|X=0)\\).\nInterestingly, two subjects may have the same propensity score in spite of having different covariate values \\(X\\), meaning they are equally likely to receive treatment. Thus reducing the data to a subset of subjects with the same \\(\\pi\\) should balance the treated and control groups. In doing so a crucial assumption in causality, ignorability i.e. how a subject ended in one or the other group can be safely ignored. In a randomized trial, the propensity score is known. In an observational study \\(\\pi\\) is unknown. However because both \\(X\\) and \\(T\\) are collected, \\(\\pi\\) can be estimated. For this I will fit a logistic regression, where the covariates are the input and the treatment variable is the output. Using this model I can get the predicted probability of treatment for each subject; i.e. the estimated \\(\\pi\\).\n\npsmodel &lt;- glm(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75,family=binomial(), data=mydata)\n\nThe model fit is summarized below.\n\n# show fit summary\nsummary(psmodel)\n\n\nCall:\nglm(formula = treat ~ hispan + white + black + age + educ + married + \n    nodegree + re74 + re75, family = binomial(), data = mydata)\n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.663e+00  9.709e-01  -1.713  0.08668 .  \nhispan      -2.082e+00  3.672e-01  -5.669 1.44e-08 ***\nwhite       -3.065e+00  2.865e-01 -10.699  &lt; 2e-16 ***\nblack               NA         NA      NA       NA    \nage          1.578e-02  1.358e-02   1.162  0.24521    \neduc         1.613e-01  6.513e-02   2.477  0.01325 *  \nmarried     -8.321e-01  2.903e-01  -2.866  0.00415 ** \nnodegree     7.073e-01  3.377e-01   2.095  0.03620 *  \nre74        -7.178e-05  2.875e-05  -2.497  0.01253 *  \nre75         5.345e-05  4.635e-05   1.153  0.24884    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 751.49  on 613  degrees of freedom\nResidual deviance: 487.84  on 605  degrees of freedom\nAIC: 505.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nNext I extract the propensity scores from the model object.\n\n# create propensity score\npscore&lt;-psmodel$fitted.values\n\nFinally, I use the MatchIt package to match subjects based on their propensity scores. Note that I set a seed for reproducibility, since matching randomizes data as a first step.\n\nset.seed(42)\nm.out&lt;-matchit(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75, data=mydata, method=\"nearest\")\n\nThe matching results are summarized below.\n\n# summarize the matching outcome\nsummary(m.out)\n\n\nCall:\nmatchit(formula = treat ~ hispan + white + black + age + educ + \n    married + nodegree + re74 + re75, data = mydata, method = \"nearest\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.1822          1.7941     0.9211    0.3774\nhispan          0.0595        0.1422         -0.3498          .    0.0827\nwhite           0.0973        0.6550         -1.8819          .    0.5577\nblack           0.8432        0.2028          1.7615          .    0.6404\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\ndistance   0.6444\nhispan     0.0827\nwhite      0.5577\nblack      0.6404\nage        0.1577\neduc       0.1114\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.3629          0.9739     0.7566    0.1321\nhispan          0.0595        0.2162         -0.6629          .    0.1568\nwhite           0.0973        0.3135         -0.7296          .    0.2162\nblack           0.8432        0.4703          1.0259          .    0.3730\nage            25.8162       25.3027          0.0718     0.4568    0.0847\neduc           10.3459       10.6054         -0.1290     0.5721    0.0239\nmarried         0.1892        0.2108         -0.0552          .    0.0216\nnodegree        0.7081        0.6378          0.1546          .    0.0703\nre74         2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75         1532.0553     1614.7451         -0.0257     1.4956    0.0452\n         eCDF Max Std. Pair Dist.\ndistance   0.4216          0.9740\nhispan     0.1568          1.0743\nwhite      0.2162          0.8390\nblack      0.3730          1.0259\nage        0.2541          1.3938\neduc       0.0757          1.2474\nmarried    0.0216          0.8281\nnodegree   0.0703          1.0106\nre74       0.2757          0.7965\nre75       0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nThe above is more intuitively approached with some plotting as shown below.\n\n# propensity score plots\nplot(m.out, type=\"hist\")\n\n\n\n\nWhat I am looking for with the plot above is an improvement in the overlap between the distribution of propensity scores of matched control and treated groups, relative to the raw. There is obviously some improvement and I’ll next check more concretely below how good the match is using SMD.\n\n# --&gt; MATCHHING WITH & WITHOUT A CALIPER\n# Matching without a caliper\n# -&gt; do greedy matching on logit (PS)\nset.seed(42)\npsmatch &lt;- Match(Tr=mydata$treat, M=1, X=pscore, replace=FALSE)\nps_matched &lt;- mydata[unlist(psmatch[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_pscore &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=ps_matched,\n                              test=FALSE)\nprint(matchedtab1_pscore, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.21 (0.41)       0.06 (0.24)     0.453\n  black (mean (SD))       0.47 (0.50)       0.84 (0.36)     0.852\n  white (mean (SD))       0.32 (0.47)       0.10 (0.30)     0.566\n  age (mean (SD))        25.34 (10.53)     25.82 (7.16)     0.053\n  educ (mean (SD))       10.59 (2.63)      10.35 (2.01)     0.106\n  married (mean (SD))     0.21 (0.41)       0.19 (0.39)     0.041\n  nodegree (mean (SD))    0.64 (0.48)       0.71 (0.46)     0.139\n  re74 (mean (SD))     2455.47 (4352.86) 2095.57 (4886.62)  0.078\n  re75 (mean (SD))     1731.45 (2813.88) 1532.06 (3219.25)  0.066\n\n\nThe table above shows marked improvement relative to the raw data. However, variables like education (\\(educ\\)) and lack of high school degree (\\(nodegree\\)) are marginal, while race-related variables (\\(hispan\\), \\(black\\), \\(white\\)) are still too high to safely avoid confounding. Overall it is quite a bit worse than the first attempt with greedy matching I did earlier using the Mahalanobis distance; though \\(re75\\) is markedly better here.\n\nMatching with a caliper\nOne way to try to improve on the matching is to use a caliper; i.e. a threshold (maximum) distance beyond which matching is not allowed. In practice, though somewhat arbitrarily, (1) the propensity scores are logit-transformed, (2) the standard deviation (SD) is calculated, and (3) the caliper is set to 0.2 times the SD, finally (4) the matching is performed subject to the caliper. A smaller caliper, which results in fewer but better pairs, trades more variance for less bias. The code below performs the aforementioned steps.\n\nset.seed(42)\nlogit_pscore = qlogis(pscore)\npsmatch_calip &lt;-Match(Tr=mydata$treat, M=1, X=logit_pscore, replace=FALSE,\n                caliper=.2)\n# Note that the caliper is in St.Dev. units\nmatched_calip &lt;- mydata[unlist(psmatch_calip[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_calip &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=matched_calip,\n                              test=FALSE)\nprint(matchedtab1_calip, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        114               114                 \n  hispan (mean (SD))      0.11 (0.32)       0.10 (0.30)     0.057\n  black (mean (SD))       0.73 (0.45)       0.75 (0.44)     0.040\n  white (mean (SD))       0.16 (0.37)       0.16 (0.37)    &lt;0.001\n  age (mean (SD))        26.35 (10.83)     25.82 (6.93)     0.059\n  educ (mean (SD))       10.53 (2.63)      10.30 (2.29)     0.092\n  married (mean (SD))     0.26 (0.44)       0.24 (0.43)     0.061\n  nodegree (mean (SD))    0.61 (0.49)       0.64 (0.48)     0.054\n  re74 (mean (SD))     2858.97 (4816.95) 2168.29 (5590.28)  0.132\n  re75 (mean (SD))     1969.43 (3027.44) 1053.23 (2597.71)  0.325\n\n# NOTE the smaller number of subjects for each treatment categories, resultng\n# from dropping previously matched subjects.\n\nUsing the caliper has reduced the number of matched pairs down to 114. The high SMDs seen previously (without the caliper). However, for 1974 and 1975 real incomes \\(SMD &gt; 0.1\\). Thus I cannot be certain that the treatment is the only cause of the outcome. The table above suggests subjects’ earning history is still a causal factor. With this in mind, I next run an outcome analysis for all the approaches described previously. I do this at the end rather than after each outcome as a habit to prevent p-hacking.\n\n\nOutcome analyses:\nTo analyze whether the difference in outcome between the treatment and the control groups are different, I run a paired t-test on the various matched data. But first a quick function to avoid some repetition.\n\n# function that accepts a matched data table and runs the paired t-test.\nrun_matched_ttest&lt;- function(matched_table){\n  # get outcome data for both groups\n  treated_outcome &lt;- matched_table$outcome[matched_table$treat==1]\n  control_outcome &lt;- matched_table$outcome[matched_table$treat==0]\n  \n  # compute pairwise difference between both groups\n  diff_outcome &lt;- treated_outcome - control_outcome\n  \n  # paired t-test\n  t.test(diff_outcome)\n}\n\n\\(\\rightarrow\\) Greedy Mahalanobis distance matching:\n\nrun_matched_ttest(greedymatched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.015636, df = 184, p-value = 0.9875\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1358.771  1380.481\nsample estimates:\nmean of x \n 10.85483 \n\n\n\\(\\rightarrow\\) Propensity score matching (no caliper)\n\nrun_matched_ttest(ps_matched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.94827, df = 184, p-value = 0.3442\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -708.6934 2020.4023\nsample estimates:\nmean of x \n 655.8544 \n\n\n\\(\\rightarrow\\) Propensity score matching with caliper\n\nrun_matched_ttest(matched_calip)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 1.2773, df = 113, p-value = 0.2041\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -607.6641 2812.9263\nsample estimates:\nmean of x \n 1102.631 \n\n\nThus, from greedy Mahalanobis distance matching to propensity score matching with caliper, the statistical significance of the difference between groups does increase, with propensity score matching with caliper resulting the in the smallest p-value (0.20). This is still conventionally quite high, so that it is impossible to reject the null hypothesis, i.e. that there are no difference between the groups. Moreover, there is still a potential confounding problem in all cases. E.g. the SMD for real income in `74 and `75 remains above 0.1 suggesting I was not able to remove the confounding effect. Note though that I worked for a subset of the original data. Thus I would next re-run the analysis on the entire data set, which could lead to better matching. But that is a story for another day.\nPeaceful coding!\n\n\n\n\nimage source"
  },
  {
    "objectID": "blog/crouching_desk_hidden_benefits/index.html",
    "href": "blog/crouching_desk_hidden_benefits/index.html",
    "title": "Why I went from sitting to standing to a crouching desk solution.",
    "section": "",
    "text": "My main health concern as I grow older is to remain limber. This is particularly important to me because one of my hobbies is to compete in Brazilian Jiu Jitsu tournaments, where my rivals are 20-30 years younger than I am. The main obstacles to that ambition is how I spend most of my time."
  },
  {
    "objectID": "assets/musings/posts/hyper_param_optim/Untitled-1.html",
    "href": "assets/musings/posts/hyper_param_optim/Untitled-1.html",
    "title": "Untitled",
    "section": "",
    "text": "Preamble\n\n\nThis project deals with classifying content as fake or real based given (1) its propagation pattern through social media and (2) features extracted from the account of users participating in the propagation.\n\n\nContent\n\n\nDisinformation in the form of fake news is nothing new. However starting with the election of 2016 and without a pause or sign of abating thereafter, disinformation has been at the forefont of concerns regarding the safety of democracies and free thought around the world. This is because social media vastly increases the potential reach of nefarious content.\n\n\nimport numpy as np\nimport matplotlib.pyplot as pp\npp.style.use('fivethirtyeight')\nn = 100\nx = np.random.randn(n).reshape(-1, 1)\nnoise = 0.1 * np.random.randn(n).reshape(-1, 1)\np = np.tanh(x + noise) / 2 + 0.5\n\n\nax = axs[2]\n\n\nax.set_vi\n\n\nf, axs = pp.subplots(nrows=2, ncols=2, figsize=(10, 10))\naxs = axs.ravel()\ntitles = ['x', 'noise', 'p']\nfor ax, q, t in zip(axs, [x, noise, p], titles):\n    ax.hist(q)\n    ax.set_title(t)\n#@axs[1].hist(noise)\n #   axs[3].hist(p)\naxs[3].set_visible('off')\n\n\n\n\n\npp.hist(noise)\n\n(array([ 6.,  9., 10., 13., 17., 11., 12., 10., 10.,  2.]),\n array([-0.19391108, -0.1505585 , -0.10720592, -0.06385333, -0.02050075,\n         0.02285183,  0.06620441,  0.10955699,  0.15290958,  0.19626216,\n         0.23961474]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\np.round(2)\n\narray([[0.08],\n       [0.17],\n       [0.04],\n       [0.73],\n       [0.69],\n       [0.03],\n       [0.02],\n       [0.02],\n       [0.78],\n       [0.9 ],\n       [0.37],\n       [0.72],\n       [0.71],\n       [0.6 ],\n       [0.84],\n       [0.35],\n       [0.02],\n       [0.33],\n       [0.  ],\n       [0.87],\n       [0.64],\n       [0.85],\n       [0.27],\n       [0.93],\n       [0.06],\n       [0.97],\n       [0.92],\n       [0.56],\n       [0.74],\n       [0.61],\n       [0.95],\n       [0.52],\n       [0.01],\n       [0.82],\n       [0.99],\n       [0.02],\n       [0.19],\n       [0.74],\n       [0.81],\n       [0.89],\n       [0.89],\n       [0.87],\n       [0.52],\n       [0.23],\n       [0.58],\n       [0.62],\n       [0.05],\n       [0.82],\n       [0.2 ],\n       [0.98],\n       [0.35],\n       [0.76],\n       [0.55],\n       [0.68],\n       [0.33],\n       [0.04],\n       [0.15],\n       [0.02],\n       [0.16],\n       [0.29],\n       [0.18],\n       [0.03],\n       [0.63],\n       [0.37],\n       [0.91],\n       [0.35],\n       [0.75],\n       [0.46],\n       [0.13],\n       [0.35],\n       [0.82],\n       [0.84],\n       [0.59],\n       [0.09],\n       [0.28],\n       [0.76],\n       [0.55],\n       [0.3 ],\n       [0.31],\n       [0.22],\n       [0.16],\n       [0.97],\n       [0.13],\n       [0.3 ],\n       [0.24],\n       [0.34],\n       [0.07],\n       [0.55],\n       [0.49],\n       [0.38],\n       [0.01],\n       [0.99],\n       [0.02],\n       [0.6 ],\n       [0.22],\n       [0.97],\n       [0.1 ],\n       [0.58],\n       [0.74],\n       [0.06]])"
  },
  {
    "objectID": "assets/musings/posts/causailty_in_observation_data/index.html",
    "href": "assets/musings/posts/causailty_in_observation_data/index.html",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "Preamble\nIn the real world, we often want to understand if something (a new medicine, a marketing campaign, a policy change) truly causes an outcome. In ideal experiments (like Randomized Control Trials or RCTs), we can control all factors, making it easier to determine cause and effect.\nBut what if we only have observational data, where things weren’t neatly controlled? Establishing a causal link becomes tricky. There might be hidden factors, called confounders, that influence both the treatment and the outcome, making it hard to tell if the treatment is the real cause. This post explores ways to tackle this challenge.\nExample: Does Exercise Cause Weight Loss?\nImagine a study on the impact of a new exercise program on weight loss. Participants with higher fitness levels might be more likely to choose the program. This becomes a confounder – fitness level influences both program participation and weight loss outcomes. Propensity score matching can help – it pairs individuals with similar fitness levels but different program participation, isolating the program’s true effect.\nUsing the Lalonde Dataset\nTo illustrate how matching and propensity scores can help with causal analysis, I’ll use a dataset from a study by Lalonde (1986) evaluating an employment and training program. The study aimed to understand if the program truly caused better employment outcomes. Let’s load the data from the ‘Matching’ R library and take a closer look:\nlibrary(MatchIt)\ndata(lalonde)\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "assets/musings/posts/causailty_in_observation_data/index.html#matching",
    "href": "assets/musings/posts/causailty_in_observation_data/index.html#matching",
    "title": "Evaluating causality in observational data",
    "section": "Matching",
    "text": "Matching\n\nTo match or not to match?\nA first step is whether the data on hand is appropriate for causal inferrence, in particular, whether it should be balanced. A commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;-c(\"hispan\", \"black\", \"white\", \"age\", \"educ\", \"married\", \"nodegree\", \n              \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata, test=FALSE)\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        429               185                 \n  hispan (mean (SD))      0.14 (0.35)       0.06 (0.24)     0.277\n  black (mean (SD))       0.20 (0.40)       0.84 (0.36)     1.668\n  white (mean (SD))       0.66 (0.48)       0.10 (0.30)     1.406\n  age (mean (SD))        28.03 (10.79)     25.82 (7.16)     0.242\n  educ (mean (SD))       10.24 (2.86)      10.35 (2.01)     0.045\n  married (mean (SD))     0.51 (0.50)       0.19 (0.39)     0.719\n  nodegree (mean (SD))    0.60 (0.49)       0.71 (0.46)     0.235\n  re74 (mean (SD))     5619.24 (6788.75) 2095.57 (4886.62)  0.596\n  re75 (mean (SD))     2466.48 (3292.00) 1532.06 (3219.25)  0.287\n\n\nNote that an alternative would be to conduct two-tailed t-tests to assess difference between group (treated and control) means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size. I therefore use \\(SMD\\) in this post.\nBy convention, an \\(SMD\\) greater than 0.1 suggest an imbalance with respect to the corresponding covariate. Here \\(SMD&gt;0.1\\) for all covariates except education. Treated subjects need each to be match via greedy matching to as close as possible a control subject. Matching between subjects is done on the basis of a distance metric indicating how separated they are in the covariate space. The specific metric used in this case is the Mahalanobis distance, which is a kind of standardized difference, computed as follows: \\[d = \\sqrt{(X_i-X_j)^T C^{-1} (X_i-X_j) }\\] where \\(X\\) is a covariate, \\(i\\) and \\(j\\) are treated and control subjects, and \\(C\\) is the covariance matrix\n\nlibrary(Matching)\n# Below M=1 refers to pairwise matching. Even so if \"ties\" is left as TRUE (default)\n# multiple subjects within the tolerance threshold will all be matched. \n# In this case, e.g. not setting ties=TRUE yields 207 pairs, even though there are only # 185 treated subjects.\ngreedymatch&lt;-Match(Tr=treat, M=1, X=mydata[xvars], ties=FALSE) \ngreedymatched&lt;-mydata[unlist(greedymatch[c(\"index.treated\", \"index.control\")]), ]\n\nI create another Table 1 with the matched data check the SMDs.\n\nmatchedtab1&lt;-CreateTableOne(vars=xvars, strata=\"treat\", data=greedymatched, test=FALSE)\nprint(matchedtab1, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.06 (0.24)       0.06 (0.24)    &lt;0.001\n  black (mean (SD))       0.84 (0.37)       0.84 (0.36)     0.015\n  white (mean (SD))       0.10 (0.30)       0.10 (0.30)     0.018\n  age (mean (SD))        25.34 (8.30)      25.82 (7.16)     0.062\n  educ (mean (SD))       10.45 (1.96)      10.35 (2.01)     0.054\n  married (mean (SD))     0.19 (0.39)       0.19 (0.39)    &lt;0.001\n  nodegree (mean (SD))    0.71 (0.46)       0.71 (0.46)    &lt;0.001\n  re74 (mean (SD))     2159.92 (4240.18) 2095.57 (4886.62)  0.014\n  re75 (mean (SD))     1119.08 (2442.29) 1532.06 (3219.25)  0.145\n\n\nGreedy pairwise matching yields, as expected, a reduced data set with 185 subjects in each group. This time all but the variable \\(re75\\) have corresponding \\(SMD&lt;0.1\\). This is not entirely satisfactory and I will attempt to balance the data set using propensity scores next"
  },
  {
    "objectID": "assets/musings/posts/causailty_in_observation_data/index.html#propensity-score-matching",
    "href": "assets/musings/posts/causailty_in_observation_data/index.html#propensity-score-matching",
    "title": "Evaluating causality in observational data",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nA propensity score denoted here \\(\\pi\\) is defined as the probability that a subject \\(i\\) received treatment conditioned on the covariates \\(X\\). I.e. \\(\\pi_i = P(T=1|X_i)\\). A \\(\\pi_i=0.4\\) means there’s a 40% chance the corresponding subject will receive treatment. Covariates can increase or decrease the probability of receiving treatment. For example, if \\(X\\), simplistically the only covariate, is a boolean variable for smoking and smokers are more likely to get a particular treatment then \\(P(T=1|X=1) \\gt P(T=1|X=0)\\).\nInterestingly, two subjects may have the same propensity score in spite of having different covariate values \\(X\\), meaning they are equally likely to receive treatment. Thus reducing the data to a subset of subjects with the same \\(\\pi\\) should balance the treated and control groups. In doing so a crucial assumption in causality, ignorability i.e. how a subject ended in one or the other group can be safely ignored. In a randomized trial, the propensity score is known. In an observational study \\(\\pi\\) is unknown. However because both \\(X\\) and \\(T\\) are collected, \\(\\pi\\) can be estimated. For this I will fit a logistic regression, where the covariates are the input and the treatment variable is the output. Using this model I can get the predicted probability of treatment for each subject; i.e. the estimated \\(\\pi\\).\n\npsmodel &lt;- glm(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75,family=binomial(), data=mydata)\n\nThe model fit is summarized below.\n\n# show fit summary\nsummary(psmodel)\n\n\nCall:\nglm(formula = treat ~ hispan + white + black + age + educ + married + \n    nodegree + re74 + re75, family = binomial(), data = mydata)\n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.663e+00  9.709e-01  -1.713  0.08668 .  \nhispan      -2.082e+00  3.672e-01  -5.669 1.44e-08 ***\nwhite       -3.065e+00  2.865e-01 -10.699  &lt; 2e-16 ***\nblack               NA         NA      NA       NA    \nage          1.578e-02  1.358e-02   1.162  0.24521    \neduc         1.613e-01  6.513e-02   2.477  0.01325 *  \nmarried     -8.321e-01  2.903e-01  -2.866  0.00415 ** \nnodegree     7.073e-01  3.377e-01   2.095  0.03620 *  \nre74        -7.178e-05  2.875e-05  -2.497  0.01253 *  \nre75         5.345e-05  4.635e-05   1.153  0.24884    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 751.49  on 613  degrees of freedom\nResidual deviance: 487.84  on 605  degrees of freedom\nAIC: 505.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nNext I extract the propensity scores from the model object.\n\n# create propensity score\npscore&lt;-psmodel$fitted.values\n\nFinally, I use the MatchIt package to match subjects based on their propensity scores. Note that I set a seed for reproducibility, since matching randomizes data as a first step.\n\nset.seed(42)\nm.out&lt;-matchit(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75, data=mydata, method=\"nearest\")\n\nThe matching results are summarized below.\n\n# summarize the matching outcome\nsummary(m.out)\n\n\nCall:\nmatchit(formula = treat ~ hispan + white + black + age + educ + \n    married + nodegree + re74 + re75, data = mydata, method = \"nearest\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.1822          1.7941     0.9211    0.3774\nhispan          0.0595        0.1422         -0.3498          .    0.0827\nwhite           0.0973        0.6550         -1.8819          .    0.5577\nblack           0.8432        0.2028          1.7615          .    0.6404\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\ndistance   0.6444\nhispan     0.0827\nwhite      0.5577\nblack      0.6404\nage        0.1577\neduc       0.1114\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.3629          0.9739     0.7566    0.1321\nhispan          0.0595        0.2162         -0.6629          .    0.1568\nwhite           0.0973        0.3135         -0.7296          .    0.2162\nblack           0.8432        0.4703          1.0259          .    0.3730\nage            25.8162       25.3027          0.0718     0.4568    0.0847\neduc           10.3459       10.6054         -0.1290     0.5721    0.0239\nmarried         0.1892        0.2108         -0.0552          .    0.0216\nnodegree        0.7081        0.6378          0.1546          .    0.0703\nre74         2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75         1532.0553     1614.7451         -0.0257     1.4956    0.0452\n         eCDF Max Std. Pair Dist.\ndistance   0.4216          0.9740\nhispan     0.1568          1.0743\nwhite      0.2162          0.8390\nblack      0.3730          1.0259\nage        0.2541          1.3938\neduc       0.0757          1.2474\nmarried    0.0216          0.8281\nnodegree   0.0703          1.0106\nre74       0.2757          0.7965\nre75       0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nThe above is more intuitively approached with some plotting as shown below.\n\n# propensity score plots\nplot(m.out, type=\"hist\")\n\n\n\n\nWhat I am looking for with the plot above is an improvement in the overlap between the distribution of propensity scores of matched control and treated groups, relative to the raw. There is obviously some improvement and I’ll next check more concretely below how good the match is using SMD.\n\n# --&gt; MATCHHING WITH & WITHOUT A CALIPER\n# Matching without a caliper\n# -&gt; do greedy matching on logit (PS)\nset.seed(42)\npsmatch &lt;- Match(Tr=mydata$treat, M=1, X=pscore, replace=FALSE)\nps_matched &lt;- mydata[unlist(psmatch[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_pscore &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=ps_matched,\n                              test=FALSE)\nprint(matchedtab1_pscore, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.21 (0.41)       0.06 (0.24)     0.453\n  black (mean (SD))       0.47 (0.50)       0.84 (0.36)     0.852\n  white (mean (SD))       0.32 (0.47)       0.10 (0.30)     0.566\n  age (mean (SD))        25.34 (10.53)     25.82 (7.16)     0.053\n  educ (mean (SD))       10.59 (2.63)      10.35 (2.01)     0.106\n  married (mean (SD))     0.21 (0.41)       0.19 (0.39)     0.041\n  nodegree (mean (SD))    0.64 (0.48)       0.71 (0.46)     0.139\n  re74 (mean (SD))     2455.47 (4352.86) 2095.57 (4886.62)  0.078\n  re75 (mean (SD))     1731.45 (2813.88) 1532.06 (3219.25)  0.066\n\n\nThe table above shows marked improvement relative to the raw data. However, variables like education (\\(educ\\)) and lack of high school degree (\\(nodegree\\)) are marginal, while race-related variables (\\(hispan\\), \\(black\\), \\(white\\)) are still too high to safely avoid confounding. Overall it is quite a bit worse than the first attempt with greedy matching I did earlier using the Mahalanobis distance; though \\(re75\\) is markedly better here.\n\nMatching with a caliper\nOne way to try to improve on the matching is to use a caliper; i.e. a threshold (maximum) distance beyond which matching is not allowed. In practice, though somewhat arbitrarily, (1) the propensity scores are logit-transformed, (2) the standard deviation (SD) is calculated, and (3) the caliper is set to 0.2 times the SD, finally (4) the matching is performed subject to the caliper. A smaller caliper, which results in fewer but better pairs, trades more variance for less bias. The code below performs the aforementioned steps.\n\nset.seed(42)\nlogit_pscore = qlogis(pscore)\npsmatch_calip &lt;-Match(Tr=mydata$treat, M=1, X=logit_pscore, replace=FALSE,\n                caliper=.2)\n# Note that the caliper is in St.Dev. units\nmatched_calip &lt;- mydata[unlist(psmatch_calip[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_calip &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=matched_calip,\n                              test=FALSE)\nprint(matchedtab1_calip, smd=TRUE)\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        114               114                 \n  hispan (mean (SD))      0.11 (0.32)       0.10 (0.30)     0.057\n  black (mean (SD))       0.73 (0.45)       0.75 (0.44)     0.040\n  white (mean (SD))       0.16 (0.37)       0.16 (0.37)    &lt;0.001\n  age (mean (SD))        26.35 (10.83)     25.82 (6.93)     0.059\n  educ (mean (SD))       10.53 (2.63)      10.30 (2.29)     0.092\n  married (mean (SD))     0.26 (0.44)       0.24 (0.43)     0.061\n  nodegree (mean (SD))    0.61 (0.49)       0.64 (0.48)     0.054\n  re74 (mean (SD))     2858.97 (4816.95) 2168.29 (5590.28)  0.132\n  re75 (mean (SD))     1969.43 (3027.44) 1053.23 (2597.71)  0.325\n\n# NOTE the smaller number of subjects for each treatment categories, resultng\n# from dropping previously matched subjects.\n\nUsing the caliper has reduced the number of matched pairs down to 114. The high SMDs seen previously (without the caliper). However, for 1974 and 1975 real incomes \\(SMD &gt; 0.1\\). Thus I cannot be certain that the treatment is the only cause of the outcome. The table above suggests subjects’ earning history is still a causal factor. With this in mind, I next run an outcome analysis for all the approaches described previously. I do this at the end rather than after each outcome as a habit to prevent p-hacking.\n\n\nOutcome analyses:\nTo analyze whether the difference in outcome between the treatment and the control groups are different, I run a paired t-test on the various matched data. But first a quick function to avoid some repetition.\n\n# function that accepts a matched data table and runs the paired t-test.\nrun_matched_ttest&lt;- function(matched_table){\n  # get outcome data for both groups\n  treated_outcome &lt;- matched_table$outcome[matched_table$treat==1]\n  control_outcome &lt;- matched_table$outcome[matched_table$treat==0]\n  \n  # compute pairwise difference between both groups\n  diff_outcome &lt;- treated_outcome - control_outcome\n  \n  # paired t-test\n  t.test(diff_outcome)\n}\n\n\\(\\rightarrow\\) Greedy Mahalanobis distance matching:\n\nrun_matched_ttest(greedymatched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.28944, df = 184, p-value = 0.7726\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1163.595  1563.699\nsample estimates:\nmean of x \n 200.0518 \n\n\n\\(\\rightarrow\\) Propensity score matching (no caliper)\n\nrun_matched_ttest(ps_matched)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.94827, df = 184, p-value = 0.3442\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -708.6934 2020.4023\nsample estimates:\nmean of x \n 655.8544 \n\n\n\\(\\rightarrow\\) Propensity score matching with caliper\n\nrun_matched_ttest(matched_calip)\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 1.2773, df = 113, p-value = 0.2041\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -607.6641 2812.9263\nsample estimates:\nmean of x \n 1102.631 \n\n\nThus, from greedy Mahalanobis distance matching to propensity score matching with caliper, the statistical significance of the difference between groups does increase, with propensity score matching with caliper resulting the in the smallest p-value (0.20). This is still conventionally quite high, so that it is impossible to reject the null hypothesis, i.e. that there are no difference between the groups. Moreover, there is still a potential confounding problem in all cases. E.g. the SMD for real income in `74 and `75 remains above 0.1 suggesting I was not able to remove the confounding effect. Note though that I worked for a subset of the original data. Thus I would next re-run the analysis on the entire data set, which could lead to better matching. But that is a story for another day.\nPeaceful coding!\n\n\n\n\nimage source"
  },
  {
    "objectID": "assets/musings/posts/crouching_desk_hidden_benefits/index.html",
    "href": "assets/musings/posts/crouching_desk_hidden_benefits/index.html",
    "title": "Why I went from sitting to standing to a crouching desk solution.",
    "section": "",
    "text": "My main health concern as I grow older is to remain limber. This is particularly important to me because one of my hobbies is to compete in Brazilian Jiu Jitsu tournaments, where my rivals are 20-30 years younger than I am. The main obstacles to that ambition is how I spend most of my time."
  },
  {
    "objectID": "assets/musings/decision_threshold.html",
    "href": "assets/musings/decision_threshold.html",
    "title": "Create and preprocess data set",
    "section": "",
    "text": "In this combined code, GridSearchCV is used to evaluate multiple scoring metrics (precision, recall, and F1-score). The best model selected by GridSearchCV is then optimized further by finding the optimal decision threshold using the F1-score as the objective function. Finally, the performance of the best model with the optimal threshold is evaluated on the test set across multiple metrics. This approach provides a comprehensive assessment of the model’s performance while ensuring the decision threshold is optimized based on the F1-score.\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import SelectFromModel\nfrom scipy.optimize import fmin\nimport seaborn as sb\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nscaler_X = StandardScaler()\nCreate and split data set\n# Generate synthetic imbalanced data for demonstration\nX, y = make_classification(\n    n_samples=1000, n_features=6, n_informative=3, n_repeated=0, n_classes=2, weights=[0.995, 0.005], random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
  },
  {
    "objectID": "assets/musings/decision_threshold.html#nested-cross-validation",
    "href": "assets/musings/decision_threshold.html#nested-cross-validation",
    "title": "Create and preprocess data set",
    "section": "Nested cross-validation",
    "text": "Nested cross-validation\nNo, the nested scores do not directly apply to the best model from the inner loop. Instead, nested cross-validation is used to estimate the performance of the entire process of model selection and hyperparameter tuning, including both the inner and outer loops.\nHere’s how it works: 1. Outer Loop: In the outer loop of nested cross-validation, the dataset is split into multiple train-test folds. Each iteration of the outer loop involves splitting the data into a training set and a holdout test set. 2. Inner Loop: In the inner loop of nested cross-validation, a separate cross-validation process is performed on the training set to tune the model hyperparameters. This helps in selecting the best hyperparameters for the model. 3. Model Selection: The best hyperparameters selected from the inner loop are then used to train a final model on the entire training set from the current iteration of the outer loop. 4. Performance Estimation: The performance of this final model is evaluated on the holdout test set from the current iteration of the outer loop. 5. Aggregation: The performance metrics (e.g., F1 score) obtained from each iteration of the outer loop are aggregated to estimate the overall performance of the model selection and hyperparameter tuning process.\nTherefore, the nested cross-validation scores reflect the aggregated performance of the entire process, including both the model selection from the inner loop and the final model evaluation from the outer loop. These scores provide an estimate of how well the model selection process generalizes to new data.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport joblib\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# Train the final model on the entire training set with the best hyperparameters\nbest_params = nested_cv.best_params_\nfinal_model = RandomForestClassifier(**best_params)\nfinal_model.fit(X_train, y_train)\n\n# Evaluate the final model on the test set\ny_pred = final_model.predict(X_test)\ntest_f1_score = f1_score(y_test, y_pred)\nprint(\"Test F1 Score: \", test_f1_score)\n\n# Save the final model for production\njoblib.dump(final_model, 'final_model.pkl')\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nclass TunableThresholdClassifier(RandomForestClassifier):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    \n    def _calculate_threshold(self, y_true, y_pred_proba, minimize_fp=False, minimize_fn=False):\n        \"\"\"\n        Calculate the optimal decision threshold based on the specified criteria.\n        \n        Args:\n            y_true (array-like): True labels.\n            y_pred_proba (array-like): Predicted probabilities.\n            minimize_fp (bool): Whether to minimize false positives.\n            minimize_fn (bool): Whether to minimize false negatives.\n        \n        Returns:\n            float: Optimal decision threshold.\n        \"\"\"\n        thresholds = np.linspace(0, 1, 100)  # Generate candidate thresholds\n        best_threshold = None\n        best_score = float('inf') if minimize_fp else float('-inf')  # Initialize best score\n        \n        for threshold in thresholds:\n            y_pred = (y_pred_proba[:, 1] &gt;= threshold).astype(int)\n            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n            \n            if minimize_fp:\n                score = fp  # Minimize false positives\n            elif minimize_fn:\n                score = fn  # Minimize false negatives\n            else:\n                score = fp + fn  # Minimize both false positives and false negatives\n            \n            if (minimize_fp and score &lt; best_score) or (minimize_fn and score &gt; best_score):\n                best_score = score\n                best_threshold = threshold\n        \n        return best_threshold\n    \n    def tune_threshold(self, X, y, minimize_fp=False, minimize_fn=False):\n        \"\"\"\n        Automatically tune the decision threshold based on the specified criteria.\n        \n        Args:\n            X (array-like): Input features.\n            y (array-like): True labels.\n            minimize_fp (bool): Whether to minimize false positives.\n            minimize_fn (bool): Whether to minimize false negatives.\n        \"\"\"\n        y_pred_proba = self.predict_proba(X)\n        threshold = self._calculate_threshold(y, y_pred_proba, minimize_fp, minimize_fn)\n        self.threshold = threshold\n    \n    def predict_with_threshold(self, X):\n        \"\"\"\n        Predict using the model with the tuned decision threshold.\n        \n        Args:\n            X (array-like): Input features.\n        \n        Returns:\n            array-like: Predicted labels.\n        \"\"\"\n        if not hasattr(self, 'threshold'):\n            raise ValueError(\"Threshold not tuned. Please run tune_threshold first.\")\n        \n        y_pred_proba = self.predict_proba(X)\n        y_pred = (y_pred_proba[:, 1] &gt;= self.threshold).astype(int)\n        return y_pred"
  },
  {
    "objectID": "assets/musings/decision_threshold.html#classifier-with-tunable-threshold",
    "href": "assets/musings/decision_threshold.html#classifier-with-tunable-threshold",
    "title": "Create and preprocess data set",
    "section": "Classifier with tunable threshold",
    "text": "Classifier with tunable threshold\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nclass TunableThresholdClassifier(RandomForestClassifier):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    \n    def _calculate_threshold(self, y_true, y_pred_proba, minimize_fp=False, minimize_fn=False):\n        \"\"\"\n        Calculate the optimal decision threshold based on the specified criteria using optimization.\n        \n        Args:\n            y_true (array-like): True labels.\n            y_pred_proba (array-like): Predicted probabilities.\n            minimize_fp (bool): Whether to minimize false positives.\n            minimize_fn (bool): Whether to minimize false negatives.\n        \n        Returns:\n            float: Optimal decision threshold.\n        \"\"\"\n        def objective(threshold):\n            y_pred = (y_pred_proba[:, 1] &gt;= threshold).astype(int)\n            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n            if minimize_fp:\n                return fp\n            elif minimize_fn:\n                return fn\n            else:\n                return fp + fn\n        \n        initial_threshold = 0.5  # Initial guess for threshold\n        best_threshold = fmin(objective, initial_threshold, disp=False)[0]\n        return best_threshold\n\n    \n    def tune_threshold(self, X, y, minimize_fp=False, minimize_fn=False):\n        \"\"\"\n        Automatically tune the decision threshold based on the specified criteria.\n        \n        Args:\n            X (array-like): Input features.\n            y (array-like): True labels.\n            minimize_fp (bool): Whether to minimize false positives.\n            minimize_fn (bool): Whether to minimize false negatives.\n        \"\"\"\n        y_pred_proba = self.predict_proba(X)\n        threshold = self._calculate_threshold(y, y_pred_proba, minimize_fp, minimize_fn)\n        self.threshold = threshold\n    \n    def predict_with_threshold(self, X):\n        \"\"\"\n        Predict using the model with the tuned decision threshold.\n        \n        Args:\n            X (array-like): Input features.\n        \n        Returns:\n            array-like: Predicted labels.\n        \"\"\"\n        if not hasattr(self, 'threshold'):\n            raise ValueError(\"Threshold not tuned. Please run tune_threshold first.\")\n        \n        y_pred_proba = self.predict_proba(X)\n        y_pred = (y_pred_proba[:, 1] &gt;= self.threshold).astype(int)\n        return y_pred\n\nThis class, CrossValidator, provides the following functionalities:\n\nIt takes a model (model) and a grid of parameters (params_grid) as input.\nIt has the option to perform nested cross-validation (_perform_outer_cv) or non-nested cross-validation (fit method) depending on the dataset size and class imbalance.\nIt fits the model using the specified data (fit method) and stores the best model, best parameters, and best F1 score.\nIt can make predictions using the best model (predict method).\nYou can use this class by instantiating it with your model and parameter grid, and then calling the fit method with your data to perform cross-validation and parameter tuning. Finally, you can use the predict method to make predictions with the best model.\n\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer, f1_score\nfrom sklearn.base import clone\nimport numpy as np\n\nclass CrossValidator:\n    def __init__(self, model, params_grid, n_splits_outer=5, n_splits_inner=3):\n        self.model = model\n        self.params_grid = params_grid\n        self.n_splits_outer = n_splits_outer\n        self.n_splits_inner = n_splits_inner\n        \n        self.best_model = None\n        self.best_params = None\n        self.best_score = None\n    \n    def _check_data_size(self, X):\n        return len(X) &lt; 1000\n    \n    def _check_class_balance(self, y):\n        return np.bincount(y).min() &lt; len(y) / 10\n    \n    def _perform_inner_cv(self, X_train, y_train):\n        scorer = make_scorer(f1_score)\n        inner_cv = StratifiedKFold(n_splits=self.n_splits_inner, shuffle=True, random_state=42)\n        grid_search = GridSearchCV(estimator=self.model, param_grid=self.params_grid, scoring=scorer, cv=inner_cv)\n        grid_search.fit(X_train, y_train)\n        return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n    \n    def _perform_outer_cv(self, X, y):\n        outer_cv = StratifiedKFold(n_splits=self.n_splits_outer, shuffle=True, random_state=42)\n        outer_scores = []\n        for train_index, test_index in outer_cv.split(X, y):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            \n            best_model_inner, best_params_inner, best_score_inner = self._perform_inner_cv(X_train, y_train)\n            best_model_inner.fit(X_train, y_train)\n            y_pred = best_model_inner.predict(X_test)\n            score = f1_score(y_test, y_pred)\n            outer_scores.append(score)\n            if self.best_score is None or score &gt; self.best_score:\n                self.best_score = score\n                self.best_model = clone(best_model_inner)\n                self.best_params = best_params_inner\n        return outer_scores\n    \n    def fit(self, X, y):\n        if self._check_data_size(X) or self._check_class_balance(y):\n            print(\"Performing non-nested cross-validation.\")\n            scorer = make_scorer(f1_score)\n            cv = StratifiedKFold(n_splits=self.n_splits_outer, shuffle=True, random_state=42)\n            grid_search = GridSearchCV(estimator=self.model, param_grid=self.params_grid, scoring=scorer, cv=cv)\n            grid_search.fit(X, y)\n            self.best_model = grid_search.best_estimator_\n            self.best_params = grid_search.best_params_\n            self.best_score = grid_search.best_score_\n        else:\n            print(\"Performing nested cross-validation.\")\n            self._perform_outer_cv(X, y)\n            print(\"Best F1 Score:\", self.best_score)\n            print(\"Best Parameters:\", self.best_params)\n    \n    def predict(self, X):\n        if self.best_model is None:\n            raise ValueError(\"Fit the model first.\")\n        return self.best_model.predict(X)\n\n\nUsing OPTUNA\n\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.base import clone\n\nclass CrossValidator:\n    def __init__(self, model, params_space, n_trials=100, n_splits_outer=5, n_splits_inner=3):\n        self.model = model\n        self.params_space = params_space\n        self.n_trials = n_trials\n        self.n_splits_outer = n_splits_outer\n        self.n_splits_inner = n_splits_inner\n        \n        self.best_model = None\n        self.best_params = None\n        self.best_score = None\n    \n    def _check_data_size(self, X):\n        return len(X) &lt; 1000\n    \n    def _check_class_balance(self, y):\n        return np.bincount(y).min() &lt; len(y) / 10\n    \n    def _objective(self, trial, X, y):\n        params = {}\n        for param_name, param_values in self.params_space.items():\n            params[param_name] = trial.suggest_categorical(param_name, param_values)\n        \n        scorer = make_scorer(f1_score)\n        outer_cv = StratifiedKFold(n_splits=self.n_splits_outer, shuffle=True, random_state=42)\n        scores = []\n        for train_index, test_index in outer_cv.split(X, y):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            \n            model = clone(self.model)\n            model.set_params(**params)\n            \n            inner_cv = StratifiedKFold(n_splits=self.n_splits_inner, shuffle=True, random_state=42)\n            inner_scores = []\n            for inner_train_index, inner_test_index in inner_cv.split(X_train, y_train):\n                X_inner_train, X_inner_val = X_train[inner_train_index], X_train[inner_test_index]\n                y_inner_train, y_inner_val = y_train[inner_train_index], y_train[inner_test_index]\n                \n                model.fit(X_inner_train, y_inner_train)\n                y_inner_pred = model.predict(X_inner_val)\n                inner_scores.append(f1_score(y_inner_val, y_inner_pred))\n            \n            scores.append(np.mean(inner_scores))\n        \n        return np.mean(scores)\n    \n    def fit(self, X, y):\n        if self._check_data_size(X) or self._check_class_balance(y):\n            print(\"Performing non-nested cross-validation.\")\n            study = optuna.create_study(direction='maximize')\n            study.optimize(lambda trial: self._objective(trial, X, y), n_trials=self.n_trials)\n            self.best_params = study.best_params\n            self.best_score = study.best_value\n        else:\n            print(\"Performing nested cross-validation.\")\n            self.best_score = self._objective(optuna.trial.FixedTrial({}), X, y)\n            print(\"Best F1 Score:\", self.best_score)\n    \n    def predict(self, X):\n        if self.best_model is None:\n            raise ValueError(\"Fit the model first.\")\n        self.best_model.set_params(**self.best_params)\n        self.best_model.fit(X, y)\n        return self.best_model.predict(X)"
  },
  {
    "objectID": "assets/musings/decision_threshold.html#decision-threshold-tuning-with-optuna",
    "href": "assets/musings/decision_threshold.html#decision-threshold-tuning-with-optuna",
    "title": "Create and preprocess data set",
    "section": "Decision Threshold tuning with Optuna",
    "text": "Decision Threshold tuning with Optuna\n\nimport optuna\nfrom sklearn.metrics import f1_score\n\nclass TunableThresholdClassifier:\n    def __init__(self, model, params_grid):\n        self.model = model\n        self.params_grid = params_grid\n        self.threshold = 0.5\n    \n    def _objective(self, trial, X_train, y_train, X_val, y_val, minimize_fp=False, minimize_fn=False):\n        threshold = trial.suggest_float('threshold', 0, 1)\n        \n        model = clone(self.model)\n        model.set_params(**self.params_grid)\n        model.fit(X_train, y_train)\n        \n        y_pred_proba = model.predict_proba(X_val)\n        y_pred = (y_pred_proba[:, 1] &gt;= threshold).astype(int)\n        \n        if minimize_fp:\n            score = f1_score(y_val, y_pred, pos_label=1)\n        elif minimize_fn:\n            score = f1_score(y_val, y_pred, pos_label=0)\n        else:\n            score = f1_score(y_val, y_pred)\n        \n        return score\n    \n    def tune_threshold(self, X_train, y_train, X_val, y_val, minimize_fp=False, minimize_fn=False):\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: self._objective(trial, X_train, y_train, X_val, y_val, minimize_fp, minimize_fn), n_trials=100)\n        self.threshold = study.best_params['threshold']\n    \n    def predict_with_threshold(self, X):\n        y_pred_proba = self.model.predict_proba(X)\n        y_pred = (y_pred_proba[:, 1] &gt;= self.threshold).astype(int)\n        return y_pred\n\nModuleNotFoundError: No module named 'optuna'\n\n\n\nimport pytest\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom your_module import TunableThresholdClassifier, CrossValidator\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef test_tunable_threshold_classifier():\n    # Instantiate TunableThresholdClassifier\n    model = RandomForestClassifier()\n    params_grid = {'n_estimators': [10, 20, 30], 'max_depth': [None, 5, 10]}\n    clf = TunableThresholdClassifier(model, params_grid)\n    \n    # Fit the model\n    clf.fit(X_train, y_train)\n    \n    # Tune the threshold\n    clf.tune_threshold(X_train, y_train, X_test, y_test)\n    \n    # Predict using the tuned threshold\n    y_pred = clf.predict_with_threshold(X_test)\n    \n    # Check if predictions are valid\n    assert len(y_pred) == len(y_test)\n    assert set(y_pred) == {0, 1}\n    \n    # Calculate F1 score\n    f1 = f1_score(y_test, y_pred)\n    assert f1 &gt;= 0 and f1 &lt;= 1\n\ndef test_cross_validator():\n    # Define model and parameter space\n    model = RandomForestClassifier()\n    params_space = {'n_estimators': [10, 20, 30], 'max_depth': [None, 5, 10]}\n    \n    # Instantiate CrossValidator\n    cv = CrossValidator(model, params_space)\n    \n    # Fit the model\n    cv.fit(X_train, y_train)\n    \n    # Check if best score is valid\n    assert cv.best_score is not None\n    assert cv.best_score &gt;= 0 and cv.best_score &lt;= 1\n    \n    # Predict using the best model\n    y_pred = cv.predict(X_test)\n    \n    # Check if predictions are valid\n    assert len(y_pred) == len(y_test)\n    assert set(y_pred) == {0, 1}\n    \n    # Calculate F1 score\n    f1 = f1_score(y_test, y_pred)\n    assert f1 &gt;= 0 and f1 &lt;= 1\n\n\nModuleNotFoundError: No module named 'pytest'"
  },
  {
    "objectID": "assets/musings/decision_thresh_dev.html",
    "href": "assets/musings/decision_thresh_dev.html",
    "title": "portfolio",
    "section": "",
    "text": "Plan:\n\nUse the breast cancer data set and train a tree-based model\n 1. use a basic train/test split\n 2. compute precision recall, f1, roc_auc to assess goodness of fit\n 3. use yellowbrick to compute optimal decision threshold\n 4. use precision_recall_curve and roc_curve to get thresholds\n 5. manually optimize decision threshold to minimize, false negatives, false positives, both.\n 6. repeat the same with an automated tuning function (compare pure straight func against fmin and optuna.)\nExpand the above by implementing a cross-validation scheme. \\(\\rightarrow\\) repeat the stesps above\nSame as 2 but this time wit nested cross-validation\nWrite classes for both models and cross-validation and verify similar reuults can be obtained."
  },
  {
    "objectID": "assets/musings/projects.html",
    "href": "assets/musings/projects.html",
    "title": "Some of my work…",
    "section": "",
    "text": "Untitled\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhy I went from sitting to standing to a crouching desk solution.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nErdem Karaköylü\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating causality in observational data\n\n\n\n\n\n\n\ncausality\n\n\npropensity score\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nErdem Karaköylü\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assets/musings/index.html",
    "href": "assets/musings/index.html",
    "title": "Welcome to my Blog-o-folio!",
    "section": "",
    "text": "This site is meant to help those - including me! - that need a different take on a technical topic of interest. The site is also meant to educate and entertain with entries Statistics, Data Science, Modeling in a variety of fields. If you dig deep enough, you’ll even fine some entries on Marine Ecology Modeling; for such was my journey. Enjoy, and by all means do leave a comment. Regardless of its polarity I find feedback a useful improvement tool.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhy I went from sitting to standing to a crouching desk solution.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nErdem Karaköylü\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating causality in observational data\n\n\n\n\n\n\n\ncausality\n\n\npropensity score\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nErdem Karaköylü\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assets/musings/about.html",
    "href": "assets/musings/about.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "I am a Data Scientist, currently employed at Research Innovations Inc (RII), where my various projects span a number of topics including sentiment analysis, directed sentiment analysis, fake news detection, uncertainty estimation in deep learning, graph analytics, and graph neural networks. For the past 10 years, my goto programming language for these and other analytics and modeling tasks has been Python, though I sometimes fire up R studio to code up some quick statistical analysis in R.\nBefore RII I was a scientific programmer at the NASA Ocean Biology Processing Group, where I cut my teeth coding up Bayesian models to estimate the prediction uncertainty for a variety of satellite processing algorithms. These algorithms are the backbone of product generation for data generated by legacy and active satellite missions, as well as the PACE (Plankton Aerosol Cloud Ecosystem) mission scheduled to begin in 2024.\nPrior to my analytics/data science career, I trained as an oceanographer. I graduated with a PhD from Scripps Institution of Oceanography, where I primarily focused on the ecology of tiny organisms that nevertheless are tremendously important in ocean ecological processes and have ramifications for both our supply of seafood (protein!) as well as climate change."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Porfolio",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .grid .quarto-listing-cols-3}\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  }
]