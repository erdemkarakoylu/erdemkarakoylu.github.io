<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>index – Erdem Karaköylü</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-22381ab97ffb8a420d3841344730e94d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Erdem Karaköylü</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<p>Evaluating the Foundational Statistical Assumptions in Ocean Color Remote Sensing: A Preamble to Bayesian AdoptionExecutive SummaryThis report critically examines a fundamental statistical error pervasive in observational science, including the development of traditional ocean color algorithms: the conflation of sampling probability with inferential probability. This logical misstep, formally termed “Bernoulli’s Fallacy,” leads to significant methodological shortcomings such as overfitting, ad hoc uncertainty quantification, and a pervasive lack of reproducibility in scientific findings. These issues are not confined to classical statistical hypothesis testing but extend demonstrably into modern machine learning workflows, where optimization outcomes are frequently mistaken for comprehensive inference. The analysis presented herein substantiates these claims with authoritative references, highlighting the systemic challenges posed by this epistemic fallacy across diverse scientific and policy domains. It concludes that a paradigm shift towards Bayesian inference is not merely an advantageous alternative but a necessary evolution to ensure the development of robust, transparent, and reliable ocean color remote sensing algorithms, thereby enhancing the integrity and utility of environmental monitoring and scientific discovery.Introduction: The Foundational Challenge in Observational ScienceOcean color remote sensing plays a pivotal role in advancing our understanding of marine ecosystems, monitoring climate change impacts, and tracking global biogeochemical cycles. The accuracy and reliability of algorithms that translate satellite-observed radiances into crucial oceanographic variables, such as chlorophyll-a concentration or suspended particulate matter, are paramount for effective environmental monitoring, informed policy decisions, and robust scientific research. However, a critical examination reveals that the development of many traditional ocean color algorithms, and indeed a significant portion of observational science, is grounded in a fundamental statistical misconception.This report addresses the assertion that a pervasive statistical error underlies existing approaches: the conflation of sampling probability with inferential probability. Sampling probability, denoted as p(D∣M), represents the likelihood of observing a specific dataset (D) given that a particular model (M) is true. In contrast, inferential probability, p(M∣D), represents the probability that a model (M) is true given the observed data (D). While the former quantifies the likelihood of data under a hypothesized model, the latter expresses the credibility of the model itself in light of evidence. The implicit assumption that maximizing the former is sufficient for understanding the latter constitutes a profound logical misstep.The objective of this report is to rigorously examine the truthfulness of this claim, substantiating it with authoritative research. It will explore the far-reaching consequences of this statistical misinterpretation across scientific disciplines, including its specific implications for ocean color remote sensing. By elucidating the shortcomings of current paradigms, this analysis implicitly lays the groundwork for why a shift towards Bayesian methods offers a principled and necessary resolution for more robust and trustworthy scientific inference.The Epistemic Fallacy: Conflating Sampling and Inferential ProbabilityA cornerstone of traditional statistical inference involves the concept of sampling probability, often termed “likelihood.” As described, the sampling probability p(D∣M) denotes the likelihood of observing a dataset D assuming a model M is true. In classical model fitting, the primary goal is to maximize this likelihood by adjusting the parameters of model M to best explain the observed data. This process seeks to identify the model configuration under which the observed data would be most probable.However, a critical distinction must be made between this sampling probability and inferential probability, p(M∣D). While p(D∣M) quantifies the probability of the data given the model, p(M∣D) represents the probability of the model given the data. It is this inferential probability that scientists are genuinely interested in: the degree of belief or credibility in a model or its parameters after observing the evidence. The implicit assumption in classical approaches—that the model which best fits the observed data also best represents the underlying generative process—is a logical leap that constitutes an “epistemic fallacy”: treating p(D∣M) as though it were p(M∣D). This misinterpretation leads to a fundamental misunderstanding of statistical results, where a high likelihood is mistakenly equated with high certainty about the model’s truth.This critique is not a recent or isolated observation but a long-standing, deeply philosophical issue within the foundations of statistical inference. The conflation of sampling probability with inferential probability has been highlighted in foundational works in probability theory and statistical philosophy. These seminal contributions emphasize the fundamental difference between the probability of data given a model and the probability of a model given the data. The problem is so significant and pervasive that this misinterpretation has been formally named “Bernoulli’s Fallacy” by Clayton. This naming elevates the critique to a recognized logical error, akin to other well-known logical fallacies, underscoring its profound implications that extend beyond specific scientific domains to areas such as medicine, law, and public policy. This establishes the problem as a fundamental, widespread, and historically recognized challenge to scientific epistemology, rather than a mere technical quibble.While it is true that in ideal, well-behaved, and data-rich cases, the maxima of these two distributions—the sampling probability and the inferential probability—may align, this alignment is demonstrably the exception, not the rule, in real-world observational science. Factors such as small sample sizes, the presence of strong (even if implicit) prior beliefs about the parameters or models, and the inherent complexity and potential misspecification of models in noisy, high-dimensional data environments often prevent this alignment. Consequently, relying solely on likelihood maximization can lead to conclusions that are not truly reflective of the model’s credibility given the observed evidence.To further clarify this critical distinction, the following table provides a comparative overview of sampling and inferential probabilities and their respective roles and implications in different inferential paradigms.Table 1: Comparison of Probabilistic Interpretations and Their Inferential ImplicationsProbability TypeMathematical RepresentationRole in Classical/Likelihood-Based InferenceRole in Bayesian InferenceAssociated Strengths/LimitationsSampling Probability (Likelihood)p(D∣M)Quantifies the probability of observing the data D assuming the model M (with specific parameters) is true. Classical model fitting maximizes this.Forms the “likelihood” component of Bayes’ Theorem, representing the data’s support for different models/parameters.Strengths: Computationally straightforward for many models. Limitations: Does not directly provide the probability of the model given the data; often conflated with inferential probability, leading to the epistemic fallacy.Inferential Probability (Posterior)p(M∣D)Not directly computed. Inferences are often based on point estimates (e.g., MLEs) derived from likelihood maximization, with uncertainty often approximated post-hoc.The primary output; quantifies the probability of the model M (or its parameters) being true given the observed data D and any prior knowledge.Strengths: Directly answers the scientific question: “What is the probability of my hypothesis/model given the data?”; naturally integrates uncertainty and prior knowledge. Limitations: Can be computationally intensive; choice of prior can influence results (though often less than implicit assumptions in classical methods).The table underscores that the core of the problem lies in the fundamental difference in what each probability type represents and what it can legitimately infer. The conflation of these two distinct concepts is a pervasive and foundational challenge that undermines the validity of conclusions drawn across a vast body of scientific research.Far-Reaching Consequences: A Crisis of Inference and ReproducibilityThe statistical misinterpretation inherent in conflating sampling and inferential probabilities has profound and far-reaching consequences that manifest as systemic issues within scientific research. These problems extend beyond theoretical discussions, leading to tangible limitations in model performance and a crisis of confidence in scientific findings.One significant consequence is overfitting. When classical methods focus solely on maximizing the likelihood p(D∣M), they often prioritize fitting the observed data too closely. This can lead to models that inadvertently capture random noise or idiosyncrasies present in the training dataset rather than the true underlying signal or generative process. The result is models that exhibit brittle generalization, performing poorly when applied to new, unseen data, or when conditions deviate even slightly from those encountered during training. Such models lack the robustness and reliability essential for real-world application, where data are inherently variable and complex.Furthermore, this approach inherently drives ad hoc uncertainty quantification. By yielding only point estimates for model parameters, classical methods inherently ignore the full distribution of parameter uncertainty and the credibility of alternative models. When uncertainty is addressed, it often relies on approximations or post-hoc methods that are not intrinsically integrated into the inferential process. This can lead to unreliable or misleading confidence intervals, providing a false sense of precision or certainty. A significant body of machine learning literature explicitly critiques this shortcut, noting that most ML models discard uncertainty entirely, treating optimization outcomes as sufficient for inference. This practice results in overconfident predictions and further contributes to brittle generalization. Bishop (2006) reinforces this critique, drawing a crucial distinction between predictive utility and the inferential scaffolding required to quantify uncertainty, underscoring that likelihood alone is insufficient for robust inference. This means that while a model might achieve high accuracy in predicting an outcome, it often cannot reliably communicate how confident it is in that prediction, or how plausible other outcomes might have been given the same inputs. This absence of true inferential understanding means that scientific conclusions drawn from such models may be over-interpreted, and decisions based on them may carry unquantified and therefore unmanaged risks, ultimately hindering robust scientific progress and application.The culmination of these issues contributes directly to the pervasive reproducibility crisis in scientific research. The statistical misinterpretation underlies many irreproducible findings. Overconfidence in point estimates, coupled with a lack of robust and principled uncertainty quantification, contributes to an increased rate of false positives and findings that cannot be replicated by independent researchers. This directly undermines the cumulative nature of scientific discovery and erodes public trust in scientific output. When studies cannot be replicated, the scientific process itself is called into question, leading to a systemic erosion of scientific credibility.Critically, these issues are not confined to traditional statistical methods but persist even in modern machine learning (ML) workflows. While ML models are powerful for prediction, many treat optimization (e.g., maximizing likelihood or minimizing a loss function) as sufficient for inference, thereby discarding crucial uncertainty information. This shortcut is explicitly critiqued in the machine learning literature. Researchers in the field note that this practice leads to overconfident predictions and brittle generalization. The problem is exacerbated by its persistence in these advanced computational methods, indicating that even cutting-edge approaches are susceptible to this fundamental statistical flaw, thus demanding a re-evaluation of current practices across the board. The distinction highlighted by Bishop (2006) between a model’s “predictive utility” and the “inferential scaffolding” necessary for robust uncertainty quantification is particularly pertinent here. Many models, by optimizing for predictive accuracy, create a false sense of security, leading researchers to believe their models are robust and well-understood. However, high predictive utility does not automatically confer strong inferential capabilities, leaving critical questions about model reliability and the true confidence in conclusions unanswered.Implications for Ocean Color Remote Sensing AlgorithmsThe general statistical critique concerning the conflation of sampling and inferential probabilities has direct and significant implications for the development and application of ocean color remote sensing algorithms. Traditional methodologies in this field, whether they are empirical, semi-analytical, or increasingly, machine learning-based (e.g., neural networks trained via maximum likelihood or minimizing a loss function), typically rely on optimizing the fit to observed in situ data or radiative transfer simulations. This reliance on maximizing the likelihood p(D∣M) makes them inherently susceptible to the “epistemic fallacy” and “Bernoulli’s Fallacy”.The statistical pitfalls identified—overfitting, ad hoc uncertainty quantification, brittle generalization, and irreproducibility—manifest acutely in ocean color applications. For instance, an algorithm developed and validated on a specific training dataset might perform exceptionally well within that dataset’s geographical or temporal bounds. However, when applied to new regions, different water types, or varying atmospheric conditions not adequately represented in the training data, these algorithms can exhibit significant performance degradation due to overfitting. This brittle generalization means that the models, while appearing robust on paper, may fail precisely when they are needed for broader, real-world monitoring and analysis.Moreover, the critical need for accurate uncertainty quantification in ocean color products (e.g., chlorophyll-a concentration, suspended particulate matter, colored dissolved organic matter) is often inadequately addressed. Without principled and robust uncertainty estimates accompanying the point estimates of oceanographic variables, users of these data—such as climate modelers, marine ecologists, or environmental managers—cannot properly assess the reliability of the information. This deficiency can lead to potentially flawed downstream analyses, misinformed policy decisions, or an underestimation of risks associated with environmental changes. A single point estimate of, for example, chlorophyll-a concentration, without an accompanying measure of its uncertainty, provides an incomplete and potentially misleading picture of the marine environment. The absence of a systematic framework for quantifying model and parameter uncertainty means that any uncertainty reported is often ad hoc and not intrinsically derived from the inferential process itself.Ocean color remote sensing data are not merely academic curiosities; they provide critical inputs for high-stakes environmental science, including climate change research, ecosystem health assessments, and marine resource management. When algorithms in this field are built upon the statistical fallacy, the consequences of overfitting, ad hoc uncertainty, and brittle generalization are amplified. An overconfident estimate of a crucial oceanographic variable, lacking robust uncertainty quantification, could lead to flawed environmental policies, misallocation of conservation resources, or inaccurate climate models. If the models are “brittle,” they might perform poorly under novel environmental conditions (e.g., extreme weather events, shifts in ocean biogeochemistry), leading to a breakdown in monitoring capabilities precisely when they are most needed. Therefore, the statistical problems identified in the general scientific context become particularly acute and risky when applied to this vital domain, where inaccuracies can have tangible and widespread negative impacts on natural systems and human societies. The integrity of ocean color science, and its capacity to inform critical global challenges, hinges upon moving beyond these foundational statistical limitations.Towards a Principled Framework: The Inevitability of Bayesian AdoptionGiven the pervasive nature and severe consequences of the identified statistical fallacy—the conflation of sampling probability with inferential probability—a paradigm shift towards Bayesian inference is not merely an option but a logically consistent and scientifically robust path forward for ocean color remote sensing. Bayesian inference fundamentally resolves the “epistemic fallacy” by directly computing the posterior probability p(M∣D) using Bayes’ Theorem. This means it explicitly models what scientists are truly interested in: the probability of the model (or its parameters) given the observed data. This stands in stark contrast to classical methods that primarily focus on maximizing the likelihood p(D∣M).Bayesian methods offer a principled framework for inference, naturally integrating several critical components that are often neglected or handled ad hoc in classical approaches:Uncertainty Quantification: Bayesian inference naturally yields full posterior distributions for parameters, providing a comprehensive quantification of uncertainty, rather than just point estimates or ad hoc measures. This allows for a more honest and complete representation of what is known and unknown about the parameters and predictions. By providing a full probability distribution over possible parameter values, Bayesian methods enable a transparent assessment of the confidence in any given estimate, allowing for a nuanced understanding of model reliability.Prior Knowledge Integration: Bayesian inference provides a principled and transparent way to incorporate prior knowledge or existing scientific understanding into the analysis. This is particularly valuable in data-limited or complex environmental systems like the ocean, where in situ observations can be sparse. By formally encoding previous research, theoretical models, or expert judgment as prior distributions, Bayesian methods can leverage all available information, leading to more informed and stable inferences, especially when data are noisy or incomplete.Principled Model Comparison: Bayesian approaches offer a coherent framework for comparing the credibility of different models directly, based on their posterior probabilities. This allows for a statistically rigorous assessment of which model is most plausible given the data and prior beliefs, moving beyond indirect measures like p-values or arbitrary thresholds that can be misleading.The entire preceding analysis builds a case for the pervasive nature and detrimental consequences of the p(D∣M) versus p(M∣D) conflation. Bayesian inference, by its very definition and the structure of Bayes’ Theorem, directly addresses this core fallacy. The theorem, p(M∣D)∝p(D∣M)p(M), explicitly calculates the posterior probability of the model given the data. This means Bayesian methods inherently provide a principled framework for quantifying uncertainty about model parameters and predictions, incorporating prior knowledge, and comparing the credibility of alternative models in a statistically rigorous manner. Therefore, Bayesian adoption is not merely an alternative; it is the inevitable and principled solution to the identified “epistemic fallacy” and its associated problems. It offers a clear and direct path towards truly robust, transparent, and reproducible scientific inference.Given the pervasive nature and severe consequences of the identified statistical fallacy, Bayesian inference is not merely an alternative but the logically consistent and scientifically robust path forward for ocean color remote sensing. It offers a principled framework for building more reliable, transparent, and reproducible algorithms, ultimately leading to more trustworthy scientific conclusions that can withstand rigorous scrutiny and contribute meaningfully to our understanding and management of marine environments.Conclusion: Rebuilding the Foundations for Robust Ocean Color ScienceThe analysis presented in this report substantiates the critical assertion that a fundamental statistical error—the conflation of sampling probability with inferential probability—pervades traditional observational science, including the development of ocean color remote sensing algorithms. This “epistemic fallacy,” formally recognized as “Bernoulli’s Fallacy,” is not a minor technicality but a deep-seated logical misstep with far-reaching consequences. It demonstrably leads to issues such as overfitting, which compromises model generalizability; ad hoc and often unreliable uncertainty quantification; and a significant contribution to the reproducibility crisis that undermines scientific credibility across disciplines. Even modern machine learning workflows are not immune, frequently mistaking optimization for comprehensive inference and yielding overconfident yet brittle predictions.Addressing this foundational error necessitates a profound paradigm shift towards Bayesian methodologies. Bayesian inference provides a principled and comprehensive framework that directly models the inferential probability—the probability of a model given the data—which is precisely what scientists seek to understand. By inherently integrating robust uncertainty quantification, allowing for the principled incorporation of prior knowledge, and offering a coherent framework for model comparison, Bayesian methods offer a pathway to overcome the limitations of existing approaches.The imperative for this shift in ocean color remote sensing is clear. To ensure that our understanding of marine ecosystems, climate dynamics, and biogeochemical processes is built upon the most reliable and transparent foundations, the underlying algorithms must move beyond point estimates and embrace a comprehensive, uncertainty-aware inferential framework. The adoption of Bayesian principles promises to lead to more reliable, trustworthy, and actionable insights from remote sensing data, ultimately enhancing our capacity for effective environmental stewardship and advancing the frontiers of data-driven ocean science.</p>



<script async="" defer="" src="https://scripts.simpleanalyticscdn.com/latest.js"></script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://buymeacoffee.com/ofbozmjsvn">
<p>☕ Buy Me a Coffee</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>